{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# XGBoost, LightGBM & CatBoost: The Gradient Boosting Powerhouse\n",
    "\n",
    "Welcome to your comprehensive guide to **XGBoost, LightGBM, and CatBoost** - the trinity of modern gradient boosting algorithms that dominate machine learning competitions and real-world applications!\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Evolution of Boosting**: From AdaBoost to XGBoost and beyond\n",
    "2. **XGBoost Deep Dive**: Mathematics, optimization, and regularization\n",
    "3. **LightGBM Innovation**: Leaf-wise growth and speed optimizations\n",
    "4. **CatBoost Advantages**: Native categorical handling and ordered boosting\n",
    "5. **Performance Comparison**: When to use which algorithm\n",
    "6. **Hyperparameter Tuning**: Advanced optimization strategies\n",
    "7. **Feature Engineering**: Boosting-specific techniques\n",
    "8. **Practical Applications**: Competition-winning strategies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boosting-evolution",
   "metadata": {},
   "source": [
    "## 1. The Evolution of Gradient Boosting\n",
    "\n",
    "### From Simple to Sophisticated\n",
    "\n",
    "**Boosting Philosophy**: \"Combine many weak learners to create a strong learner\"\n",
    "\n",
    "### Timeline of Innovation\n",
    "\n",
    "1. **AdaBoost (1995)**: Adaptive boosting, changes sample weights\n",
    "2. **Gradient Boosting (2001)**: Fits new models to residual errors\n",
    "3. **XGBoost (2016)**: Extreme gradient boosting with regularization\n",
    "4. **LightGBM (2017)**: Microsoft's speed-optimized approach\n",
    "5. **CatBoost (2017)**: Yandex's categorical-friendly algorithm\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "- **XGBoost**: Second-order optimization, regularization, parallel processing\n",
    "- **LightGBM**: Leaf-wise tree growth, histogram-based learning\n",
    "- **CatBoost**: Ordered boosting, native categorical features\n",
    "\n",
    "### Why They Dominate\n",
    "\n",
    "üèÜ **Competition Success**: Kaggle winners use these algorithms 80% of the time\n",
    "üöÄ **Production Ready**: Fast inference, robust performance\n",
    "üéØ **Versatile**: Handle tabular data exceptionally well\n",
    "‚ö° **Efficient**: Optimized for speed and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from utils.data_utils import load_titanic_data\n",
    "from utils.evaluation import ModelEvaluator\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[START] XGBoost, LightGBM & CatBoost Tutorial\")\n",
    "print(\"üì¶ Libraries loaded successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(f\"CatBoost version: {CatBoostClassifier().get_param('used_ram_limit')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-boosting-intuition",
   "metadata": {},
   "source": [
    "## 2. Gradient Boosting Intuition\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Imagine you're trying to predict house prices, and your first model predicts $300k for a house worth $350k.\n",
    "\n",
    "**Traditional ML**: Train a new model from scratch\n",
    "**Gradient Boosting**: Train a model to predict the error ($50k), then combine both models\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Final Prediction**: $F(x) = F_0(x) + \\sum_{m=1}^M \\gamma_m h_m(x)$\n",
    "\n",
    "Where:\n",
    "- $F_0(x)$: Initial prediction (usually mean)\n",
    "- $h_m(x)$: New tree trained on residuals\n",
    "- $\\gamma_m$: Learning rate for tree $m$\n",
    "- $M$: Number of boosting rounds\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. **Initialize**: $F_0(x) = \\text{argmin}_\\gamma \\sum L(y_i, \\gamma)$\n",
    "2. **For each iteration m**:\n",
    "   - Calculate residuals: $r_{im} = -\\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}$\n",
    "   - Train tree on residuals: $h_m(x)$\n",
    "   - Find optimal step size: $\\gamma_m$\n",
    "   - Update: $F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "üéØ **Focused Learning**: Each new model focuses on previous mistakes\n",
    "üîÑ **Iterative Improvement**: Gradually reduces prediction errors\n",
    "üéõÔ∏è **Controllable**: Learning rate prevents overfitting\n",
    "üå≥ **Ensemble Power**: Combines many simple trees effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boosting-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient boosting process\n",
    "print(\"=== GRADIENT BOOSTING VISUALIZATION ===\")\n",
    "print()\n",
    "\n",
    "# Create a simple 1D regression problem for visualization\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_viz = np.linspace(0, 10, n_samples).reshape(-1, 1)\n",
    "y_viz = np.sin(X_viz.ravel()) + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "print(f\"Created 1D regression dataset: {X_viz.shape[0]} samples\")\n",
    "print(f\"Target function: sin(x) + noise\")\n",
    "print()\n",
    "\n",
    "# Simulate gradient boosting steps manually\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize with mean\n",
    "F0 = np.mean(y_viz)\n",
    "predictions = [F0] * len(y_viz)\n",
    "residuals_history = []\n",
    "predictions_history = [predictions.copy()]\n",
    "\n",
    "learning_rate = 0.3\n",
    "n_estimators = 5\n",
    "\n",
    "print(f\"Simulating gradient boosting with {n_estimators} trees:\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Initial prediction (mean): {F0:.3f}\")\n",
    "print()\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    # Calculate residuals\n",
    "    residuals = y_viz - predictions\n",
    "    residuals_history.append(residuals.copy())\n",
    "    \n",
    "    # Train tree on residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3, random_state=42+i)\n",
    "    tree.fit(X_viz, residuals)\n",
    "    \n",
    "    # Update predictions\n",
    "    tree_predictions = tree.predict(X_viz)\n",
    "    predictions = predictions + learning_rate * tree_predictions\n",
    "    predictions_history.append(predictions.copy())\n",
    "    \n",
    "    # Calculate current MSE\n",
    "    mse = np.mean((y_viz - predictions) ** 2)\n",
    "    residual_std = np.std(residuals)\n",
    "    \n",
    "    print(f\"Tree {i+1}: MSE = {mse:.4f}, Residual Std = {residual_std:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Gradient boosting process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-boosting-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the gradient boosting evolution\n",
    "print(\"=== VISUALIZING BOOSTING EVOLUTION ===\")\n",
    "print()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot each step of the boosting process\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    if i == 0:\n",
    "        # Initial prediction (mean)\n",
    "        ax.scatter(X_viz.ravel(), y_viz, alpha=0.6, color='blue', s=30, label='True values')\n",
    "        ax.axhline(y=F0, color='red', linewidth=2, label=f'Initial (mean={F0:.2f})')\n",
    "        ax.set_title('Step 0: Initial Prediction')\n",
    "        mse_initial = np.mean((y_viz - F0) ** 2)\n",
    "        ax.text(0.05, 0.95, f'MSE: {mse_initial:.3f}', transform=ax.transAxes, \n",
    "               bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    else:\n",
    "        # Boosting steps\n",
    "        step = i - 1\n",
    "        ax.scatter(X_viz.ravel(), y_viz, alpha=0.6, color='blue', s=30, label='True values')\n",
    "        ax.plot(X_viz.ravel(), predictions_history[i], color='red', linewidth=2, \n",
    "               label=f'Prediction after {i} trees')\n",
    "        ax.set_title(f'Step {i}: After Tree {i}')\n",
    "        mse = np.mean((y_viz - predictions_history[i]) ** 2)\n",
    "        ax.text(0.05, 0.95, f'MSE: {mse:.3f}', transform=ax.transAxes,\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot MSE evolution\n",
    "mse_evolution = []\n",
    "for pred in predictions_history:\n",
    "    mse = np.mean((y_viz - pred) ** 2)\n",
    "    mse_evolution.append(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(mse_evolution)), mse_evolution, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Boosting Step')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE Reduction During Gradient Boosting')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(len(mse_evolution)))\n",
    "for i, mse in enumerate(mse_evolution):\n",
    "    plt.annotate(f'{mse:.3f}', (i, mse), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"  1. Each tree focuses on fixing previous mistakes (residuals)\")\n",
    "print(\"  2. MSE consistently decreases with each boosting step\")\n",
    "print(\"  3. Final prediction combines all weak learners\")\n",
    "print(\"  4. Learning rate controls how much each tree contributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgboost-deep-dive",
   "metadata": {},
   "source": [
    "## 3. XGBoost: Extreme Gradient Boosting\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1. Second-Order Optimization\n",
    "Traditional gradient boosting uses only first derivatives (gradients). XGBoost uses **both gradients and Hessians** (second derivatives) for more accurate optimization.\n",
    "\n",
    "**Objective Function**:\n",
    "$$\\text{Obj} = \\sum_{i=1}^n L(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)$$\n",
    "\n",
    "Where $\\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda ||w||^2$ (regularization)\n",
    "\n",
    "#### 2. Regularization Terms\n",
    "- **$\\gamma$**: Minimum loss reduction for split (complexity control)\n",
    "- **$\\lambda$**: L2 regularization on leaf weights\n",
    "- **$\\alpha$**: L1 regularization on leaf weights\n",
    "\n",
    "#### 3. Advanced System Features\n",
    "- **Column Block Structure**: Efficient memory access\n",
    "- **Cache-Aware Access**: Optimized data layout\n",
    "- **Out-of-Core Computing**: Handle datasets larger than memory\n",
    "- **Distributed Computing**: Scale across multiple machines\n",
    "\n",
    "#### 4. Missing Value Handling\n",
    "XGBoost automatically learns the optimal direction for missing values during training.\n",
    "\n",
    "### XGBoost vs Traditional Gradient Boosting\n",
    "\n",
    "| Feature | Traditional GB | XGBoost |\n",
    "|---------|----------------|----------|\n",
    "| Optimization | First-order | Second-order |\n",
    "| Regularization | Limited | Built-in L1/L2 |\n",
    "| Missing Values | Manual handling | Automatic |\n",
    "| Parallelization | Sequential | Parallel tree construction |\n",
    "| Memory Usage | High | Optimized |\n",
    "| Speed | Slower | Much faster |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost comprehensive demonstration\n",
    "print(\"=== XGBOOST COMPREHENSIVE DEMO ===\")\n",
    "print()\n",
    "\n",
    "# Load dataset\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "print(f\"Dataset: Titanic Survival Prediction\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_train)}\")\n",
    "print()\n",
    "\n",
    "# Create XGBoost datasets\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names)\n",
    "\n",
    "print(\"XGBoost DMatrix objects created\")\n",
    "print(f\"  Training DMatrix: {dtrain.num_row()} rows, {dtrain.num_col()} cols\")\n",
    "print(f\"  Test DMatrix: {dtest.num_row()} rows, {dtest.num_col()} cols\")\n",
    "print()\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'eval_metric': 'logloss',        # Evaluation metric\n",
    "    'max_depth': 6,                  # Maximum tree depth\n",
    "    'eta': 0.1,                      # Learning rate\n",
    "    'subsample': 0.8,                # Sample ratio for each tree\n",
    "    'colsample_bytree': 0.8,         # Feature sampling ratio\n",
    "    'reg_alpha': 0.1,                # L1 regularization\n",
    "    'reg_lambda': 1.0,               # L2 regularization\n",
    "    'seed': 42                       # Random seed\n",
    "}\n",
    "\n",
    "print(\"XGBoost Parameters:\")\n",
    "for param, value in params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print()\n",
    "\n",
    "# Train XGBoost model with evaluation\n",
    "evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "evals_result = {}\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=200,\n",
    "    evals=evals,\n",
    "    evals_result=evals_result,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"Best iteration: {xgb_model.best_iteration}\")\n",
    "print(f\"Best score: {xgb_model.best_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = xgb_model.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"XGBoost Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "print(f\"  AUC: {auc:.3f}\")\n",
    "print(f\"  Trees used: {xgb_model.best_iteration + 1}\")\n",
    "print(f\"  Training time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze XGBoost training process\n",
    "print(\"=== XGBOOST TRAINING ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss curves\n",
    "train_loss = evals_result['train']['logloss']\n",
    "eval_loss = evals_result['eval']['logloss']\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "axes[0].plot(epochs, train_loss, label='Training Loss', linewidth=2)\n",
    "axes[0].plot(epochs, eval_loss, label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(x=xgb_model.best_iteration, color='red', linestyle='--', \n",
    "               label=f'Best Iteration ({xgb_model.best_iteration})')\n",
    "axes[0].set_xlabel('Boosting Round')\n",
    "axes[0].set_ylabel('Log Loss')\n",
    "axes[0].set_title('XGBoost Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance\n",
    "importance_dict = xgb_model.get_score(importance_type='weight')\n",
    "features = list(importance_dict.keys())\n",
    "importances = list(importance_dict.values())\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = np.argsort(importances)\n",
    "sorted_features = [features[i] for i in sorted_idx]\n",
    "sorted_importances = [importances[i] for i in sorted_idx]\n",
    "\n",
    "axes[1].barh(range(len(sorted_features)), sorted_importances)\n",
    "axes[1].set_yticks(range(len(sorted_features)))\n",
    "axes[1].set_yticklabels(sorted_features)\n",
    "axes[1].set_xlabel('Feature Importance (Weight)')\n",
    "axes[1].set_title('XGBoost Feature Importance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"Training Analysis:\")\n",
    "print(f\"  Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"  Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"  Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"  Early stopping triggered: {len(train_loss) < 200}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "top_features = list(zip(sorted_features[-5:], sorted_importances[-5:]))[::-1]\n",
    "for i, (feature, importance) in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feature}: {importance}\")\n",
    "print()\n",
    "\n",
    "# Model statistics\n",
    "print(\"Model Statistics:\")\n",
    "print(f\"  Total trees: {xgb_model.best_iteration + 1}\")\n",
    "print(f\"  Avg tree depth: ~{params['max_depth']} (max allowed)\")\n",
    "print(f\"  Learning rate: {params['eta']}\")\n",
    "print(f\"  L1 regularization: {params['reg_alpha']}\")\n",
    "print(f\"  L2 regularization: {params['reg_lambda']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightgbm-deep-dive",
   "metadata": {},
   "source": [
    "## 4. LightGBM: Microsoft's Speed Champion\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1. Leaf-wise Tree Growth\n",
    "- **Traditional (Level-wise)**: Grows trees level by level\n",
    "- **LightGBM (Leaf-wise)**: Grows by adding leaves that reduce loss most\n",
    "- **Result**: Faster convergence, better accuracy with same number of leaves\n",
    "\n",
    "#### 2. Gradient-based One-Side Sampling (GOSS)\n",
    "- Keeps all data points with large gradients (important for learning)\n",
    "- Randomly samples data points with small gradients\n",
    "- **Result**: Same accuracy with much less data\n",
    "\n",
    "#### 3. Exclusive Feature Bundling (EFB)\n",
    "- Bundles sparse features together\n",
    "- Reduces number of features without losing information\n",
    "- **Result**: Faster training and lower memory usage\n",
    "\n",
    "#### 4. Histogram-based Algorithm\n",
    "- Buckets continuous features into histograms\n",
    "- Faster split finding compared to pre-sorted algorithms\n",
    "- **Result**: Significant speed improvement\n",
    "\n",
    "### LightGBM Advantages\n",
    "\n",
    "‚úÖ **Speed**: 10x faster than XGBoost in many cases\n",
    "‚úÖ **Memory**: Lower memory consumption\n",
    "‚úÖ **Accuracy**: Often better results with default parameters\n",
    "‚úÖ **GPU Support**: Native GPU acceleration\n",
    "‚úÖ **Categorical Features**: Built-in categorical feature support\n",
    "\n",
    "### When to Use LightGBM\n",
    "- Large datasets (>10k samples)\n",
    "- Speed is critical\n",
    "- Limited computational resources\n",
    "- Good default performance needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightgbm-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM comprehensive demonstration\n",
    "print(\"=== LIGHTGBM COMPREHENSIVE DEMO ===\")\n",
    "print()\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train, feature_name=feature_names)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, feature_name=feature_names, reference=train_data)\n",
    "\n",
    "print(\"LightGBM Dataset objects created\")\n",
    "print(f\"  Training dataset: {train_data.num_data()} samples\")\n",
    "print(f\"  Validation dataset: {valid_data.num_data()} samples\")\n",
    "print()\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'binary',           # Binary classification\n",
    "    'metric': 'binary_logloss',      # Evaluation metric\n",
    "    'boosting_type': 'gbdt',         # Gradient Boosting Decision Tree\n",
    "    'num_leaves': 31,                # Number of leaves in one tree\n",
    "    'learning_rate': 0.1,            # Shrinkage rate\n",
    "    'feature_fraction': 0.8,         # Feature sampling ratio\n",
    "    'bagging_fraction': 0.8,         # Data sampling ratio\n",
    "    'bagging_freq': 5,               # Bagging frequency\n",
    "    'reg_alpha': 0.1,                # L1 regularization\n",
    "    'reg_lambda': 1.0,               # L2 regularization\n",
    "    'min_child_samples': 20,         # Minimum samples in leaf\n",
    "    'verbose': -1,                   # Suppress warnings\n",
    "    'seed': 42                       # Random seed\n",
    "}\n",
    "\n",
    "print(\"LightGBM Parameters:\")\n",
    "for param, value in lgb_params.items():\n",
    "    if param != 'verbose':\n",
    "        print(f\"  {param}: {value}\")\n",
    "print()\n",
    "\n",
    "# Train LightGBM model\n",
    "print(\"Training LightGBM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "evals_result_lgb = {}\n",
    "lgb_model = lgb.train(\n",
    "    params=lgb_params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'eval'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.record_evaluation(evals_result_lgb)\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_time_lgb = time.time() - start_time\n",
    "print(f\"Training completed in {training_time_lgb:.2f} seconds\")\n",
    "print(f\"Best iteration: {lgb_model.best_iteration}\")\n",
    "print(f\"Best score: {lgb_model.best_score['eval']['binary_logloss']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "y_pred_lgb = (y_pred_proba_lgb > 0.5).astype(int)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "auc_lgb = roc_auc_score(y_test, y_pred_proba_lgb)\n",
    "\n",
    "print(f\"LightGBM Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_lgb:.3f}\")\n",
    "print(f\"  AUC: {auc_lgb:.3f}\")\n",
    "print(f\"  Trees used: {lgb_model.best_iteration}\")\n",
    "print(f\"  Training time: {training_time_lgb:.2f}s\")\n",
    "print()\n",
    "\n",
    "# Compare with XGBoost\n",
    "print(\"Comparison with XGBoost:\")\n",
    "print(f\"  Speed improvement: {training_time / training_time_lgb:.1f}x faster\")\n",
    "print(f\"  Accuracy difference: {accuracy_lgb - accuracy:+.3f}\")\n",
    "print(f\"  AUC difference: {auc_lgb - auc:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightgbm-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze LightGBM specific features\n",
    "print(\"=== LIGHTGBM SPECIFIC ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# Plot training curves comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training curves comparison\n",
    "xgb_train_loss = evals_result['train']['logloss']\n",
    "xgb_eval_loss = evals_result['eval']['logloss']\n",
    "lgb_train_loss = evals_result_lgb['train']['binary_logloss']\n",
    "lgb_eval_loss = evals_result_lgb['eval']['binary_logloss']\n",
    "\n",
    "# Align the lengths for comparison\n",
    "min_len = min(len(xgb_eval_loss), len(lgb_eval_loss))\n",
    "epochs_comp = range(min_len)\n",
    "\n",
    "axes[0,0].plot(epochs_comp, xgb_eval_loss[:min_len], label='XGBoost', linewidth=2)\n",
    "axes[0,0].plot(epochs_comp, lgb_eval_loss[:min_len], label='LightGBM', linewidth=2)\n",
    "axes[0,0].set_xlabel('Boosting Round')\n",
    "axes[0,0].set_ylabel('Validation Loss')\n",
    "axes[0,0].set_title('Training Speed Comparison')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance comparison\n",
    "lgb_importance = lgb_model.feature_importance(importance_type='split')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'lgb_importance': lgb_importance\n",
    "})\n",
    "\n",
    "# XGBoost importance (align with LightGBM features)\n",
    "xgb_imp_dict = xgb_model.get_score(importance_type='weight')\n",
    "feature_importance_df['xgb_importance'] = [\n",
    "    xgb_imp_dict.get(f, 0) for f in feature_names\n",
    "]\n",
    "\n",
    "# Sort by LightGBM importance\n",
    "feature_importance_df = feature_importance_df.sort_values('lgb_importance', ascending=True)\n",
    "top_features_df = feature_importance_df.tail(8)  # Top 8 features\n",
    "\n",
    "x_pos = np.arange(len(top_features_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,1].barh(x_pos - width/2, top_features_df['xgb_importance'], width, \n",
    "              label='XGBoost', alpha=0.8)\n",
    "axes[0,1].barh(x_pos + width/2, top_features_df['lgb_importance'], width, \n",
    "              label='LightGBM', alpha=0.8)\n",
    "axes[0,1].set_yticks(x_pos)\n",
    "axes[0,1].set_yticklabels(top_features_df['feature'])\n",
    "axes[0,1].set_xlabel('Feature Importance')\n",
    "axes[0,1].set_title('Feature Importance Comparison')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Tree structure comparison (leaf-wise vs level-wise visualization)\n",
    "# Create synthetic tree structures for visualization\n",
    "level_wise_x = [0, -1, 1, -1.5, -0.5, 0.5, 1.5]\n",
    "level_wise_y = [3, 2, 2, 1, 1, 1, 1]\n",
    "leaf_wise_x = [0, -1, -1.5, -1.7, -1.3, -0.8, -0.6]\n",
    "leaf_wise_y = [4, 3, 2, 1, 1, 1, 1]\n",
    "\n",
    "axes[1,0].scatter(level_wise_x, level_wise_y, s=100, c='blue', alpha=0.7)\n",
    "for i in range(len(level_wise_x)-1):\n",
    "    if i < 2:  # Root to level 1\n",
    "        axes[1,0].plot([level_wise_x[0], level_wise_x[i+1]], \n",
    "                      [level_wise_y[0], level_wise_y[i+1]], 'b-', alpha=0.7)\n",
    "    elif i < 6:  # Level 1 to level 2\n",
    "        parent_idx = (i-3)//2 + 1\n",
    "        axes[1,0].plot([level_wise_x[parent_idx], level_wise_x[i+1]], \n",
    "                      [level_wise_y[parent_idx], level_wise_y[i+1]], 'b-', alpha=0.7)\n",
    "\n",
    "axes[1,0].set_title('XGBoost: Level-wise Growth')\n",
    "axes[1,0].set_xlim(-2, 2)\n",
    "axes[1,0].set_ylim(0.5, 4.5)\n",
    "axes[1,0].set_xticks([])\n",
    "axes[1,0].set_yticks([])\n",
    "\n",
    "axes[1,1].scatter(leaf_wise_x, leaf_wise_y, s=100, c='green', alpha=0.7)\n",
    "# Draw connections for leaf-wise growth\n",
    "connections = [(0,1), (1,2), (2,3), (2,4), (1,5), (1,6)]\n",
    "for parent, child in connections:\n",
    "    axes[1,1].plot([leaf_wise_x[parent], leaf_wise_x[child]], \n",
    "                  [leaf_wise_y[parent], leaf_wise_y[child]], 'g-', alpha=0.7)\n",
    "\n",
    "axes[1,1].set_title('LightGBM: Leaf-wise Growth')\n",
    "axes[1,1].set_xlim(-2, 0.5)\n",
    "axes[1,1].set_ylim(0.5, 4.5)\n",
    "axes[1,1].set_xticks([])\n",
    "axes[1,1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Differences Observed:\")\n",
    "print(f\"  Training Speed: LightGBM is {training_time/training_time_lgb:.1f}x faster\")\n",
    "print(f\"  Convergence: LightGBM reached best score in {lgb_model.best_iteration} iterations\")\n",
    "print(f\"  Memory Usage: LightGBM typically uses ~50% less memory\")\n",
    "print(f\"  Tree Growth: Leaf-wise (LightGBM) vs Level-wise (XGBoost)\")\n",
    "print(f\"  Feature Importance: Similar patterns but different scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catboost-deep-dive",
   "metadata": {},
   "source": [
    "## 5. CatBoost: The Categorical Champion\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1. Ordered Boosting\n",
    "Traditional boosting suffers from **prediction shift** - using the same data for both training and calculating residuals.\n",
    "\n",
    "**CatBoost Solution**: \n",
    "- Uses different random permutations of training data\n",
    "- For each sample, uses only \"past\" samples to calculate residuals\n",
    "- **Result**: Reduces overfitting, better generalization\n",
    "\n",
    "#### 2. Native Categorical Feature Handling\n",
    "Most algorithms require manual encoding (one-hot, label encoding). CatBoost handles categoricals automatically.\n",
    "\n",
    "**Categorical Feature Processing**:\n",
    "- **Target Statistics**: Uses target mean for categorical encoding\n",
    "- **Combinations**: Creates combinations of categorical features\n",
    "- **Ordered TS**: Prevents target leakage in time series\n",
    "\n",
    "#### 3. Symmetric Trees\n",
    "- All trees are balanced and symmetric\n",
    "- Same feature and split value used at each level\n",
    "- **Result**: Faster prediction, better CPU cache usage\n",
    "\n",
    "#### 4. GPU Acceleration\n",
    "- Native GPU support with optimized algorithms\n",
    "- Multi-GPU training support\n",
    "- Optimized for NVIDIA GPUs\n",
    "\n",
    "### CatBoost Advantages\n",
    "\n",
    "‚úÖ **Categorical Handling**: No manual preprocessing needed\n",
    "‚úÖ **Robustness**: Less prone to overfitting\n",
    "‚úÖ **Ease of Use**: Great default parameters\n",
    "‚úÖ **Interpretability**: Built-in feature importance and visualization\n",
    "‚úÖ **Production Ready**: Fast inference, model compression\n",
    "\n",
    "### When to Use CatBoost\n",
    "- Many categorical features\n",
    "- Mixed data types (numerical + categorical)\n",
    "- Time series data\n",
    "- Want minimal preprocessing\n",
    "- Need robust performance with defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catboost-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost comprehensive demonstration\n",
    "print(\"=== CATBOOST COMPREHENSIVE DEMO ===\")\n",
    "print()\n",
    "\n",
    "# For CatBoost, let's create a dataset with explicit categorical features\n",
    "# We'll use the original Titanic data with categorical columns\n",
    "\n",
    "# Load raw titanic data to demonstrate categorical handling\n",
    "titanic_raw = pd.read_csv('../datasets/titanic.csv')\n",
    "print(f\"Raw Titanic dataset shape: {titanic_raw.shape}\")\n",
    "print(f\"Columns: {list(titanic_raw.columns)}\")\n",
    "print()\n",
    "\n",
    "# Prepare data with categorical features\n",
    "# Keep some categorical columns as-is for CatBoost\n",
    "cat_features_df = titanic_raw.copy()\n",
    "cat_features_df = cat_features_df.drop(['Name', 'Ticket'], axis=1)  # Remove high cardinality\n",
    "cat_features_df['Cabin'] = cat_features_df['Cabin'].fillna('Unknown')\n",
    "cat_features_df['Age'] = cat_features_df['Age'].fillna(cat_features_df['Age'].median())\n",
    "cat_features_df['Embarked'] = cat_features_df['Embarked'].fillna('S')\n",
    "cat_features_df['Fare'] = cat_features_df['Fare'].fillna(cat_features_df['Fare'].median())\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = ['Sex', 'Embarked', 'Cabin']\n",
    "categorical_indices = [cat_features_df.columns.get_loc(col) for col in categorical_columns if col in cat_features_df.columns]\n",
    "\n",
    "print(f\"Categorical columns: {categorical_columns}\")\n",
    "print(f\"Categorical column indices: {categorical_indices}\")\n",
    "\n",
    "# Split target and features\n",
    "y_cat = cat_features_df['Survived'].values\n",
    "X_cat = cat_features_df.drop(['Survived', 'PassengerId'], axis=1)\n",
    "\n",
    "# Update categorical indices after dropping columns\n",
    "categorical_indices = [X_cat.columns.get_loc(col) for col in categorical_columns if col in X_cat.columns]\n",
    "\n",
    "print(f\"Final dataset shape: {X_cat.shape}\")\n",
    "print(f\"Final categorical indices: {categorical_indices}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_cat.shape}\")\n",
    "print(f\"Test set: {X_test_cat.shape}\")\n",
    "print()\n",
    "\n",
    "# Create CatBoost datasets\n",
    "train_pool = Pool(X_train_cat, y_train_cat, cat_features=categorical_indices)\n",
    "test_pool = Pool(X_test_cat, y_test_cat, cat_features=categorical_indices)\n",
    "\n",
    "print(\"CatBoost Pool objects created\")\n",
    "print(f\"  Training pool: {train_pool.num_row()} samples\")\n",
    "print(f\"  Test pool: {test_pool.num_row()} samples\")\n",
    "print(f\"  Categorical features: {len(categorical_indices)}\")\n",
    "print()\n",
    "\n",
    "# CatBoost model\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3.0,\n",
    "    bootstrap_type='Bernoulli',\n",
    "    subsample=0.8,\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training CatBoost model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with automatic categorical handling\n",
    "catboost_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "training_time_cat = time.time() - start_time\n",
    "print(f\"Training completed in {training_time_cat:.2f} seconds\")\n",
    "print(f\"Best iteration: {catboost_model.get_best_iteration()}\")\n",
    "print(f\"Best score: {catboost_model.get_best_score()['validation']['Logloss']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba_cat = catboost_model.predict_proba(test_pool)[:, 1]\n",
    "y_pred_cat = catboost_model.predict(test_pool)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_cat = accuracy_score(y_test_cat, y_pred_cat)\n",
    "auc_cat = roc_auc_score(y_test_cat, y_pred_proba_cat)\n",
    "\n",
    "print(f\"CatBoost Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_cat:.3f}\")\n",
    "print(f\"  AUC: {auc_cat:.3f}\")\n",
    "print(f\"  Trees used: {catboost_model.get_best_iteration()}\")\n",
    "print(f\"  Training time: {training_time_cat:.2f}s\")\n",
    "print()\n",
    "\n",
    "print(\"Categorical Feature Handling Demo:\")\n",
    "for col in categorical_columns:\n",
    "    if col in X_cat.columns:\n",
    "        unique_values = X_cat[col].nunique()\n",
    "        print(f\"  {col}: {unique_values} unique values\")\n",
    "        print(f\"    Sample values: {list(X_cat[col].unique()[:5])}\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ CatBoost handled all categorical features automatically!\")\n",
    "print(\"   No manual encoding (one-hot, label) was required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-way-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive three-way comparison\n",
    "print(\"=== COMPREHENSIVE THREE-WAY COMPARISON ===\")\n",
    "print()\n",
    "\n",
    "# Performance comparison table\n",
    "comparison_data = {\n",
    "    'Algorithm': ['XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'Accuracy': [accuracy, accuracy_lgb, accuracy_cat],\n",
    "    'AUC': [auc, auc_lgb, auc_cat],\n",
    "    'Training Time (s)': [training_time, training_time_lgb, training_time_cat],\n",
    "    'Trees Used': [xgb_model.best_iteration + 1, lgb_model.best_iteration, catboost_model.get_best_iteration()],\n",
    "    'Best Validation Loss': [\n",
    "        min(evals_result['eval']['logloss']),\n",
    "        min(evals_result_lgb['eval']['binary_logloss']),\n",
    "        catboost_model.get_best_score()['validation']['Logloss']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))\n",
    "print()\n",
    "\n",
    "# Speed comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training time comparison\n",
    "algorithms = comparison_df['Algorithm']\n",
    "times = comparison_df['Training Time (s)']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "bars = axes[0,0].bar(algorithms, times, color=colors, alpha=0.7)\n",
    "axes[0,0].set_ylabel('Training Time (seconds)')\n",
    "axes[0,0].set_title('Training Speed Comparison')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                  f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracies = comparison_df['Accuracy']\n",
    "bars = axes[0,1].bar(algorithms, accuracies, color=colors, alpha=0.7)\n",
    "axes[0,1].set_ylabel('Accuracy')\n",
    "axes[0,1].set_title('Accuracy Comparison')\n",
    "axes[0,1].set_ylim([0.78, 0.84])\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                  f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# AUC comparison\n",
    "aucs = comparison_df['AUC']\n",
    "bars = axes[1,0].bar(algorithms, aucs, color=colors, alpha=0.7)\n",
    "axes[1,0].set_ylabel('AUC')\n",
    "axes[1,0].set_title('AUC Comparison')\n",
    "axes[1,0].set_ylim([0.84, 0.90])\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, auc_val in zip(bars, aucs):\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                  f'{auc_val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Trees used comparison\n",
    "trees = comparison_df['Trees Used']\n",
    "bars = axes[1,1].bar(algorithms, trees, color=colors, alpha=0.7)\n",
    "axes[1,1].set_ylabel('Number of Trees')\n",
    "axes[1,1].set_title('Trees Used (Early Stopping)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, tree_count in zip(bars, trees):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                  f'{int(tree_count)}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(\"Key Insights from Comparison:\")\n",
    "print()\n",
    "\n",
    "# Find best performer in each category\n",
    "fastest = comparison_df.loc[comparison_df['Training Time (s)'].idxmin(), 'Algorithm']\n",
    "most_accurate = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Algorithm']\n",
    "best_auc = comparison_df.loc[comparison_df['AUC'].idxmax(), 'Algorithm']\n",
    "fewest_trees = comparison_df.loc[comparison_df['Trees Used'].idxmin(), 'Algorithm']\n",
    "\n",
    "print(f\"üöÄ Fastest Training: {fastest} ({comparison_df.loc[comparison_df['Algorithm']==fastest, 'Training Time (s)'].values[0]:.2f}s)\")\n",
    "print(f\"üéØ Best Accuracy: {most_accurate} ({comparison_df.loc[comparison_df['Algorithm']==most_accurate, 'Accuracy'].values[0]:.3f})\")\n",
    "print(f\"üìä Best AUC: {best_auc} ({comparison_df.loc[comparison_df['Algorithm']==best_auc, 'AUC'].values[0]:.3f})\")\n",
    "print(f\"üå≥ Most Efficient: {fewest_trees} ({int(comparison_df.loc[comparison_df['Algorithm']==fewest_trees, 'Trees Used'].values[0])} trees)\")\n",
    "print()\n",
    "\n",
    "print(\"Algorithm Characteristics:\")\n",
    "print(\"  XGBoost: üéØ Balanced performance, excellent for competitions\")\n",
    "print(\"  LightGBM: ‚ö° Speed champion, great for large datasets\")\n",
    "print(\"  CatBoost: üîß Handles categoricals natively, robust defaults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter-tuning",
   "metadata": {},
   "source": [
    "## 6. Advanced Hyperparameter Tuning\n",
    "\n",
    "### Key Hyperparameters by Algorithm\n",
    "\n",
    "#### XGBoost Important Parameters\n",
    "- **n_estimators**: Number of boosting rounds (100-1000)\n",
    "- **max_depth**: Maximum tree depth (3-10)\n",
    "- **learning_rate**: Step size shrinkage (0.01-0.3)\n",
    "- **subsample**: Sample ratio of training instances (0.5-1.0)\n",
    "- **colsample_bytree**: Sample ratio of features (0.5-1.0)\n",
    "- **reg_alpha/reg_lambda**: L1/L2 regularization (0-10)\n",
    "\n",
    "#### LightGBM Important Parameters\n",
    "- **num_leaves**: Maximum leaves in one tree (10-300)\n",
    "- **min_child_samples**: Minimum samples in leaf (5-100)\n",
    "- **feature_fraction**: Feature sampling ratio (0.5-1.0)\n",
    "- **bagging_fraction**: Data sampling ratio (0.5-1.0)\n",
    "- **reg_alpha/reg_lambda**: L1/L2 regularization (0-10)\n",
    "\n",
    "#### CatBoost Important Parameters\n",
    "- **iterations**: Number of boosting rounds (100-1000)\n",
    "- **depth**: Tree depth (4-10)\n",
    "- **learning_rate**: Step size (0.01-0.3)\n",
    "- **l2_leaf_reg**: L2 regularization (1-10)\n",
    "- **bootstrap_type**: Sampling method (Bayesian, Bernoulli)\n",
    "\n",
    "### Tuning Strategies\n",
    "\n",
    "1. **Sequential Tuning**: Tune parameters in groups\n",
    "2. **Bayesian Optimization**: Use libraries like Optuna\n",
    "3. **Random Search**: Often better than grid search\n",
    "4. **Early Stopping**: Prevent overfitting during tuning\n",
    "5. **Cross-Validation**: Robust performance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter-tuning-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced hyperparameter tuning demonstration\n",
    "print(\"=== ADVANCED HYPERPARAMETER TUNING ===\")\n",
    "print()\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define parameter distributions for each algorithm\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(3, 8),\n",
    "    'learning_rate': uniform(0.01, 0.29),\n",
    "    'subsample': uniform(0.7, 0.3),\n",
    "    'colsample_bytree': uniform(0.7, 0.3),\n",
    "    'reg_alpha': uniform(0, 2),\n",
    "    'reg_lambda': uniform(0, 2)\n",
    "}\n",
    "\n",
    "lgb_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'num_leaves': randint(20, 100),\n",
    "    'learning_rate': uniform(0.01, 0.29),\n",
    "    'feature_fraction': uniform(0.7, 0.3),\n",
    "    'bagging_fraction': uniform(0.7, 0.3),\n",
    "    'reg_alpha': uniform(0, 2),\n",
    "    'reg_lambda': uniform(0, 2),\n",
    "    'min_child_samples': randint(10, 50)\n",
    "}\n",
    "\n",
    "# For demonstration, we'll do quick tuning with fewer iterations\n",
    "print(\"Performing hyperparameter tuning (quick demo with 20 iterations)...\")\n",
    "print()\n",
    "\n",
    "# XGBoost tuning\n",
    "print(\"üîß Tuning XGBoost...\")\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, verbose=0, eval_metric='logloss')\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_classifier, \n",
    "    xgb_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "print(f\"   Best XGBoost score: {xgb_search.best_score_:.4f}\")\n",
    "print(f\"   Best params: {xgb_search.best_params_}\")\n",
    "print()\n",
    "\n",
    "# LightGBM tuning\n",
    "print(\"üîß Tuning LightGBM...\")\n",
    "lgb_classifier = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb_classifier, \n",
    "    lgb_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lgb_search.fit(X_train, y_train)\n",
    "print(f\"   Best LightGBM score: {lgb_search.best_score_:.4f}\")\n",
    "print(f\"   Best params: {lgb_search.best_params_}\")\n",
    "print()\n",
    "\n",
    "# Evaluate tuned models\n",
    "print(\"Evaluating tuned models on test set:\")\n",
    "\n",
    "# XGBoost tuned\n",
    "xgb_tuned_pred = xgb_search.predict(X_test)\n",
    "xgb_tuned_proba = xgb_search.predict_proba(X_test)[:, 1]\n",
    "xgb_tuned_acc = accuracy_score(y_test, xgb_tuned_pred)\n",
    "xgb_tuned_auc = roc_auc_score(y_test, xgb_tuned_proba)\n",
    "\n",
    "# LightGBM tuned\n",
    "lgb_tuned_pred = lgb_search.predict(X_test)\n",
    "lgb_tuned_proba = lgb_search.predict_proba(X_test)[:, 1]\n",
    "lgb_tuned_acc = accuracy_score(y_test, lgb_tuned_pred)\n",
    "lgb_tuned_auc = roc_auc_score(y_test, lgb_tuned_proba)\n",
    "\n",
    "print()\n",
    "print(\"Tuning Results Comparison:\")\n",
    "print(f\"Algorithm    | Default AUC | Tuned AUC | Improvement\")\n",
    "print(f\"-------------|-------------|-----------|------------\")\n",
    "print(f\"XGBoost      | {auc:11.3f} | {xgb_tuned_auc:9.3f} | {xgb_tuned_auc - auc:+10.3f}\")\n",
    "print(f\"LightGBM     | {auc_lgb:11.3f} | {lgb_tuned_auc:9.3f} | {lgb_tuned_auc - auc_lgb:+10.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"üí° Hyperparameter Tuning Insights:\")\n",
    "print(f\"   ‚Ä¢ XGBoost improvement: {((xgb_tuned_auc - auc) / auc * 100):+.1f}%\")\n",
    "print(f\"   ‚Ä¢ LightGBM improvement: {((lgb_tuned_auc - auc_lgb) / auc_lgb * 100):+.1f}%\")\n",
    "print(f\"   ‚Ä¢ Tuning is crucial for optimal performance!\")\n",
    "print(f\"   ‚Ä¢ Even small improvements matter in competitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-tips",
   "metadata": {},
   "source": [
    "## 7. Practical Tips and Best Practices\n",
    "\n",
    "### üöÄ Getting Started\n",
    "\n",
    "#### 1. Algorithm Selection Guide\n",
    "- **Start with LightGBM**: Often best default performance\n",
    "- **Use CatBoost**: If many categorical features\n",
    "- **Choose XGBoost**: For maximum tunability\n",
    "\n",
    "#### 2. Default Parameters Strategy\n",
    "- Begin with library defaults\n",
    "- Focus on learning_rate and n_estimators first\n",
    "- Use early stopping to find optimal iterations\n",
    "- Then tune regularization parameters\n",
    "\n",
    "### ‚ö° Performance Optimization\n",
    "\n",
    "#### 1. Training Speed\n",
    "- **LightGBM**: Fastest for large datasets\n",
    "- **Parallel training**: Use all CPU cores\n",
    "- **GPU acceleration**: CatBoost > LightGBM > XGBoost\n",
    "- **Early stopping**: Prevents overtraining\n",
    "\n",
    "#### 2. Memory Optimization\n",
    "- Use appropriate data types (int8, float32)\n",
    "- Feature selection to reduce dimensions\n",
    "- Batch processing for very large datasets\n",
    "\n",
    "### üéØ Competition Strategies\n",
    "\n",
    "#### 1. Feature Engineering\n",
    "- **Interaction features**: Manual or automated\n",
    "- **Target encoding**: For high-cardinality categoricals\n",
    "- **Time-based features**: Lags, rolling statistics\n",
    "- **Binning**: Convert continuous to categorical\n",
    "\n",
    "#### 2. Ensemble Methods\n",
    "- Combine all three algorithms\n",
    "- Use different random seeds\n",
    "- Stack with meta-learners\n",
    "- Blend predictions with optimal weights\n",
    "\n",
    "### üõ°Ô∏è Avoiding Overfitting\n",
    "\n",
    "#### 1. Regularization\n",
    "- Use L1/L2 regularization\n",
    "- Limit tree depth and leaves\n",
    "- Increase minimum samples per leaf\n",
    "- Use feature and sample subsampling\n",
    "\n",
    "#### 2. Validation Strategy\n",
    "- Time-based splits for time series\n",
    "- Stratified K-fold for imbalanced data\n",
    "- Group K-fold for grouped data\n",
    "- Hold-out validation set\n",
    "\n",
    "### üîß Production Deployment\n",
    "\n",
    "#### 1. Model Optimization\n",
    "- Model compression and pruning\n",
    "- Convert to ONNX for faster inference\n",
    "- Use appropriate prediction batch sizes\n",
    "- Cache frequent predictions\n",
    "\n",
    "#### 2. Monitoring\n",
    "- Track prediction distributions\n",
    "- Monitor feature importance drift\n",
    "- Set up automated retraining\n",
    "- A/B testing for model updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering demonstration for boosting algorithms\n",
    "print(\"=== FEATURE ENGINEERING FOR BOOSTING ===\")\n",
    "print()\n",
    "\n",
    "# Load original data for feature engineering\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig, _ = load_titanic_data()\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame(X_train_orig)\n",
    "test_df = pd.DataFrame(X_test_orig)\n",
    "\n",
    "print(f\"Original features: {train_df.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# 1. Interaction Features\n",
    "print(\"Creating interaction features...\")\n",
    "interaction_features = []\n",
    "\n",
    "# Select top numerical features for interactions\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "top_numerical = numerical_cols[:4]  # Use top 4 to avoid explosion\n",
    "\n",
    "for i, col1 in enumerate(top_numerical):\n",
    "    for col2 in top_numerical[i+1:]:\n",
    "        # Multiplicative interaction\n",
    "        interaction_name = f\"{col1}_x_{col2}\"\n",
    "        train_df[interaction_name] = train_df[col1] * train_df[col2]\n",
    "        test_df[interaction_name] = test_df[col1] * test_df[col2]\n",
    "        interaction_features.append(interaction_name)\n",
    "        \n",
    "        # Ratio interaction (avoid division by zero)\n",
    "        ratio_name = f\"{col1}_div_{col2}\"\n",
    "        train_df[ratio_name] = train_df[col1] / (train_df[col2] + 1e-8)\n",
    "        test_df[ratio_name] = test_df[col1] / (test_df[col2] + 1e-8)\n",
    "        interaction_features.append(ratio_name)\n",
    "\n",
    "print(f\"   Created {len(interaction_features)} interaction features\")\n",
    "\n",
    "# 2. Binning Features\n",
    "print(\"Creating binning features...\")\n",
    "binning_features = []\n",
    "\n",
    "for col in top_numerical[:3]:  # Bin top 3 numerical features\n",
    "    # Equal-width binning\n",
    "    bin_name = f\"{col}_binned\"\n",
    "    train_df[bin_name] = pd.cut(train_df[col], bins=5, labels=False)\n",
    "    test_df[bin_name] = pd.cut(test_df[col], bins=5, labels=False)\n",
    "    binning_features.append(bin_name)\n",
    "    \n",
    "    # Quantile binning\n",
    "    qbin_name = f\"{col}_qbinned\"\n",
    "    train_df[qbin_name] = pd.qcut(train_df[col], q=5, labels=False, duplicates='drop')\n",
    "    test_df[qbin_name] = pd.qcut(test_df[col], q=5, labels=False, duplicates='drop')\n",
    "    binning_features.append(qbin_name)\n",
    "\n",
    "print(f\"   Created {len(binning_features)} binning features\")\n",
    "\n",
    "# 3. Statistical Features\n",
    "print(\"Creating statistical features...\")\n",
    "statistical_features = []\n",
    "\n",
    "# Row-wise statistics\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "train_df['row_mean'] = train_df[numeric_cols].mean(axis=1)\n",
    "train_df['row_std'] = train_df[numeric_cols].std(axis=1)\n",
    "train_df['row_max'] = train_df[numeric_cols].max(axis=1)\n",
    "train_df['row_min'] = train_df[numeric_cols].min(axis=1)\n",
    "\n",
    "test_df['row_mean'] = test_df[numeric_cols].mean(axis=1)\n",
    "test_df['row_std'] = test_df[numeric_cols].std(axis=1)\n",
    "test_df['row_max'] = test_df[numeric_cols].max(axis=1)\n",
    "test_df['row_min'] = test_df[numeric_cols].min(axis=1)\n",
    "\n",
    "statistical_features = ['row_mean', 'row_std', 'row_max', 'row_min']\n",
    "print(f\"   Created {len(statistical_features)} statistical features\")\n",
    "\n",
    "print()\n",
    "print(f\"Total features after engineering: {train_df.shape[1]}\")\n",
    "print(f\"Feature increase: {train_df.shape[1] - X_train_orig.shape[1]} new features\")\n",
    "\n",
    "# Handle any NaN values created during feature engineering\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "print()\n",
    "print(\"Testing engineered features with LightGBM...\")\n",
    "\n",
    "# Compare baseline vs engineered features\n",
    "baseline_model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "baseline_model.fit(X_train_orig, y_train_orig)\n",
    "baseline_pred = baseline_model.predict_proba(X_test_orig)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test_orig, baseline_pred)\n",
    "\n",
    "engineered_model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "engineered_model.fit(train_df, y_train_orig)\n",
    "engineered_pred = engineered_model.predict_proba(test_df)[:, 1]\n",
    "engineered_auc = roc_auc_score(y_test_orig, engineered_pred)\n",
    "\n",
    "print(f\"Baseline AUC (original features): {baseline_auc:.4f}\")\n",
    "print(f\"Engineered AUC (with new features): {engineered_auc:.4f}\")\n",
    "print(f\"Improvement: {engineered_auc - baseline_auc:+.4f} ({((engineered_auc - baseline_auc) / baseline_auc * 100):+.1f}%)\")\n",
    "\n",
    "# Feature importance of new features\n",
    "feature_importance = engineered_model.feature_importance_\n",
    "feature_names_eng = train_df.columns.tolist()\n",
    "\n",
    "# Find top new features\n",
    "new_feature_names = interaction_features + binning_features + statistical_features\n",
    "new_feature_importance = []\n",
    "\n",
    "for name in new_feature_names:\n",
    "    if name in feature_names_eng:\n",
    "        idx = feature_names_eng.index(name)\n",
    "        new_feature_importance.append((name, feature_importance[idx]))\n",
    "\n",
    "new_feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print()\n",
    "print(\"Top 5 Most Important New Features:\")\n",
    "for i, (name, importance) in enumerate(new_feature_importance[:5]):\n",
    "    print(f\"  {i+1}. {name}: {importance}\")\n",
    "\n",
    "print()\n",
    "print(\"üî• Feature Engineering Impact:\")\n",
    "if engineered_auc > baseline_auc:\n",
    "    print(f\"   ‚úÖ Significant improvement with feature engineering!\")\n",
    "    print(f\"   ‚úÖ New features added predictive power\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Minimal improvement - feature engineering needs refinement\")\n",
    "print(f\"   üìä Always validate new features on holdout data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### üéØ What You've Learned\n",
    "\n",
    "1. **Gradient Boosting Evolution**: From AdaBoost to modern implementations\n",
    "2. **XGBoost Mastery**: Second-order optimization and regularization\n",
    "3. **LightGBM Speed**: Leaf-wise growth and histogram optimization\n",
    "4. **CatBoost Robustness**: Ordered boosting and categorical handling\n",
    "5. **Performance Comparison**: When to use which algorithm\n",
    "6. **Hyperparameter Tuning**: Advanced optimization strategies\n",
    "7. **Feature Engineering**: Boosting-specific techniques\n",
    "8. **Production Best Practices**: Deployment and monitoring\n",
    "\n",
    "### üöÄ Algorithm Quick Reference\n",
    "\n",
    "| Use Case | Best Choice | Why |\n",
    "|----------|-------------|-----|\n",
    "| **Kaggle Competition** | XGBoost + LightGBM ensemble | Maximum tunability + speed |\n",
    "| **Large Dataset (>100k)** | LightGBM | Speed and memory efficiency |\n",
    "| **Many Categoricals** | CatBoost | Native categorical handling |\n",
    "| **Production (Speed)** | LightGBM | Fastest inference |\n",
    "| **Production (Stability)** | CatBoost | Robust defaults |\n",
    "| **Research/Experimentation** | XGBoost | Most documentation/examples |\n",
    "| **Time Series** | CatBoost | Ordered boosting prevents leakage |\n",
    "| **Imbalanced Data** | XGBoost | Best class weight handling |\n",
    "\n",
    "### üí° Key Insights\n",
    "\n",
    "#### Algorithm Strengths\n",
    "- **XGBoost**: üèÜ Most battle-tested, excellent for competitions\n",
    "- **LightGBM**: ‚ö° Speed champion, great for large-scale problems\n",
    "- **CatBoost**: üõ°Ô∏è Most robust, handles messy data well\n",
    "\n",
    "#### Common Pitfalls\n",
    "1. **Overfitting**: Use early stopping and validation\n",
    "2. **Feature scaling**: Not required but can help with interpretability\n",
    "3. **Data leakage**: Be careful with time series and grouped data\n",
    "4. **Categorical encoding**: Let CatBoost handle automatically\n",
    "5. **Default parameters**: Always tune for your specific problem\n",
    "\n",
    "### üõ†Ô∏è Best Practices Checklist\n",
    "\n",
    "#### Before Training\n",
    "- [ ] Choose algorithm based on dataset size and features\n",
    "- [ ] Set up proper train/validation/test splits\n",
    "- [ ] Handle missing values appropriately\n",
    "- [ ] Consider feature engineering opportunities\n",
    "\n",
    "#### During Training\n",
    "- [ ] Use early stopping to prevent overfitting\n",
    "- [ ] Monitor both training and validation metrics\n",
    "- [ ] Start with default parameters, then tune systematically\n",
    "- [ ] Use cross-validation for robust estimates\n",
    "\n",
    "#### After Training\n",
    "- [ ] Analyze feature importance for insights\n",
    "- [ ] Test on truly holdout data\n",
    "- [ ] Consider ensemble methods for best performance\n",
    "- [ ] Optimize for production deployment\n",
    "\n",
    "### üéØ Next Steps\n",
    "\n",
    "1. **Practice**: Apply to your own datasets\n",
    "2. **Experiment**: Try different parameter combinations\n",
    "3. **Ensemble**: Combine multiple algorithms\n",
    "4. **Advanced**: Explore custom objectives and metrics\n",
    "5. **Production**: Deploy and monitor your models\n",
    "\n",
    "### üèÜ Competition Tips\n",
    "\n",
    "#### Winning Strategies\n",
    "1. **Feature Engineering**: 70% of performance comes from features\n",
    "2. **Ensembling**: Combine diverse models for best results\n",
    "3. **Cross-Validation**: Use robust validation strategies\n",
    "4. **Hyperparameter Optimization**: Fine-tune everything\n",
    "5. **Domain Knowledge**: Understand your data deeply\n",
    "\n",
    "#### Final Wisdom\n",
    "- **Start simple**: Baseline first, complexity later\n",
    "- **Measure everything**: What gets measured gets improved\n",
    "- **Trust your validation**: Good CV prevents overfitting\n",
    "- **Learn continuously**: These algorithms keep evolving\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a comprehensive understanding of XGBoost, LightGBM, and CatBoost. These algorithms are the backbone of modern machine learning and will serve you well in competitions, research, and production systems.\n",
    "\n",
    "Remember: **The best algorithm is the one that works best for your specific problem!** üöÄ\n",
    "\n",
    "Keep experimenting, keep learning, and most importantly - have fun with these powerful tools! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}