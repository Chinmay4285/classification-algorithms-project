{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification: A Complete Guide\n",
    "\n",
    "Welcome to your comprehensive introduction to **Naive Bayes classification**! This notebook will take you from zero knowledge to understanding one of the most elegant and effective machine learning algorithms.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Mathematical Foundation**: Bayes' theorem and probability concepts\n",
    "2. **Algorithm Intuition**: How Naive Bayes makes predictions\n",
    "3. **Types of Naive Bayes**: Gaussian, Multinomial, and Bernoulli variants\n",
    "4. **Practical Implementation**: Real-world examples with code\n",
    "5. **Advantages & Limitations**: When to use and when to avoid\n",
    "6. **Performance Analysis**: Results on different datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayes-theorem",
   "metadata": {},
   "source": [
    "## 1. Foundation: Bayes' Theorem\n",
    "\n",
    "### What is Bayes' Theorem?\n",
    "\n",
    "Imagine you're a doctor trying to diagnose a patient. You know:\n",
    "- 1% of people have a rare disease\n",
    "- A test is 95% accurate for detecting the disease\n",
    "- The test gives false positives 10% of the time\n",
    "\n",
    "If the test comes back positive, what's the probability the patient actually has the disease?\n",
    "\n",
    "**Bayes' theorem helps us answer this!**\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "- **P(A|B)**: Probability of A given B (what we want to find)\n",
    "- **P(B|A)**: Probability of B given A (likelihood)\n",
    "- **P(A)**: Prior probability of A\n",
    "- **P(B)**: Marginal probability of B\n",
    "\n",
    "### In Machine Learning Context:\n",
    "\n",
    "$$P(Class|Features) = \\frac{P(Features|Class) \\times P(Class)}{P(Features)}$$\n",
    "\n",
    "**Translation**: \n",
    "- \"Given these features, what's the probability of this class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Naive Bayes Classification Tutorial\n",
      "All libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from utils.data_utils import load_titanic_data\n",
    "from utils.evaluation import ModelEvaluator\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[START] Naive Bayes Classification Tutorial\")\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intuition",
   "metadata": {},
   "source": [
    "## 2. Algorithm Intuition: How Does Naive Bayes Think?\n",
    "\n",
    "### Simple Example: Email Spam Detection\n",
    "\n",
    "Let's say you want to classify emails as **Spam** or **Ham** (not spam).\n",
    "\n",
    "**Training Data:**\n",
    "- Spam emails often contain: \"free\", \"money\", \"urgent\"\n",
    "- Ham emails often contain: \"meeting\", \"family\", \"work\"\n",
    "\n",
    "**New Email**: \"Get free money now!\"\n",
    "\n",
    "**Naive Bayes Process:**\n",
    "1. **Calculate P(Spam)** and **P(Ham)** from training data\n",
    "2. **For each word**, calculate:\n",
    "   - P(\"free\" | Spam) vs P(\"free\" | Ham)\n",
    "   - P(\"money\" | Spam) vs P(\"money\" | Ham)\n",
    "3. **Multiply probabilities** (assumes independence - \"naive\")\n",
    "4. **Compare** P(Spam | email) vs P(Ham | email)\n",
    "5. **Choose** the higher probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate the spam detection example\n",
    "print(\"=== SPAM DETECTION EXAMPLE ===\")\n",
    "print()\n",
    "\n",
    "# Simulated training data\n",
    "training_data = {\n",
    "    'Spam': ['free money now', 'get rich quick', 'urgent action required', 'free prize winner'],\n",
    "    'Ham': ['meeting tomorrow', 'family dinner', 'work project', 'grocery shopping']\n",
    "}\n",
    "\n",
    "print(\"Training Data:\")\n",
    "for category, emails in training_data.items():\n",
    "    print(f\"{category}: {emails}\")\n",
    "print()\n",
    "\n",
    "# New email to classify\n",
    "new_email = \"get free money\"\n",
    "print(f\"New email to classify: '{new_email}'\")\n",
    "print()\n",
    "\n",
    "# Calculate word frequencies\n",
    "def count_words(emails):\n",
    "    word_count = {}\n",
    "    total_words = 0\n",
    "    for email in emails:\n",
    "        words = email.split()\n",
    "        for word in words:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "            total_words += 1\n",
    "    return word_count, total_words\n",
    "\n",
    "spam_words, spam_total = count_words(training_data['Spam'])\n",
    "ham_words, ham_total = count_words(training_data['Ham'])\n",
    "\n",
    "print(\"Word frequencies in training data:\")\n",
    "print(f\"Spam words: {spam_words}\")\n",
    "print(f\"Ham words: {ham_words}\")\n",
    "print()\n",
    "\n",
    "# Calculate probabilities\n",
    "prior_spam = len(training_data['Spam']) / (len(training_data['Spam']) + len(training_data['Ham']))\n",
    "prior_ham = 1 - prior_spam\n",
    "\n",
    "print(f\"Prior probabilities:\")\n",
    "print(f\"P(Spam) = {prior_spam:.2f}\")\n",
    "print(f\"P(Ham) = {prior_ham:.2f}\")\n",
    "print()\n",
    "\n",
    "# Calculate likelihood for new email words\n",
    "new_words = new_email.split()\n",
    "print(f\"Calculating likelihood for words: {new_words}\")\n",
    "\n",
    "spam_likelihood = 1\n",
    "ham_likelihood = 1\n",
    "\n",
    "for word in new_words:\n",
    "    # With smoothing to avoid zero probabilities\n",
    "    spam_prob = (spam_words.get(word, 0) + 1) / (spam_total + len(set(list(spam_words.keys()) + list(ham_words.keys()))))\n",
    "    ham_prob = (ham_words.get(word, 0) + 1) / (ham_total + len(set(list(spam_words.keys()) + list(ham_words.keys()))))\n",
    "    \n",
    "    spam_likelihood *= spam_prob\n",
    "    ham_likelihood *= ham_prob\n",
    "    \n",
    "    print(f\"  P('{word}' | Spam) = {spam_prob:.4f}\")\n",
    "    print(f\"  P('{word}' | Ham) = {ham_prob:.4f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Combined likelihoods:\")\n",
    "print(f\"P(words | Spam) = {spam_likelihood:.6f}\")\n",
    "print(f\"P(words | Ham) = {ham_likelihood:.6f}\")\n",
    "print()\n",
    "\n",
    "# Final classification\n",
    "spam_posterior = spam_likelihood * prior_spam\n",
    "ham_posterior = ham_likelihood * prior_ham\n",
    "\n",
    "print(f\"Final probabilities (unnormalized):\")\n",
    "print(f\"P(Spam | email) âˆ {spam_posterior:.8f}\")\n",
    "print(f\"P(Ham | email) âˆ {ham_posterior:.8f}\")\n",
    "print()\n",
    "\n",
    "if spam_posterior > ham_posterior:\n",
    "    print(f\"[RESULT] Email classified as: SPAM\")\n",
    "else:\n",
    "    print(f\"[RESULT] Email classified as: HAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-naive",
   "metadata": {},
   "source": [
    "## 3. Why \"Naive\"?\n",
    "\n",
    "The algorithm is called \"naive\" because it makes a **strong independence assumption**:\n",
    "\n",
    "**Assumption**: All features are **conditionally independent** given the class.\n",
    "\n",
    "### What does this mean?\n",
    "\n",
    "In our spam example:\n",
    "- It assumes that the word \"free\" appearing doesn't affect the probability of \"money\" appearing\n",
    "- In reality, \"free\" and \"money\" often appear together in spam emails\n",
    "\n",
    "### Why is this assumption useful?\n",
    "\n",
    "1. **Simplifies calculation**: Instead of P(word1, word2, word3 | class), we calculate P(word1|class) Ã— P(word2|class) Ã— P(word3|class)\n",
    "2. **Reduces data requirements**: We don't need to see every possible combination of features\n",
    "3. **Fast training and prediction**: Linear time complexity\n",
    "4. **Works surprisingly well**: Despite the \"naive\" assumption, often performs competitively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "types",
   "metadata": {},
   "source": [
    "## 4. Types of Naive Bayes\n",
    "\n",
    "Different types of Naive Bayes classifiers handle different types of data:\n",
    "\n",
    "### 4.1 Gaussian Naive Bayes\n",
    "- **Use case**: Continuous numerical features\n",
    "- **Assumption**: Features follow a normal (Gaussian) distribution\n",
    "- **Example**: Height, weight, temperature measurements\n",
    "\n",
    "### 4.2 Multinomial Naive Bayes  \n",
    "- **Use case**: Discrete count data\n",
    "- **Assumption**: Features represent counts or frequencies\n",
    "- **Example**: Word counts in text, number of occurrences\n",
    "\n",
    "### 4.3 Bernoulli Naive Bayes\n",
    "- **Use case**: Binary/boolean features\n",
    "- **Assumption**: Features are binary (0 or 1, True or False)\n",
    "- **Example**: Presence/absence of words, binary indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gaussian-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Gaussian Naive Bayes Example\n",
    "print(\"=== GAUSSIAN NAIVE BAYES EXAMPLE ===\")\n",
    "print(\"Use case: Continuous numerical features (like Iris dataset)\")\n",
    "print()\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show some predictions with probabilities\n",
    "y_proba = gnb.predict_proba(X_test[:5])\n",
    "print(\"Sample predictions with probabilities:\")\n",
    "for i in range(5):\n",
    "    actual = iris.target_names[y_test[i]]\n",
    "    predicted = iris.target_names[y_pred[i]]\n",
    "    probs = y_proba[i]\n",
    "    print(f\"  Sample {i+1}: Actual={actual}, Predicted={predicted}\")\n",
    "    print(f\"    Probabilities: {iris.target_names[0]}={probs[0]:.3f}, {iris.target_names[1]}={probs[1]:.3f}, {iris.target_names[2]}={probs[2]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-gaussian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how Gaussian Naive Bayes sees the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot each feature's distribution by class\n",
    "for i, feature_name in enumerate(iris.feature_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for class_idx, class_name in enumerate(iris.target_names):\n",
    "        class_data = X_iris[y_iris == class_idx, i]\n",
    "        ax.hist(class_data, alpha=0.7, label=class_name, bins=15)\n",
    "    \n",
    "    ax.set_title(f'{feature_name} Distribution by Class')\n",
    "    ax.set_xlabel(feature_name)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The plots show how Gaussian Naive Bayes assumes each feature follows\")\n",
    "print(\"a normal distribution for each class. The algorithm learns the mean\")\n",
    "print(\"and variance of each feature for each class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multinomial-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Multinomial Naive Bayes Example\n",
    "print(\"=== MULTINOMIAL NAIVE BAYES EXAMPLE ===\")\n",
    "print(\"Use case: Discrete count data (like word counts in text)\")\n",
    "print()\n",
    "\n",
    "# Create synthetic text data with word counts\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate document-term matrix (documents Ã— words)\n",
    "# Class 0: Technology articles (high counts for tech words)\n",
    "# Class 1: Sports articles (high counts for sports words)\n",
    "\n",
    "vocab = ['algorithm', 'computer', 'data', 'software', 'game', 'player', 'team', 'score']\n",
    "\n",
    "# Technology articles - higher counts for tech words (indices 0-3)\n",
    "tech_docs = np.random.poisson(3, (50, 4))  # Tech words\n",
    "tech_docs = np.hstack([tech_docs, np.random.poisson(0.5, (50, 4))])  # Sports words\n",
    "\n",
    "# Sports articles - higher counts for sports words (indices 4-7)\n",
    "sports_docs = np.random.poisson(0.5, (50, 4))  # Tech words  \n",
    "sports_docs = np.hstack([sports_docs, np.random.poisson(3, (50, 4))])  # Sports words\n",
    "\n",
    "# Combine data\n",
    "X_text = np.vstack([tech_docs, sports_docs])\n",
    "y_text = np.hstack([np.zeros(50), np.ones(50)])\n",
    "\n",
    "print(f\"Text dataset shape: {X_text.shape}\")\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "print(f\"Classes: Technology (0), Sports (1)\")\n",
    "print()\n",
    "\n",
    "# Show sample documents\n",
    "print(\"Sample documents (word counts):\")\n",
    "for i in [0, 50]:  # One from each class\n",
    "    class_name = \"Technology\" if y_text[i] == 0 else \"Sports\"\n",
    "    print(f\"  {class_name} document: {dict(zip(vocab, X_text[i]))}\")\n",
    "print()\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y_text, test_size=0.3, random_state=42)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Multinomial Naive Bayes Accuracy: {accuracy:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show feature importance (log probabilities)\n",
    "feature_log_prob = mnb.feature_log_prob_\n",
    "print(\"Feature importance (log probabilities):\")\n",
    "print(\"Word\\t\\tTechnology\\tSports\")\n",
    "for i, word in enumerate(vocab):\n",
    "    print(f\"{word:<12}\\t{feature_log_prob[0, i]:.3f}\\t\\t{feature_log_prob[1, i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bernoulli-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Bernoulli Naive Bayes Example\n",
    "print(\"=== BERNOULLI NAIVE BAYES EXAMPLE ===\")\n",
    "print(\"Use case: Binary features (presence/absence of words)\")\n",
    "print()\n",
    "\n",
    "# Convert the text data to binary (word present=1, absent=0)\n",
    "X_binary = (X_text > 0).astype(int)\n",
    "\n",
    "print(f\"Binary text dataset shape: {X_binary.shape}\")\n",
    "print(\"Features now represent presence (1) or absence (0) of words\")\n",
    "print()\n",
    "\n",
    "# Show sample documents\n",
    "print(\"Sample documents (binary features):\")\n",
    "for i in [0, 50]:  # One from each class\n",
    "    class_name = \"Technology\" if y_text[i] == 0 else \"Sports\"\n",
    "    present_words = [vocab[j] for j in range(len(vocab)) if X_binary[i, j] == 1]\n",
    "    print(f\"  {class_name} document contains: {present_words}\")\n",
    "print()\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_text, test_size=0.3, random_state=42)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Bernoulli Naive Bayes Accuracy: {accuracy:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show feature probabilities\n",
    "feature_log_prob = bnb.feature_log_prob_\n",
    "print(\"Feature probabilities (log P(word present | class)):\")\n",
    "print(\"Word\\t\\tTechnology\\tSports\")\n",
    "for i, word in enumerate(vocab):\n",
    "    print(f\"{word:<12}\\t{feature_log_prob[0, i]:.3f}\\t\\t{feature_log_prob[1, i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-world",
   "metadata": {},
   "source": [
    "## 5. Real-World Application: Titanic Survival Prediction\n",
    "\n",
    "Let's apply Gaussian Naive Bayes to predict Titanic passenger survival - a classic binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "titanic-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "print(\"=== TITANIC SURVIVAL PREDICTION ===\")\n",
    "print(\"Using Gaussian Naive Bayes for binary classification\")\n",
    "print()\n",
    "\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "print(f\"Dataset shape: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: Died (0), Survived (1)\")\n",
    "print()\n",
    "\n",
    "# Train model\n",
    "nb_titanic = GaussianNB()\n",
    "nb_titanic.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_titanic.predict(X_test)\n",
    "y_proba = nb_titanic.predict_proba(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = ModelEvaluator(\"Naive Bayes - Titanic\")\n",
    "metrics = evaluator.evaluate_classification(y_test, y_pred, y_proba)\n",
    "\n",
    "print(f\"Performance Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.3f}\")\n",
    "print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "print(f\"  F1-Score: {metrics['f1_score']:.3f}\")\n",
    "if metrics['auc_score']:\n",
    "    print(f\"  AUC Score: {metrics['auc_score']:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which features are most important for each class\n",
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "print(\"How Naive Bayes sees each feature for survival prediction:\")\n",
    "print()\n",
    "\n",
    "# Get the learned parameters\n",
    "class_means = nb_titanic.theta_  # Mean of each feature for each class\n",
    "class_vars = nb_titanic.sigma_   # Variance of each feature for each class\n",
    "\n",
    "print(\"Average feature values by class:\")\n",
    "print(\"Feature\\t\\t\\tDied (0)\\tSurvived (1)\\tDifference\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    died_mean = class_means[0, i]\n",
    "    survived_mean = class_means[1, i]\n",
    "    difference = survived_mean - died_mean\n",
    "    print(f\"{feature:<20}\\t{died_mean:.3f}\\t\\t{survived_mean:.3f}\\t\\t{difference:+.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  Positive difference = Feature increases survival probability\")\n",
    "print(\"  Negative difference = Feature decreases survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-titanic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix and feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Confusion Matrix\n",
    "ax1 = axes[0, 0]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])\n",
    "ax1.set_title('Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Feature importance visualization\n",
    "ax2 = axes[0, 1]\n",
    "feature_importance = [abs(class_means[1, i] - class_means[0, i]) for i in range(len(feature_names))]\n",
    "sorted_indices = sorted(range(len(feature_importance)), key=lambda i: feature_importance[i], reverse=True)\n",
    "\n",
    "sorted_features = [feature_names[i] for i in sorted_indices]\n",
    "sorted_importance = [feature_importance[i] for i in sorted_indices]\n",
    "\n",
    "ax2.bar(range(len(sorted_importance)), sorted_importance)\n",
    "ax2.set_title('Feature Importance (Mean Difference)')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Absolute Difference in Means')\n",
    "ax2.set_xticks(range(len(sorted_features)))\n",
    "ax2.set_xticklabels(sorted_features, rotation=45, ha='right')\n",
    "\n",
    "# Distribution of most important feature\n",
    "ax3 = axes[1, 0]\n",
    "most_important_idx = sorted_indices[0]\n",
    "most_important_feature = feature_names[most_important_idx]\n",
    "\n",
    "died_data = X_train[y_train == 0, most_important_idx]\n",
    "survived_data = X_train[y_train == 1, most_important_idx]\n",
    "\n",
    "ax3.hist(died_data, alpha=0.7, label='Died', bins=20, color='red')\n",
    "ax3.hist(survived_data, alpha=0.7, label='Survived', bins=20, color='blue')\n",
    "ax3.set_title(f'{most_important_feature} Distribution by Survival')\n",
    "ax3.set_xlabel(most_important_feature)\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Probability distribution example\n",
    "ax4 = axes[1, 1]\n",
    "sample_probs = y_proba[:50, 1]  # Survival probabilities for first 50 test samples\n",
    "colors = ['red' if actual == 0 else 'blue' for actual in y_test[:50]]\n",
    "\n",
    "ax4.scatter(range(50), sample_probs, c=colors, alpha=0.7)\n",
    "ax4.axhline(y=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "ax4.set_title('Predicted Survival Probabilities')\n",
    "ax4.set_xlabel('Test Sample')\n",
    "ax4.set_ylabel('P(Survival)')\n",
    "ax4.legend(['Decision Threshold', 'Died', 'Survived'])\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advantages",
   "metadata": {},
   "source": [
    "## 6. Advantages and Limitations\n",
    "\n",
    "### âœ… Advantages\n",
    "\n",
    "1. **Simple and Fast**: Easy to understand and implement\n",
    "2. **Small Data Friendly**: Works well with limited training data\n",
    "3. **Handles Multiple Classes**: Naturally supports multi-class classification\n",
    "4. **No Hyperparameter Tuning**: Few parameters to tune\n",
    "5. **Good Baseline**: Often provides a strong baseline for comparison\n",
    "6. **Probabilistic Output**: Provides probability estimates, not just classifications\n",
    "7. **Handles Irrelevant Features**: Robust to irrelevant features\n",
    "8. **Memory Efficient**: Low memory requirements\n",
    "\n",
    "### âŒ Limitations\n",
    "\n",
    "1. **Independence Assumption**: Features are rarely truly independent in real data\n",
    "2. **Categorical Inputs**: Struggles with categorical inputs (needs preprocessing)\n",
    "3. **Zero Probabilities**: Can assign zero probability if a feature value never appears with a class\n",
    "4. **Limited Expressiveness**: Cannot capture complex feature interactions\n",
    "5. **Gaussian Assumption**: Gaussian NB assumes normal distribution (may not hold)\n",
    "6. **Bias**: Can be biased when classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different types of Naive Bayes\n",
    "print(\"=== COMPARING NAIVE BAYES VARIANTS ===\")\n",
    "print(\"Testing all three types on the same dataset\")\n",
    "print()\n",
    "\n",
    "# Use the Titanic dataset for comparison\n",
    "models = {\n",
    "    'Gaussian NB': GaussianNB(),\n",
    "    'Multinomial NB': MultinomialNB(),  # Note: Requires non-negative features\n",
    "    'Bernoulli NB': BernoulliNB()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        # For Multinomial NB, we need non-negative features\n",
    "        if name == 'Multinomial NB':\n",
    "            # Scale features to be non-negative\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append({'Model': name, 'Accuracy': accuracy})\n",
    "        print(f\"{name:<15}: {accuracy:.3f} accuracy\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{name:<15}: Failed - {str(e)[:50]}\")\n",
    "\n",
    "print()\n",
    "print(\"Note: Different variants work better for different types of data:\")\n",
    "print(\"  - Gaussian: Best for continuous numerical features\")\n",
    "print(\"  - Multinomial: Best for count data (text analysis)\")\n",
    "print(\"  - Bernoulli: Best for binary features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "## 7. When to Use Naive Bayes\n",
    "\n",
    "### ðŸŽ¯ Perfect Use Cases\n",
    "\n",
    "1. **Text Classification**: Email spam detection, sentiment analysis, document categorization\n",
    "2. **Medical Diagnosis**: When you have symptom probabilities\n",
    "3. **Real-time Prediction**: Fast inference needed\n",
    "4. **Small Datasets**: Limited training data available\n",
    "5. **Baseline Models**: Quick first attempt at a classification problem\n",
    "6. **Multi-class Problems**: Naturally handles multiple classes\n",
    "\n",
    "### âŒ Avoid When\n",
    "\n",
    "1. **Strong Feature Correlations**: Features are highly dependent\n",
    "2. **Need High Accuracy**: More sophisticated models likely to perform better\n",
    "3. **Complex Relationships**: Non-linear relationships between features and classes\n",
    "4. **Small Number of Features**: Other algorithms might be more suitable\n",
    "\n",
    "### ðŸ† Real-World Success Stories\n",
    "\n",
    "- **Google's Gmail**: Uses Naive Bayes for spam detection\n",
    "- **Netflix**: Early recommendation systems\n",
    "- **Medical AI**: Diagnostic systems\n",
    "- **E-commerce**: Product categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final practical tips and code snippets\n",
    "print(\"=== PRACTICAL TIPS FOR NAIVE BAYES ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. HANDLING ZERO PROBABILITIES:\")\n",
    "print(\"   Use Laplace smoothing (alpha parameter):\")\n",
    "print(\"   model = MultinomialNB(alpha=1.0)  # Default is 1.0\")\n",
    "print(\"   model = BernoulliNB(alpha=1.0)\")\n",
    "print()\n",
    "\n",
    "print(\"2. FEATURE PREPROCESSING:\")\n",
    "print(\"   - Gaussian NB: Scale features if they have very different ranges\")\n",
    "print(\"   - Multinomial NB: Features must be non-negative\")\n",
    "print(\"   - Bernoulli NB: Convert to binary (0/1) features\")\n",
    "print()\n",
    "\n",
    "print(\"3. HANDLING CATEGORICAL FEATURES:\")\n",
    "print(\"   Use label encoding or one-hot encoding before applying Naive Bayes\")\n",
    "print()\n",
    "\n",
    "print(\"4. MODEL SELECTION:\")\n",
    "print(\"   - Use cross-validation to compare different NB variants\")\n",
    "print(\"   - Try different alpha values for smoothing\")\n",
    "print()\n",
    "\n",
    "print(\"5. INTERPRETING RESULTS:\")\n",
    "print(\"   - Use predict_proba() for probability estimates\")\n",
    "print(\"   - Examine feature_log_prob_ to understand feature importance\")\n",
    "print(\"   - Check class_prior_ to see class distribution\")\n",
    "print()\n",
    "\n",
    "# Demonstrate some of these tips\n",
    "print(\"Example - Examining model parameters:\")\n",
    "print(f\"Class priors: {nb_titanic.class_prior_}\")\n",
    "print(f\"Classes: {nb_titanic.classes_}\")\n",
    "print(f\"Number of features: {nb_titanic.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### ðŸŽ¯ What You've Learned\n",
    "\n",
    "1. **Mathematical Foundation**: Bayes' theorem and conditional probability\n",
    "2. **Algorithm Mechanics**: How Naive Bayes makes predictions\n",
    "3. **Three Variants**: Gaussian, Multinomial, and Bernoulli Naive Bayes\n",
    "4. **Practical Implementation**: Real examples with code\n",
    "5. **Strengths & Weaknesses**: When to use and when to avoid\n",
    "6. **Best Practices**: Tips for successful implementation\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. **Practice**: Try Naive Bayes on your own datasets\n",
    "2. **Compare**: Test against other algorithms (Logistic Regression, SVM, Random Forest)\n",
    "3. **Feature Engineering**: Experiment with different feature representations\n",
    "4. **Domain Application**: Apply to text classification or medical diagnosis problems\n",
    "5. **Advanced Topics**: Learn about ensemble methods that combine Naive Bayes with other algorithms\n",
    "\n",
    "### ðŸ’¡ Key Insights\n",
    "\n",
    "- **Simplicity is Powerful**: Despite being \"naive\", often performs surprisingly well\n",
    "- **Speed Matters**: Fast training and prediction make it great for prototyping\n",
    "- **Interpretability**: Easy to understand what the model learned\n",
    "- **Probabilistic**: Provides confidence estimates, not just classifications\n",
    "- **Foundation**: Understanding Naive Bayes helps with more advanced probabilistic models\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand one of the most fundamental and widely-used machine learning algorithms. Naive Bayes may be simple, but it's a powerful tool in your machine learning toolkit.\n",
    "\n",
    "Remember: Sometimes the simplest solution is the best solution! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
