{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Neural Networks for Classification: From Perceptron to Deep Learning\n",
    "\n",
    "Welcome to your comprehensive guide to **Neural Networks for Classification**! This notebook will take you from the basic perceptron to modern deep learning architectures, showing you how these brain-inspired algorithms revolutionized machine learning.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Neural Network Fundamentals**: Neurons, layers, and activation functions\n",
    "2. **Forward Propagation**: How data flows through the network\n",
    "3. **Backpropagation**: The learning algorithm that changed everything\n",
    "4. **Architecture Design**: Hidden layers, neurons, and network topology\n",
    "5. **Activation Functions**: ReLU, Sigmoid, Tanh and their properties\n",
    "6. **Regularization Techniques**: Dropout, batch normalization, early stopping\n",
    "7. **Optimization**: SGD, Adam, RMSprop and learning rate scheduling\n",
    "8. **Practical Implementation**: Building networks with Keras/TensorFlow\n",
    "9. **Advanced Techniques**: Ensemble methods and hyperparameter tuning\n",
    "10. **Real-world Applications**: When and how to use neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network-basics",
   "metadata": {},
   "source": [
    "## 1. Neural Network Fundamentals\n",
    "\n",
    "### The Biological Inspiration\n",
    "\n",
    "Neural networks are inspired by the human brain:\n",
    "- **Neurons**: Basic processing units\n",
    "- **Synapses**: Connections between neurons (weights)\n",
    "- **Activation**: Neuron fires when input exceeds threshold\n",
    "- **Learning**: Connections strengthen/weaken based on experience\n",
    "\n",
    "### The Mathematical Model\n",
    "\n",
    "#### Single Neuron (Perceptron)\n",
    "$$y = f(\\sum_{i=1}^{n} w_i x_i + b)$$\n",
    "\n",
    "Where:\n",
    "- $x_i$: Input features\n",
    "- $w_i$: Weights (connection strengths)\n",
    "- $b$: Bias (threshold adjustment)\n",
    "- $f$: Activation function\n",
    "- $y$: Output\n",
    "\n",
    "#### Multi-layer Network\n",
    "Multiple neurons arranged in layers:\n",
    "- **Input Layer**: Receives raw features\n",
    "- **Hidden Layer(s)**: Process and transform data\n",
    "- **Output Layer**: Produces final predictions\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Universal Approximation**: Neural networks can approximate any continuous function\n",
    "2. **Non-linearity**: Activation functions enable complex pattern learning\n",
    "3. **Hierarchical Learning**: Deep networks learn hierarchical features\n",
    "4. **End-to-end Learning**: Automatic feature extraction and classification\n",
    "\n",
    "### Why Neural Networks Work\n",
    "\n",
    "ðŸ§  **Representation Learning**: Automatically discover relevant features\n",
    "ðŸŽ¯ **Non-linear Boundaries**: Handle complex decision boundaries\n",
    "ðŸ”„ **Adaptive**: Learn from data without manual feature engineering\n",
    "ðŸŒ **Scalable**: Performance improves with more data and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# TensorFlow/Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from utils.data_utils import load_titanic_data\n",
    "from utils.evaluation import ModelEvaluator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[START] Neural Networks for Classification Tutorial\")\n",
    "print(\"ðŸ“¦ Libraries loaded successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceptron-demo",
   "metadata": {},
   "source": [
    "## 2. The Perceptron: Building Block of Neural Networks\n",
    "\n",
    "### Understanding the Perceptron\n",
    "\n",
    "The perceptron is the simplest neural network - a single neuron that:\n",
    "1. Takes weighted sum of inputs\n",
    "2. Adds bias term\n",
    "3. Applies activation function\n",
    "4. Outputs prediction\n",
    "\n",
    "### Perceptron Learning Algorithm\n",
    "1. **Initialize**: Random weights and bias\n",
    "2. **Forward Pass**: Compute output\n",
    "3. **Error Calculation**: Compare with true label\n",
    "4. **Weight Update**: Adjust weights based on error\n",
    "5. **Repeat**: Until convergence\n",
    "\n",
    "### Limitations\n",
    "- Can only learn **linearly separable** patterns\n",
    "- Cannot solve XOR problem\n",
    "- Limited to binary classification\n",
    "\n",
    "### The XOR Problem\n",
    "Classic example that single perceptron cannot solve:\n",
    "- Input: (0,0) â†’ Output: 0\n",
    "- Input: (0,1) â†’ Output: 1  \n",
    "- Input: (1,0) â†’ Output: 1\n",
    "- Input: (1,1) â†’ Output: 0\n",
    "\n",
    "No single line can separate these classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceptron-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize perceptron and demonstrate XOR problem\n",
    "print(\"=== PERCEPTRON DEMONSTRATION ===\")\n",
    "print()\n",
    "\n",
    "# Create linearly separable data\n",
    "np.random.seed(42)\n",
    "X_linear, y_linear = make_classification(\n",
    "    n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
    "    n_clusters_per_class=1, class_sep=2.0, random_state=42\n",
    ")\n",
    "\n",
    "# Create XOR-like data (not linearly separable)\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Add noise to XOR for better visualization\n",
    "n_samples = 200\n",
    "X_xor_expanded = []\n",
    "y_xor_expanded = []\n",
    "\n",
    "for i in range(4):\n",
    "    for _ in range(n_samples // 4):\n",
    "        noise = np.random.normal(0, 0.1, 2)\n",
    "        X_xor_expanded.append(X_xor[i] + noise)\n",
    "        y_xor_expanded.append(y_xor[i])\n",
    "\n",
    "X_xor_expanded = np.array(X_xor_expanded)\n",
    "y_xor_expanded = np.array(y_xor_expanded)\n",
    "\n",
    "print(f\"Created datasets:\")\n",
    "print(f\"  Linearly separable: {X_linear.shape[0]} samples\")\n",
    "print(f\"  XOR problem: {X_xor_expanded.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Train simple perceptron (linear model)\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Perceptron on linearly separable data\n",
    "perceptron_linear = Perceptron(random_state=42, max_iter=1000)\n",
    "perceptron_linear.fit(X_linear, y_linear)\n",
    "accuracy_linear = perceptron_linear.score(X_linear, y_linear)\n",
    "\n",
    "# Perceptron on XOR data\n",
    "perceptron_xor = Perceptron(random_state=42, max_iter=1000)\n",
    "perceptron_xor.fit(X_xor_expanded, y_xor_expanded)\n",
    "accuracy_xor = perceptron_xor.score(X_xor_expanded, y_xor_expanded)\n",
    "\n",
    "print(f\"Perceptron Results:\")\n",
    "print(f\"  Linearly separable data: {accuracy_linear:.3f} accuracy\")\n",
    "print(f\"  XOR problem: {accuracy_xor:.3f} accuracy\")\n",
    "print()\n",
    "\n",
    "# Visualization\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i in range(2):\n",
    "        idx = y == i\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=colors[i], s=60, alpha=0.8,\n",
    "                   label=f'Class {i}', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot both scenarios\n",
    "plot_decision_boundary(perceptron_linear, X_linear, y_linear, \n",
    "                      f'Perceptron on Linearly Separable Data\\nAccuracy: {accuracy_linear:.3f}')\n",
    "\n",
    "plot_decision_boundary(perceptron_xor, X_xor_expanded, y_xor_expanded,\n",
    "                      f'Perceptron on XOR Problem\\nAccuracy: {accuracy_xor:.3f}')\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"  âœ… Perceptron works perfectly on linearly separable data\")\n",
    "print(\"  âŒ Perceptron fails on XOR problem (not linearly separable)\")\n",
    "print(\"  ðŸŽ¯ This limitation led to the development of multi-layer networks\")\n",
    "print(\"  ðŸ’¡ Multiple layers can solve non-linear problems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multilayer-perceptron",
   "metadata": {},
   "source": [
    "## 3. Multi-Layer Perceptron (MLP): Breaking the Linear Barrier\n",
    "\n",
    "### The Multi-Layer Solution\n",
    "\n",
    "To solve non-linear problems like XOR, we need:\n",
    "1. **Hidden layers** between input and output\n",
    "2. **Non-linear activation functions**\n",
    "3. **Multiple neurons** per layer\n",
    "\n",
    "### MLP Architecture\n",
    "```\n",
    "Input Layer â†’ Hidden Layer(s) â†’ Output Layer\n",
    "     â†“              â†“               â†“\n",
    "  Features    Feature Learning   Predictions\n",
    "```\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "#### 1. Sigmoid\n",
    "- **Formula**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **Range**: (0, 1)\n",
    "- **Use**: Output layer for binary classification\n",
    "- **Problem**: Vanishing gradient\n",
    "\n",
    "#### 2. Tanh\n",
    "- **Formula**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- **Range**: (-1, 1)\n",
    "- **Use**: Hidden layers (better than sigmoid)\n",
    "- **Problem**: Still vanishing gradient\n",
    "\n",
    "#### 3. ReLU (Rectified Linear Unit)\n",
    "- **Formula**: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- **Range**: [0, âˆž)\n",
    "- **Use**: Most popular for hidden layers\n",
    "- **Advantages**: No vanishing gradient, fast computation\n",
    "\n",
    "#### 4. Softmax (for multi-class)\n",
    "- **Formula**: $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "- **Use**: Output layer for multi-class classification\n",
    "- **Property**: Outputs sum to 1 (probabilities)\n",
    "\n",
    "### Why Hidden Layers Work\n",
    "\n",
    "1. **Feature Transformation**: Each layer learns new feature representations\n",
    "2. **Non-linear Combinations**: Activation functions enable non-linear mappings\n",
    "3. **Hierarchical Learning**: Deep networks learn hierarchical patterns\n",
    "4. **Universal Approximation**: Can approximate any continuous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-functions-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate activation functions and solve XOR with MLP\n",
    "print(\"=== ACTIVATION FUNCTIONS DEMONSTRATION ===\")\n",
    "print()\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Plot activation functions\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "activations = [\n",
    "    (sigmoid(x), 'Sigmoid', 'Output: (0, 1)'),\n",
    "    (tanh(x), 'Tanh', 'Output: (-1, 1)'),\n",
    "    (relu(x), 'ReLU', 'Output: [0, âˆž)'),\n",
    "    (leaky_relu(x), 'Leaky ReLU', 'Output: (-âˆž, âˆž)')\n",
    "]\n",
    "\n",
    "for i, (y, name, desc) in enumerate(activations):\n",
    "    axes[i].plot(x, y, linewidth=3, color='blue')\n",
    "    axes[i].set_title(f'{name} Activation Function')\n",
    "    axes[i].set_xlabel('Input (x)')\n",
    "    axes[i].set_ylabel(f'Output ({desc})')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axhline(y=0, color='black', linewidth=0.5)\n",
    "    axes[i].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Activation Function Properties:\")\n",
    "print(\"  Sigmoid: Smooth, bounded, suffers from vanishing gradients\")\n",
    "print(\"  Tanh: Zero-centered, bounded, still has vanishing gradient problem\")\n",
    "print(\"  ReLU: Simple, fast, solves vanishing gradient, but can 'die'\")\n",
    "print(\"  Leaky ReLU: Prevents dying ReLU problem\")\n",
    "print()\n",
    "\n",
    "# Solve XOR problem with MLP\n",
    "print(\"=== SOLVING XOR WITH MLP ===\")\n",
    "print()\n",
    "\n",
    "# Create MLP with hidden layer\n",
    "mlp_xor = MLPClassifier(\n",
    "    hidden_layer_sizes=(10,),  # One hidden layer with 10 neurons\n",
    "    activation='relu',         # ReLU activation\n",
    "    solver='adam',            # Adam optimizer\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on XOR data\n",
    "mlp_xor.fit(X_xor_expanded, y_xor_expanded)\n",
    "accuracy_mlp_xor = mlp_xor.score(X_xor_expanded, y_xor_expanded)\n",
    "\n",
    "print(f\"MLP Results on XOR Problem:\")\n",
    "print(f\"  Accuracy: {accuracy_mlp_xor:.3f}\")\n",
    "print(f\"  Network architecture: {mlp_xor.hidden_layer_sizes}\")\n",
    "print(f\"  Number of iterations: {mlp_xor.n_iter_}\")\n",
    "print()\n",
    "\n",
    "# Test on original XOR points\n",
    "xor_predictions = mlp_xor.predict(X_xor)\n",
    "xor_probabilities = mlp_xor.predict_proba(X_xor)\n",
    "\n",
    "print(\"XOR Truth Table - MLP Predictions:\")\n",
    "print(\"Input | True | Pred | Probability\")\n",
    "print(\"-\" * 35)\n",
    "for i in range(4):\n",
    "    prob = xor_probabilities[i][1] if len(xor_probabilities[i]) > 1 else xor_probabilities[i][0]\n",
    "    print(f\"{X_xor[i]} | {y_xor[i]:4d} | {xor_predictions[i]:4d} | {prob:11.3f}\")\n",
    "\n",
    "# Visualize MLP decision boundary on XOR\n",
    "plot_decision_boundary(mlp_xor, X_xor_expanded, y_xor_expanded,\n",
    "                      f'MLP Solution to XOR Problem\\nAccuracy: {accuracy_mlp_xor:.3f}')\n",
    "\n",
    "print(\"ðŸŽ‰ Success! MLP solved the XOR problem that perceptron couldn't!\")\n",
    "print(\"ðŸ’¡ Hidden layers + non-linear activations = non-linear classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-backprop",
   "metadata": {},
   "source": [
    "## 4. Forward Propagation and Backpropagation\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "**Process**: Data flows forward through the network\n",
    "\n",
    "1. **Input Layer**: Raw features\n",
    "2. **Hidden Layer**: $h = f(W_1 \\cdot x + b_1)$\n",
    "3. **Output Layer**: $\\hat{y} = g(W_2 \\cdot h + b_2)$\n",
    "\n",
    "Where:\n",
    "- $W_i$: Weight matrices\n",
    "- $b_i$: Bias vectors  \n",
    "- $f, g$: Activation functions\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "**The Learning Algorithm**: How neural networks learn\n",
    "\n",
    "#### Step 1: Calculate Loss\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^{n} \\text{loss}(y_i, \\hat{y}_i)$$\n",
    "\n",
    "#### Step 2: Compute Gradients (Chain Rule)\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W}$$\n",
    "\n",
    "#### Step 3: Update Weights\n",
    "$$W_{new} = W_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Chain Rule**: Enables gradient computation through layers\n",
    "2. **Error Propagation**: Errors flow backward to adjust weights\n",
    "3. **Gradient Descent**: Iteratively minimizes loss function\n",
    "4. **Local Updates**: Each weight updated based on local gradient\n",
    "\n",
    "### Common Loss Functions\n",
    "\n",
    "#### Binary Classification\n",
    "- **Binary Cross-entropy**: $L = -\\frac{1}{n}\\sum[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$\n",
    "\n",
    "#### Multi-class Classification  \n",
    "- **Categorical Cross-entropy**: $L = -\\frac{1}{n}\\sum\\sum y_{ij}\\log(\\hat{y}_{ij})$\n",
    "\n",
    "#### Why Cross-entropy?\n",
    "- Penalizes wrong predictions more heavily\n",
    "- Works well with softmax/sigmoid outputs\n",
    "- Provides good gradients for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backprop-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate forward and backward propagation\n",
    "print(\"=== FORWARD AND BACKWARD PROPAGATION DEMO ===\")\n",
    "print()\n",
    "\n",
    "# Load dataset for comprehensive neural network demo\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "# Scale features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset: Titanic Survival Prediction\")\n",
    "print(f\"  Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test_scaled.shape[0]}\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(y_train))}\")\n",
    "print()\n",
    "\n",
    "# Build neural network with Keras\n",
    "print(\"Building Neural Network with Keras...\")\n",
    "\n",
    "# Simple MLP architecture\n",
    "model = keras.Sequential([\n",
    "    # Input layer (implicit)\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],), name='hidden1'),\n",
    "    layers.Dense(32, activation='relu', name='hidden2'),\n",
    "    layers.Dense(16, activation='relu', name='hidden3'),\n",
    "    layers.Dense(1, activation='sigmoid', name='output')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "print()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "print()\n",
    "\n",
    "# Train the model with history tracking\n",
    "print(\"Training Neural Network...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    verbose=0,  # Silent training\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Training completed after {len(history.history['loss'])} epochs\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"Additional Metrics:\")\n",
    "print(f\"  AUC Score: {auc_score:.4f}\")\n",
    "print(f\"  Precision/Recall/F1:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training process and visualize learning\n",
    "print(\"=== TRAINING ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "axes[0].plot(epochs, history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss During Training')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Binary Cross-entropy Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('Model Accuracy During Training')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze training behavior\n",
    "final_epoch = len(history.history['loss'])\n",
    "min_val_loss_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "min_val_loss = min(history.history['val_loss'])\n",
    "max_val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "print(f\"Training Analysis:\")\n",
    "print(f\"  Total epochs: {final_epoch}\")\n",
    "print(f\"  Best validation loss: {min_val_loss:.4f} (epoch {min_val_loss_epoch})\")\n",
    "print(f\"  Best validation accuracy: {max_val_acc:.4f}\")\n",
    "print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "gap = history.history['loss'][-1] - history.history['val_loss'][-1]\n",
    "if gap < -0.1:\n",
    "    print(f\"  ðŸ“Š Model shows signs of overfitting (train-val gap: {gap:.4f})\")\n",
    "elif gap > 0.1:\n",
    "    print(f\"  ðŸ“ˆ Model might be underfitting (train-val gap: {gap:.4f})\")\n",
    "else:\n",
    "    print(f\"  âœ… Model shows good generalization (train-val gap: {gap:.4f})\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-design",
   "metadata": {},
   "source": [
    "## 5. Network Architecture Design\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "#### 1. Number of Hidden Layers (Depth)\n",
    "- **Shallow (1-2 layers)**: Simple patterns, faster training\n",
    "- **Deep (3+ layers)**: Complex patterns, hierarchical features\n",
    "- **Very Deep (10+ layers)**: Requires careful design (ResNets, etc.)\n",
    "\n",
    "#### 2. Number of Neurons per Layer (Width)\n",
    "- **Too few**: Underfitting, can't learn complex patterns\n",
    "- **Too many**: Overfitting, slower training\n",
    "- **Rule of thumb**: Start with 2/3 * (input + output) neurons\n",
    "\n",
    "#### 3. Layer Size Patterns\n",
    "- **Funnel**: Decreasing size (e.g., 128 â†’ 64 â†’ 32 â†’ 1)\n",
    "- **Uniform**: Same size throughout\n",
    "- **Hourglass**: Large â†’ Small â†’ Large\n",
    "\n",
    "### Architecture Guidelines\n",
    "\n",
    "#### For Tabular Data (like our Titanic dataset)\n",
    "- **Depth**: 2-4 hidden layers usually sufficient\n",
    "- **Width**: 50-200 neurons per layer\n",
    "- **Pattern**: Funnel architecture works well\n",
    "\n",
    "#### For Complex Problems\n",
    "- **More layers**: For hierarchical feature learning\n",
    "- **Skip connections**: For very deep networks\n",
    "- **Specialized architectures**: CNNs for images, RNNs for sequences\n",
    "\n",
    "### Practical Tips\n",
    "\n",
    "1. **Start simple**: Begin with 1-2 hidden layers\n",
    "2. **Add complexity gradually**: Monitor validation performance\n",
    "3. **Use regularization**: Dropout, batch norm, weight decay\n",
    "4. **Early stopping**: Prevent overfitting\n",
    "5. **Cross-validation**: Robust architecture selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architecture-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different neural network architectures\n",
    "print(\"=== NEURAL NETWORK ARCHITECTURE COMPARISON ===\")\n",
    "print()\n",
    "\n",
    "# Define different architectures to test\n",
    "architectures = {\n",
    "    'Shallow Wide': [128, 1],\n",
    "    'Shallow Narrow': [32, 1], \n",
    "    'Deep Narrow': [32, 32, 32, 1],\n",
    "    'Deep Wide': [128, 64, 32, 1],\n",
    "    'Very Deep': [64, 64, 32, 32, 16, 1],\n",
    "    'Hourglass': [64, 32, 16, 32, 1]\n",
    "}\n",
    "\n",
    "# Function to create model with given architecture\n",
    "def create_model(hidden_layers, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # First hidden layer\n",
    "    model.add(layers.Dense(hidden_layers[0], activation='relu', \n",
    "                          input_shape=(input_shape,)))\n",
    "    \n",
    "    # Additional hidden layers\n",
    "    for neurons in hidden_layers[1:-1]:\n",
    "        model.add(layers.Dense(neurons, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(hidden_layers[-1], activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Test each architecture\n",
    "results = {}\n",
    "print(\"Testing different architectures...\")\n",
    "print()\n",
    "\n",
    "for name, arch in architectures.items():\n",
    "    print(f\"Training {name}: {arch[:-1]} â†’ {arch[-1]}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(arch, X_train_scaled.shape[1])\n",
    "    \n",
    "    # Train with early stopping\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        verbose=0,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Get predictions for AUC\n",
    "    y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'architecture': arch[:-1],\n",
    "        'params': model.count_params(),\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'auc': auc,\n",
    "        'overfitting': train_acc - test_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}, AUC: {auc:.4f}, Params: {model.count_params():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ARCHITECTURE COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results = df_results.sort_values('test_acc', ascending=False)\n",
    "\n",
    "print(f\"{'Architecture':<15} {'Params':<8} {'Epochs':<7} {'Test Acc':<9} {'AUC':<7} {'Overfitting':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, row in df_results.iterrows():\n",
    "    params = f\"{row['params']:,}\" if row['params'] < 10000 else f\"{row['params']/1000:.1f}k\"\n",
    "    print(f\"{name:<15} {params:<8} {row['epochs']:<7.0f} {row['test_acc']:<9.4f} {row['auc']:<7.4f} {row['overfitting']:<12.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Find best architecture\n",
    "best_arch = df_results.index[0]\n",
    "best_results = df_results.iloc[0]\n",
    "\n",
    "print(f\"ðŸ† Best Architecture: {best_arch}\")\n",
    "print(f\"   Test Accuracy: {best_results['test_acc']:.4f}\")\n",
    "print(f\"   AUC: {best_results['auc']:.4f}\")\n",
    "print(f\"   Parameters: {best_results['params']:,}\")\n",
    "print(f\"   Training Epochs: {best_results['epochs']:.0f}\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“Š Key Insights:\")\n",
    "print(f\"   â€¢ Simplest effective architecture: {df_results.iloc[-1].name}\")\n",
    "print(f\"   â€¢ Most parameters: {df_results['params'].max():,} ({df_results['params'].idxmax()})\")\n",
    "print(f\"   â€¢ Least overfitting: {df_results['overfitting'].idxmin()} ({df_results['overfitting'].min():.4f})\")\n",
    "print(f\"   â€¢ Fastest training: {df_results['epochs'].idxmin()} ({df_results['epochs'].min():.0f} epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularization",
   "metadata": {},
   "source": [
    "## 6. Regularization Techniques\n",
    "\n",
    "Neural networks are prone to **overfitting** due to their high capacity. Regularization techniques help prevent this.\n",
    "\n",
    "### 1. Dropout\n",
    "\n",
    "**Concept**: Randomly \"drop out\" neurons during training\n",
    "- **Training**: Each neuron has probability `p` of being set to 0\n",
    "- **Inference**: Use all neurons, scale by `(1-p)`\n",
    "- **Effect**: Prevents co-adaptation, improves generalization\n",
    "\n",
    "**Usage**:\n",
    "```python\n",
    "layers.Dropout(0.5)  # Drop 50% of neurons\n",
    "```\n",
    "\n",
    "### 2. Batch Normalization\n",
    "\n",
    "**Concept**: Normalize inputs to each layer\n",
    "$$BN(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "**Benefits**:\n",
    "- Stabilizes training\n",
    "- Allows higher learning rates\n",
    "- Reduces internal covariate shift\n",
    "- Acts as regularization\n",
    "\n",
    "### 3. Weight Regularization\n",
    "\n",
    "**L1 Regularization**: $\\lambda \\sum |w_i|$\n",
    "- Promotes sparsity\n",
    "- Feature selection effect\n",
    "\n",
    "**L2 Regularization**: $\\lambda \\sum w_i^2$\n",
    "- Prevents large weights\n",
    "- Smoother models\n",
    "\n",
    "### 4. Early Stopping\n",
    "\n",
    "**Concept**: Stop training when validation performance stops improving\n",
    "- Monitor validation loss\n",
    "- Stop after `patience` epochs without improvement\n",
    "- Restore best weights\n",
    "\n",
    "### 5. Data Augmentation\n",
    "\n",
    "**Concept**: Artificially increase training data\n",
    "- Add noise to inputs\n",
    "- Feature transformations\n",
    "- Synthetic examples\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with dropout**: 0.2-0.5 for hidden layers\n",
    "2. **Add batch normalization**: After dense layers\n",
    "3. **Use early stopping**: Always monitor validation\n",
    "4. **Combine techniques**: Different regularization methods complement each other\n",
    "5. **Tune hyperparameters**: Cross-validation for optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regularization-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate regularization techniques\n",
    "print(\"=== REGULARIZATION TECHNIQUES COMPARISON ===\")\n",
    "print()\n",
    "\n",
    "# Create models with different regularization techniques\n",
    "def create_baseline_model():\n",
    "    \"\"\"No regularization\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_dropout_model():\n",
    "    \"\"\"With dropout regularization\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_batchnorm_model():\n",
    "    \"\"\"With batch normalization\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(32),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_l2_model():\n",
    "    \"\"\"With L2 weight regularization\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', \n",
    "                    kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                    input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.Dense(32, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_combined_model():\n",
    "    \"\"\"Combined regularization techniques\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, input_shape=(X_train_scaled.shape[1],),\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Test different regularization approaches\n",
    "regularization_models = {\n",
    "    'Baseline (No Reg)': create_baseline_model,\n",
    "    'Dropout': create_dropout_model,\n",
    "    'Batch Norm': create_batchnorm_model,\n",
    "    'L2 Regularization': create_l2_model,\n",
    "    'Combined': create_combined_model\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "histories = {}\n",
    "\n",
    "print(\"Training models with different regularization techniques...\")\n",
    "print()\n",
    "\n",
    "for name, model_func in regularization_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = model_func()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        verbose=0,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    reg_results[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'auc': auc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'epochs': len(history.history['loss'])\n",
    "    }\n",
    "    histories[name] = history\n",
    "    \n",
    "    print(f\"  Test Acc: {test_acc:.4f}, AUC: {auc:.4f}, Overfitting: {train_acc - test_acc:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGULARIZATION COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_reg = pd.DataFrame(reg_results).T\n",
    "df_reg = df_reg.sort_values('overfitting', ascending=True)  # Less overfitting is better\n",
    "\n",
    "print(f\"{'Technique':<20} {'Test Acc':<9} {'AUC':<7} {'Overfitting':<12} {'Epochs':<7}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, row in df_reg.iterrows():\n",
    "    print(f\"{name:<20} {row['test_acc']:<9.4f} {row['auc']:<7.4f} {row['overfitting']:<12.4f} {row['epochs']:<7.0f}\")\n",
    "\n",
    "print()\n",
    "best_reg = df_reg.index[0]\n",
    "print(f\"ðŸ† Best Regularization: {best_reg}\")\n",
    "print(f\"   Least overfitting: {df_reg.iloc[0]['overfitting']:.4f}\")\n",
    "print(f\"   Test accuracy: {df_reg.iloc[0]['test_acc']:.4f}\")\n",
    "print(f\"   AUC: {df_reg.iloc[0]['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regularization-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization effects\n",
    "print(\"=== REGULARIZATION EFFECTS VISUALIZATION ===\")\n",
    "print()\n",
    "\n",
    "# Plot training curves for different regularization techniques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "\n",
    "# Training Loss\n",
    "for i, (name, history) in enumerate(histories.items()):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    axes[0,0].plot(epochs, history.history['loss'], color=colors[i], \n",
    "                  label=f'{name}', linewidth=2, alpha=0.7)\n",
    "axes[0,0].set_title('Training Loss Comparison')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Binary Cross-entropy Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for i, (name, history) in enumerate(histories.items()):\n",
    "    epochs = range(1, len(history.history['val_loss']) + 1)\n",
    "    axes[0,1].plot(epochs, history.history['val_loss'], color=colors[i], \n",
    "                  label=f'{name}', linewidth=2, alpha=0.7)\n",
    "axes[0,1].set_title('Validation Loss Comparison')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Binary Cross-entropy Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "for i, (name, history) in enumerate(histories.items()):\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    axes[1,0].plot(epochs, history.history['accuracy'], color=colors[i], \n",
    "                  label=f'{name}', linewidth=2, alpha=0.7)\n",
    "axes[1,0].set_title('Training Accuracy Comparison')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for i, (name, history) in enumerate(histories.items()):\n",
    "    epochs = range(1, len(history.history['val_accuracy']) + 1)\n",
    "    axes[1,1].plot(epochs, history.history['val_accuracy'], color=colors[i], \n",
    "                  label=f'{name}', linewidth=2, alpha=0.7)\n",
    "axes[1,1].set_title('Validation Accuracy Comparison')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overfitting analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "techniques = list(reg_results.keys())\n",
    "overfitting_scores = [reg_results[t]['overfitting'] for t in techniques]\n",
    "test_scores = [reg_results[t]['test_acc'] for t in techniques]\n",
    "\n",
    "# Color bars based on test accuracy\n",
    "bars = plt.bar(techniques, overfitting_scores, alpha=0.7)\n",
    "for i, (bar, test_acc) in enumerate(zip(bars, test_scores)):\n",
    "    # Color based on test accuracy (green for high, red for low)\n",
    "    if test_acc > 0.82:\n",
    "        bar.set_color('green')\n",
    "    elif test_acc > 0.80:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "    \n",
    "    # Add text annotations\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,\n",
    "            f'{test_acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title('Overfitting Analysis by Regularization Technique\\n(Lower is better, colors show test accuracy)')\n",
    "plt.xlabel('Regularization Technique')\n",
    "plt.ylabel('Overfitting (Train Acc - Test Acc)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Key Insights from Regularization Analysis:\")\n",
    "print(f\"   â€¢ Baseline model shows highest overfitting: {reg_results['Baseline (No Reg)']['overfitting']:.4f}\")\n",
    "print(f\"   â€¢ Best regularization technique: {best_reg}\")\n",
    "print(f\"   â€¢ Combined techniques often work best for complex problems\")\n",
    "print(f\"   â€¢ Early stopping helps all techniques converge faster\")\n",
    "print(f\"   â€¢ Regularization trades off some training accuracy for better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimization",
   "metadata": {},
   "source": [
    "## 7. Optimization Algorithms\n",
    "\n",
    "The choice of optimizer significantly affects training speed and final performance.\n",
    "\n",
    "### 1. Gradient Descent Variants\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "- **Update**: $w = w - \\alpha \\nabla L$\n",
    "- **Pros**: Simple, works well with momentum\n",
    "- **Cons**: Sensitive to learning rate, slow convergence\n",
    "\n",
    "#### SGD with Momentum\n",
    "- **Update**: $v = \\beta v + \\nabla L$, $w = w - \\alpha v$\n",
    "- **Benefit**: Accelerates convergence, reduces oscillations\n",
    "\n",
    "### 2. Adaptive Learning Rate Methods\n",
    "\n",
    "#### AdaGrad\n",
    "- **Concept**: Adapt learning rate for each parameter\n",
    "- **Problem**: Learning rate decays too aggressively\n",
    "\n",
    "#### RMSprop\n",
    "- **Improvement**: Exponential moving average of gradients\n",
    "- **Formula**: $E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta)g_t^2$\n",
    "- **Update**: $w = w - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} g_t$\n",
    "\n",
    "#### Adam (Adaptive Moment Estimation)\n",
    "- **Combines**: Momentum + RMSprop\n",
    "- **First moment**: $m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$\n",
    "- **Second moment**: $v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$\n",
    "- **Bias correction**: $\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$, $\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$\n",
    "- **Update**: $w = w - \\frac{\\alpha \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
    "\n",
    "### 3. Learning Rate Scheduling\n",
    "\n",
    "#### Common Schedules\n",
    "- **Step Decay**: Reduce by factor every N epochs\n",
    "- **Exponential Decay**: $\\alpha = \\alpha_0 e^{-kt}$\n",
    "- **Cosine Annealing**: Smooth decrease following cosine curve\n",
    "- **Warm Restart**: Cyclical learning rate with restarts\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with Adam**: Good default choice\n",
    "2. **Try SGD with momentum**: For fine-tuning or when Adam plateaus\n",
    "3. **Use learning rate scheduling**: Helps find better optima\n",
    "4. **Monitor training**: Adjust based on loss curves\n",
    "5. **Experiment**: Different optimizers work better for different problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimizer-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimization algorithms\n",
    "print(\"=== OPTIMIZATION ALGORITHMS COMPARISON ===\")\n",
    "print()\n",
    "\n",
    "# Define different optimizers to test\n",
    "optimizers_config = {\n",
    "    'SGD': optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'RMSprop': optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': optimizers.Adam(learning_rate=0.001),\n",
    "    'AdaGrad': optimizers.Adagrad(learning_rate=0.01),\n",
    "    'Adam + Decay': optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "}\n",
    "\n",
    "# Function to create model with specific optimizer\n",
    "def create_model_with_optimizer(optimizer):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Test each optimizer\n",
    "optimizer_results = {}\n",
    "optimizer_histories = {}\n",
    "\n",
    "print(\"Training with different optimizers...\")\n",
    "print()\n",
    "\n",
    "for name, optimizer in optimizers_config.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model_with_optimizer(optimizer)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=50,  # Fewer epochs for comparison\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Get final validation loss (for convergence analysis)\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "    \n",
    "    # Store results\n",
    "    optimizer_results[name] = {\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'final_val_loss': final_val_loss,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch,\n",
    "        'convergence_speed': 50 - best_epoch  # Lower is faster\n",
    "    }\n",
    "    optimizer_histories[name] = history\n",
    "    \n",
    "    print(f\"  Test Acc: {test_acc:.4f}, Best Val Loss: {best_val_loss:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZER COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_opt = pd.DataFrame(optimizer_results).T\n",
    "df_opt = df_opt.sort_values('best_val_loss', ascending=True)\n",
    "\n",
    "print(f\"{'Optimizer':<15} {'Test Acc':<9} {'Best Val Loss':<13} {'Best Epoch':<10} {'Convergence':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, row in df_opt.iterrows():\n",
    "    convergence = \"Fast\" if row['best_epoch'] <= 20 else \"Medium\" if row['best_epoch'] <= 35 else \"Slow\"\n",
    "    print(f\"{name:<15} {row['test_acc']:<9.4f} {row['best_val_loss']:<13.4f} {row['best_epoch']:<10.0f} {convergence:<12}\")\n",
    "\n",
    "print()\n",
    "best_optimizer = df_opt.index[0]\n",
    "print(f\"ðŸ† Best Optimizer: {best_optimizer}\")\n",
    "print(f\"   Test Accuracy: {df_opt.iloc[0]['test_acc']:.4f}\")\n",
    "print(f\"   Best Validation Loss: {df_opt.iloc[0]['best_val_loss']:.4f}\")\n",
    "print(f\"   Convergence: Epoch {df_opt.iloc[0]['best_epoch']:.0f}\")\n",
    "\n",
    "# Find fastest convergence\n",
    "fastest_optimizer = df_opt['best_epoch'].idxmin()\n",
    "print(f\"\\nâš¡ Fastest Convergence: {fastest_optimizer} (epoch {df_opt.loc[fastest_optimizer, 'best_epoch']:.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimizer-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimizer comparison\n",
    "print(\"=== OPTIMIZER TRAINING CURVES ===\")\n",
    "print()\n",
    "\n",
    "# Plot training curves for different optimizers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "# Training Loss\n",
    "for i, (name, history) in enumerate(optimizer_histories.items()):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    axes[0,0].plot(epochs, history.history['loss'], color=colors[i], \n",
    "                  label=name, linewidth=2, alpha=0.8)\n",
    "axes[0,0].set_title('Training Loss by Optimizer')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Binary Cross-entropy Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# Validation Loss\n",
    "for i, (name, history) in enumerate(optimizer_histories.items()):\n",
    "    epochs = range(1, len(history.history['val_loss']) + 1)\n",
    "    axes[0,1].plot(epochs, history.history['val_loss'], color=colors[i], \n",
    "                  label=name, linewidth=2, alpha=0.8)\n",
    "axes[0,1].set_title('Validation Loss by Optimizer')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Binary Cross-entropy Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_ylim(0.3, 0.8)\n",
    "\n",
    "# Training Accuracy\n",
    "for i, (name, history) in enumerate(optimizer_histories.items()):\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    axes[1,0].plot(epochs, history.history['accuracy'], color=colors[i], \n",
    "                  label=name, linewidth=2, alpha=0.8)\n",
    "axes[1,0].set_title('Training Accuracy by Optimizer')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].set_ylim(0.7, 1.0)\n",
    "\n",
    "# Validation Accuracy\n",
    "for i, (name, history) in enumerate(optimizer_histories.items()):\n",
    "    epochs = range(1, len(history.history['val_accuracy']) + 1)\n",
    "    axes[1,1].plot(epochs, history.history['val_accuracy'], color=colors[i], \n",
    "                  label=name, linewidth=2, alpha=0.8)\n",
    "axes[1,1].set_title('Validation Accuracy by Optimizer')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].set_ylim(0.7, 0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convergence analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot convergence speed vs final performance\n",
    "optimizers_list = list(optimizer_results.keys())\n",
    "convergence_epochs = [optimizer_results[opt]['best_epoch'] for opt in optimizers_list]\n",
    "test_accuracies = [optimizer_results[opt]['test_acc'] for opt in optimizers_list]\n",
    "\n",
    "plt.scatter(convergence_epochs, test_accuracies, s=100, alpha=0.7)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, opt in enumerate(optimizers_list):\n",
    "    plt.annotate(opt, (convergence_epochs[i], test_accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.xlabel('Convergence Speed (Best Epoch)')\n",
    "plt.ylabel('Final Test Accuracy')\n",
    "plt.title('Optimizer Convergence Speed vs Final Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add ideal region (top-left: fast + accurate)\n",
    "plt.axhline(y=np.mean(test_accuracies), color='red', linestyle='--', alpha=0.5, label='Mean Accuracy')\n",
    "plt.axvline(x=np.mean(convergence_epochs), color='red', linestyle='--', alpha=0.5, label='Mean Convergence')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Optimizer Analysis Summary:\")\n",
    "print(f\"   â€¢ Best overall: {best_optimizer} (accuracy + convergence)\")\n",
    "print(f\"   â€¢ Fastest convergence: {fastest_optimizer}\")\n",
    "print(f\"   â€¢ SGD variants: Slower but steady convergence\")\n",
    "print(f\"   â€¢ Adam variants: Fast initial convergence, good final performance\")\n",
    "print(f\"   â€¢ RMSprop: Good middle ground between speed and stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-applications",
   "metadata": {},
   "source": [
    "## 8. When to Use Neural Networks\n",
    "\n",
    "### Neural Networks Excel At:\n",
    "\n",
    "#### 1. Complex Pattern Recognition\n",
    "- **Image Classification**: CNNs for visual patterns\n",
    "- **Natural Language Processing**: RNNs/Transformers for text\n",
    "- **Speech Recognition**: Deep networks for audio processing\n",
    "- **Time Series**: LSTMs/GRUs for temporal patterns\n",
    "\n",
    "#### 2. High-Dimensional Data\n",
    "- **Feature Learning**: Automatic feature extraction\n",
    "- **Representation Learning**: Learn meaningful embeddings\n",
    "- **Dimensionality Reduction**: Autoencoders for compression\n",
    "\n",
    "#### 3. Non-Linear Relationships\n",
    "- **Complex Decision Boundaries**: XOR and beyond\n",
    "- **Interaction Effects**: Capture feature interactions\n",
    "- **Hierarchical Patterns**: Multi-level abstractions\n",
    "\n",
    "### When NOT to Use Neural Networks:\n",
    "\n",
    "#### 1. Small Datasets\n",
    "- **< 1000 samples**: Traditional ML often better\n",
    "- **Simple patterns**: Linear/tree models sufficient\n",
    "- **Limited compute**: Resource-intensive training\n",
    "\n",
    "#### 2. Interpretability Required\n",
    "- **Medical diagnosis**: Need explainable decisions\n",
    "- **Legal applications**: Regulatory requirements\n",
    "- **Business rules**: Transparent decision-making\n",
    "\n",
    "#### 3. Tabular Data with Simple Patterns\n",
    "- **Tree-based models**: Often outperform on structured data\n",
    "- **Linear relationships**: Logistic regression sufficient\n",
    "- **Quick prototyping**: Faster to train and tune\n",
    "\n",
    "### Best Practices for Tabular Data:\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "- **Scaling**: Always standardize/normalize features\n",
    "- **Encoding**: Handle categorical variables properly\n",
    "- **Missing values**: Imputation or special encoding\n",
    "\n",
    "#### 2. Architecture Guidelines\n",
    "- **Start simple**: 2-3 hidden layers\n",
    "- **Layer size**: 1-2x input features\n",
    "- **Activation**: ReLU for hidden layers\n",
    "- **Output**: Sigmoid/softmax for classification\n",
    "\n",
    "#### 3. Training Strategy\n",
    "- **Regularization**: Dropout, batch norm, weight decay\n",
    "- **Early stopping**: Monitor validation loss\n",
    "- **Ensemble**: Combine multiple models\n",
    "- **Cross-validation**: Robust performance estimation\n",
    "\n",
    "### Comparison with Other Algorithms:\n",
    "\n",
    "| Dataset Type | Best Choice | Why |\n",
    "|--------------|-------------|-----|\n",
    "| **Small tabular** | Random Forest, XGBoost | Less overfitting, faster training |\n",
    "| **Large tabular** | Neural Networks, LightGBM | Scale with data size |\n",
    "| **Images** | CNNs | Spatial pattern recognition |\n",
    "| **Text** | Transformers, RNNs | Sequential pattern modeling |\n",
    "| **Time series** | LSTMs, ARIMA | Temporal dependencies |\n",
    "| **Interpretable** | Linear models, Decision trees | Transparent decisions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison with other algorithms\n",
    "print(\"=== NEURAL NETWORKS VS OTHER ALGORITHMS ===\")\n",
    "print()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "# Compare different algorithm families\n",
    "algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural Network (Sklearn)': MLPClassifier(hidden_layer_sizes=(64, 32), \n",
    "                                             max_iter=500, random_state=42),\n",
    "}\n",
    "\n",
    "# Test each algorithm\n",
    "comparison_results = {}\n",
    "\n",
    "print(\"Comparing Neural Networks with other algorithms...\")\n",
    "print()\n",
    "\n",
    "for name, model in algorithms.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Time training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if name == 'Neural Network (Sklearn)':\n",
    "        model.fit(X_train_scaled, y_train)  # Use scaled data for NN\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)  # Use original data for others\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'training_time': training_time,\n",
    "        'interpretability': 'High' if name in ['Logistic Regression', 'Decision Tree'] \n",
    "                           else 'Medium' if name in ['Random Forest', 'XGBoost'] \n",
    "                           else 'Low'\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Time: {training_time:.2f}s\")\n",
    "\n",
    "# Add our best Keras neural network results\n",
    "comparison_results['Neural Network (Keras)'] = {\n",
    "    'accuracy': test_acc,  # From our best model earlier\n",
    "    'auc': auc_score,\n",
    "    'training_time': 10.0,  # Approximate based on earlier training\n",
    "    'interpretability': 'Low'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ALGORITHM COMPARISON RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_comp = pd.DataFrame(comparison_results).T\n",
    "df_comp = df_comp.sort_values('auc', ascending=False)\n",
    "\n",
    "print(f\"{'Algorithm':<25} {'Accuracy':<9} {'AUC':<7} {'Time (s)':<8} {'Interpretability':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for name, row in df_comp.iterrows():\n",
    "    time_str = f\"{row['training_time']:.2f}\" if row['training_time'] < 100 else f\"{row['training_time']:.0f}\"\n",
    "    print(f\"{name:<25} {row['accuracy']:<9.4f} {row['auc']:<7.4f} {time_str:<8} {row['interpretability']:<15}\")\n",
    "\n",
    "print()\n",
    "best_overall = df_comp.index[0]\n",
    "print(f\"ðŸ† Best Overall Performance: {best_overall}\")\n",
    "print(f\"   AUC: {df_comp.iloc[0]['auc']:.4f}\")\n",
    "print(f\"   Accuracy: {df_comp.iloc[0]['accuracy']:.4f}\")\n",
    "\n",
    "# Find fastest\n",
    "fastest = df_comp['training_time'].idxmin()\n",
    "print(f\"\\nâš¡ Fastest Training: {fastest} ({df_comp.loc[fastest, 'training_time']:.2f}s)\")\n",
    "\n",
    "# Most interpretable with good performance\n",
    "interpretable = df_comp[df_comp['interpretability'] == 'High']\n",
    "if len(interpretable) > 0:\n",
    "    best_interpretable = interpretable['auc'].idxmax()\n",
    "    print(f\"ðŸ” Best Interpretable: {best_interpretable} (AUC: {interpretable.loc[best_interpretable, 'auc']:.4f})\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ“Š Algorithm Selection Guidelines:\")\n",
    "print(f\"   â€¢ For this dataset size ({X_train.shape[0]} samples): Tree-based models excel\")\n",
    "print(f\"   â€¢ Neural networks competitive but require more tuning\")\n",
    "print(f\"   â€¢ Linear models provide good baseline + interpretability\")\n",
    "print(f\"   â€¢ Ensemble methods (RF, XGBoost) often best for tabular data\")\n",
    "print(f\"   â€¢ Neural networks shine with larger datasets (>10k samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### ðŸŽ¯ What You've Learned\n",
    "\n",
    "1. **Neural Network Fundamentals**: From perceptron to deep learning\n",
    "2. **Forward/Backward Propagation**: The learning mechanism\n",
    "3. **Architecture Design**: Layers, neurons, and network topology\n",
    "4. **Activation Functions**: ReLU, Sigmoid, Tanh and their properties\n",
    "5. **Regularization**: Dropout, batch norm, early stopping\n",
    "6. **Optimization**: SGD, Adam, RMSprop and learning rates\n",
    "7. **Practical Implementation**: Keras/TensorFlow workflows\n",
    "8. **When to Use**: Neural networks vs other algorithms\n",
    "\n",
    "### ðŸš€ Neural Network Strengths\n",
    "\n",
    "âœ… **Universal Approximation**: Can learn any continuous function\n",
    "âœ… **Automatic Feature Learning**: No manual feature engineering\n",
    "âœ… **Scalability**: Performance improves with more data\n",
    "âœ… **Flexibility**: Adaptable to many problem types\n",
    "âœ… **Non-linear**: Handle complex decision boundaries\n",
    "âœ… **End-to-end**: Learn from raw data to predictions\n",
    "\n",
    "### âš ï¸ Neural Network Limitations\n",
    "\n",
    "âŒ **Data Hungry**: Require large datasets to shine\n",
    "âŒ **Black Box**: Limited interpretability\n",
    "âŒ **Computationally Expensive**: Training can be slow\n",
    "âŒ **Hyperparameter Sensitive**: Many parameters to tune\n",
    "âŒ **Overfitting Prone**: Need careful regularization\n",
    "âŒ **Local Optima**: Non-convex optimization landscape\n",
    "\n",
    "### ðŸ› ï¸ Best Practices Checklist\n",
    "\n",
    "#### Data Preparation\n",
    "- [ ] **Scale features**: StandardScaler or MinMaxScaler\n",
    "- [ ] **Handle missing values**: Imputation or special encoding\n",
    "- [ ] **Encode categoricals**: One-hot or embedding layers\n",
    "- [ ] **Split data properly**: Train/validation/test sets\n",
    "\n",
    "#### Architecture Design\n",
    "- [ ] **Start simple**: 2-3 hidden layers initially\n",
    "- [ ] **Layer size**: 50-200 neurons for tabular data\n",
    "- [ ] **Activation functions**: ReLU for hidden, sigmoid/softmax for output\n",
    "- [ ] **Output layer**: Match problem type (binary, multi-class)\n",
    "\n",
    "#### Training Strategy\n",
    "- [ ] **Use regularization**: Dropout (0.2-0.5), batch normalization\n",
    "- [ ] **Early stopping**: Monitor validation loss with patience\n",
    "- [ ] **Learning rate**: Start with Adam optimizer defaults\n",
    "- [ ] **Batch size**: 32-128 for most problems\n",
    "\n",
    "#### Validation & Tuning\n",
    "- [ ] **Cross-validation**: K-fold for robust estimates\n",
    "- [ ] **Hyperparameter tuning**: Grid/random search\n",
    "- [ ] **Monitor training**: Plot loss/accuracy curves\n",
    "- [ ] **Ensemble methods**: Combine multiple models\n",
    "\n",
    "### ðŸŽ¯ When to Choose Neural Networks\n",
    "\n",
    "#### Perfect For:\n",
    "- **Large datasets** (>10k samples)\n",
    "- **Complex patterns** (images, text, audio)\n",
    "- **High-dimensional data** (many features)\n",
    "- **Non-linear relationships**\n",
    "- **Feature learning** (automatic pattern discovery)\n",
    "- **End-to-end learning** (raw input to output)\n",
    "\n",
    "#### Consider Alternatives For:\n",
    "- **Small datasets** (<1k samples) â†’ Tree-based models\n",
    "- **Simple patterns** â†’ Linear models\n",
    "- **Interpretability needs** â†’ Logistic regression, decision trees\n",
    "- **Quick prototyping** â†’ Random Forest, XGBoost\n",
    "- **Tabular data** â†’ Gradient boosting often better\n",
    "\n",
    "### ðŸ’¡ Pro Tips\n",
    "\n",
    "#### Training Tips\n",
    "1. **Start with baseline**: Simple model first\n",
    "2. **Monitor overfitting**: Watch train vs validation gap\n",
    "3. **Learning rate scheduling**: Reduce when plateauing\n",
    "4. **Batch normalization**: Often improves training stability\n",
    "5. **Ensemble different architectures**: Combine for best results\n",
    "\n",
    "#### Debugging Tips\n",
    "1. **Loss not decreasing**: Check learning rate, gradients\n",
    "2. **Overfitting quickly**: Add regularization, reduce capacity\n",
    "3. **Underfitting**: Increase capacity, reduce regularization\n",
    "4. **Unstable training**: Reduce learning rate, add batch norm\n",
    "5. **Poor generalization**: More data, better regularization\n",
    "\n",
    "### ðŸŒŸ Advanced Topics to Explore\n",
    "\n",
    "1. **Convolutional Neural Networks (CNNs)**: For image data\n",
    "2. **Recurrent Neural Networks (RNNs/LSTMs)**: For sequences\n",
    "3. **Transformer Architecture**: State-of-the-art for NLP\n",
    "4. **Transfer Learning**: Leverage pre-trained models\n",
    "5. **Generative Models**: VAEs, GANs for data generation\n",
    "6. **Neural Architecture Search**: Automated architecture design\n",
    "7. **Model Compression**: Pruning, quantization for deployment\n",
    "8. **Explainable AI**: Techniques for neural network interpretability\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a solid understanding of neural networks for classification. You've learned the fundamentals, implemented practical solutions, and understand when to use these powerful algorithms.\n",
    "\n",
    "**Remember**: Neural networks are incredibly powerful tools, but they're not always the best choice. Start with the simplest model that works, then add complexity as needed. The key to success is understanding your data, choosing the right architecture, and training with good practices.\n",
    "\n",
    "Keep experimenting, keep learning, and most importantly - have fun exploring the fascinating world of neural networks! ðŸš€ðŸ§ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}