{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Transformers for Classification: The Revolution in Deep Learning\n",
    "\n",
    "Welcome to your comprehensive guide to **Transformers for Classification**! This notebook will take you through the revolutionary architecture that changed the landscape of machine learning, from its origins in natural language processing to its applications across various domains.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Transformer Architecture**: Attention mechanisms and self-attention\n",
    "2. **BERT and Beyond**: Pre-trained transformer models\n",
    "3. **Text Classification**: Using transformers for NLP tasks\n",
    "4. **Vision Transformers**: Applying transformers to image data\n",
    "5. **Tabular Transformers**: Modern approaches to structured data\n",
    "6. **Transfer Learning**: Leveraging pre-trained models\n",
    "7. **Fine-tuning Strategies**: Adapting models to specific tasks\n",
    "8. **Practical Implementation**: Using Hugging Face Transformers\n",
    "9. **Performance Optimization**: Efficient training and inference\n",
    "10. **Comparison with Other Methods**: When to use transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer-revolution",
   "metadata": {},
   "source": [
    "## 1. The Transformer Revolution\n",
    "\n",
    "### \"Attention Is All You Need\" (2017)\n",
    "\n",
    "The transformer architecture, introduced in the seminal paper \"Attention Is All You Need\" by Vaswani et al., revolutionized deep learning by:\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1. Self-Attention Mechanism\n",
    "Instead of processing sequences step-by-step (RNNs) or using convolutions (CNNs), transformers use **self-attention** to:\n",
    "- Process all positions simultaneously\n",
    "- Capture long-range dependencies efficiently\n",
    "- Allow parallel computation\n",
    "\n",
    "#### 2. No Recurrence or Convolution\n",
    "- **Faster Training**: Parallel processing of sequences\n",
    "- **Better Long-range Dependencies**: Direct connections between distant positions\n",
    "- **Scalability**: Efficient on modern hardware (GPUs/TPUs)\n",
    "\n",
    "#### 3. Transfer Learning Revolution\n",
    "- **Pre-training**: Learn general representations on large datasets\n",
    "- **Fine-tuning**: Adapt to specific tasks with minimal data\n",
    "- **Universal Architecture**: Same model for various tasks\n",
    "\n",
    "### The Attention Mechanism\n",
    "\n",
    "**Core Idea**: For each position, determine how much attention to pay to every other position.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$: Query matrix\n",
    "- $K$: Key matrix\n",
    "- $V$: Value matrix\n",
    "- $d_k$: Dimension of keys (for scaling)\n",
    "\n",
    "### Why Transformers Work\n",
    "\n",
    "ðŸŽ¯ **Global Context**: Each position can attend to all other positions\n",
    "âš¡ **Parallelization**: No sequential dependencies in computation\n",
    "ðŸ”„ **Transfer Learning**: Pre-trained models work across tasks\n",
    "ðŸ“ˆ **Scalability**: Performance improves with model size and data\n",
    "ðŸŒ **Universality**: Same architecture for text, images, and more\n",
    "\n",
    "### Timeline of Transformer Evolution\n",
    "\n",
    "- **2017**: Original Transformer (Vaswani et al.)\n",
    "- **2018**: BERT - Bidirectional representations\n",
    "- **2019**: GPT-2 - Large-scale language modeling\n",
    "- **2020**: GPT-3 - Few-shot learning capabilities\n",
    "- **2020**: Vision Transformer (ViT) - Images as sequences\n",
    "- **2021**: TabTransformer - Transformers for tabular data\n",
    "- **2022-2024**: ChatGPT, GPT-4, and the AI revolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "from utils.data_utils import load_titanic_data\n",
    "from utils.evaluation import ModelEvaluator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[START] Transformers for Classification Tutorial\")\n",
    "print(\"ðŸ“¦ Libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-mechanism",
   "metadata": {},
   "source": [
    "## 2. Understanding the Attention Mechanism\n",
    "\n",
    "### The Intuition Behind Attention\n",
    "\n",
    "Imagine reading a sentence and trying to understand the meaning of each word:\n",
    "- Some words depend heavily on other words in the sentence\n",
    "- The importance of these dependencies varies\n",
    "- We want to **attend** to the most relevant words for each position\n",
    "\n",
    "### Example: \"The cat that chased the mouse was fast\"\n",
    "When processing \"was\":\n",
    "- High attention to \"cat\" (subject of the verb)\n",
    "- Lower attention to \"the\", \"that\", \"mouse\"\n",
    "- Medium attention to \"chased\" (provides context)\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of using a single attention function, transformers use **multiple attention heads**:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "**Benefits**:\n",
    "- Different heads can focus on different types of relationships\n",
    "- Increased model capacity\n",
    "- Better representation learning\n",
    "\n",
    "### Self-Attention vs Cross-Attention\n",
    "\n",
    "#### Self-Attention\n",
    "- $Q$, $K$, $V$ all come from the same sequence\n",
    "- Each position attends to all positions in the same sequence\n",
    "- Used in encoder layers\n",
    "\n",
    "#### Cross-Attention\n",
    "- $Q$ from one sequence, $K$ and $V$ from another\n",
    "- Used in decoder layers (attending to encoder output)\n",
    "- Common in sequence-to-sequence tasks\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Since attention doesn't inherently understand position, we add **positional encodings**:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$\n",
    "\n",
    "Where:\n",
    "- $pos$: position in sequence\n",
    "- $i$: dimension index\n",
    "- $d$: model dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement basic attention mechanism from scratch\n",
    "print(\"=== ATTENTION MECHANISM IMPLEMENTATION ===\")\n",
    "print()\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simple scaled dot-product attention\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.sqrt_d = math.sqrt(d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.sqrt_d\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = SimpleAttention(self.d_k)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections in batch from d_model => h x d_k\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention on all the projected vectors in batch\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.w_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding using sine and cosine functions\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_length=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Test the attention mechanism\n",
    "print(\"Testing Attention Mechanism:\")\n",
    "print()\n",
    "\n",
    "# Create sample data\n",
    "batch_size, seq_len, d_model = 2, 10, 64\n",
    "num_heads = 8\n",
    "\n",
    "# Sample input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Test positional encoding\n",
    "pos_encoding = PositionalEncoding(d_model)\n",
    "x_with_pos = pos_encoding(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"With positional encoding: {x_with_pos.shape}\")\n",
    "print()\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attention_weights = mha(x_with_pos, x_with_pos, x_with_pos)\n",
    "\n",
    "print(f\"Multi-head attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Number of attention heads: {num_heads}\")\n",
    "print(f\"Attention dimension per head: {d_model // num_heads}\")\n",
    "print()\n",
    "\n",
    "# Verify attention weights sum to 1\n",
    "weights_sum = attention_weights.sum(dim=-1)\n",
    "print(f\"Attention weights sum (should be ~1.0): {weights_sum[0, 0, 0]:.4f}\")\n",
    "print(f\"All weights sum to 1: {torch.allclose(weights_sum, torch.ones_like(weights_sum))}\")\n",
    "\n",
    "print(\"\\nâœ… Attention mechanism implemented and tested successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "print(\"=== ATTENTION PATTERN VISUALIZATION ===\")\n",
    "print()\n",
    "\n",
    "def visualize_attention(attention_weights, tokens=None, head_idx=0):\n",
    "    \"\"\"Visualize attention weights as heatmap\"\"\"\n",
    "    # Take first sample, specific head\n",
    "    attn = attention_weights[0, head_idx].detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(attn, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='Blues',\n",
    "                xticklabels=tokens if tokens else range(attn.shape[1]),\n",
    "                yticklabels=tokens if tokens else range(attn.shape[0]))\n",
    "    \n",
    "    plt.title(f'Attention Weights (Head {head_idx})')\n",
    "    plt.xlabel('Keys (Attend to)')\n",
    "    plt.ylabel('Queries (Attend from)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a simple sequence for visualization\n",
    "seq_len = 8\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "\n",
    "# Create simple input (each position has different pattern)\n",
    "simple_x = torch.zeros(1, seq_len, d_model)\n",
    "for i in range(seq_len):\n",
    "    simple_x[0, i, :] = torch.randn(d_model) + i  # Add position-dependent bias\n",
    "\n",
    "# Apply positional encoding\n",
    "pos_enc = PositionalEncoding(d_model)\n",
    "simple_x_pos = pos_enc(simple_x)\n",
    "\n",
    "# Get attention\n",
    "simple_mha = MultiHeadAttention(d_model, num_heads)\n",
    "simple_output, simple_attn = simple_mha(simple_x_pos, simple_x_pos, simple_x_pos)\n",
    "\n",
    "# Create token labels for visualization\n",
    "tokens = [f\"Token_{i}\" for i in range(seq_len)]\n",
    "\n",
    "print(f\"Visualizing attention patterns for {num_heads} heads:\")\n",
    "print()\n",
    "\n",
    "# Visualize different attention heads\n",
    "for head in range(min(4, num_heads)):  # Show up to 4 heads\n",
    "    print(f\"\\nAttention Head {head}:\")\n",
    "    visualize_attention(simple_attn, tokens, head)\n",
    "\n",
    "# Analyze attention patterns\n",
    "print(\"\\nðŸ“Š Attention Pattern Analysis:\")\n",
    "avg_self_attention = torch.diagonal(simple_attn[0], dim1=-2, dim2=-1).mean(dim=1)\n",
    "print(f\"Average self-attention per head: {avg_self_attention.numpy()}\")\n",
    "print(\"Self-attention: how much each token attends to itself\")\n",
    "\n",
    "# Check attention diversity across heads\n",
    "head_similarities = []\n",
    "for i in range(num_heads):\n",
    "    for j in range(i+1, num_heads):\n",
    "        sim = F.cosine_similarity(\n",
    "            simple_attn[0, i].flatten(),\n",
    "            simple_attn[0, j].flatten(),\n",
    "            dim=0\n",
    "        )\n",
    "        head_similarities.append(sim.item())\n",
    "\n",
    "avg_similarity = np.mean(head_similarities)\n",
    "print(f\"\\nAverage similarity between attention heads: {avg_similarity:.3f}\")\n",
    "print(\"Lower similarity indicates more diverse attention patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer-architecture",
   "metadata": {},
   "source": [
    "## 3. Complete Transformer Architecture\n",
    "\n",
    "### Transformer Block Components\n",
    "\n",
    "A standard Transformer block consists of:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "2. **Add & Norm** (Residual connection + Layer Normalization)\n",
    "3. **Feed-Forward Network** (Position-wise)\n",
    "4. **Add & Norm** (Another residual connection)\n",
    "\n",
    "### Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "**Characteristics**:\n",
    "- Applied independently to each position\n",
    "- Usually: $d_{ff} = 4 \\times d_{model}$\n",
    "- Adds non-linearity to the model\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$, $\\sigma$: mean and std across features (not batch)\n",
    "- $\\gamma$, $\\beta$: learnable parameters\n",
    "- Applied before or after sub-layers (Pre-LN vs Post-LN)\n",
    "\n",
    "### Residual Connections\n",
    "\n",
    "$$\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "**Benefits**:\n",
    "- Helps gradient flow in deep networks\n",
    "- Enables training of very deep transformers\n",
    "- Provides skip connections for information flow\n",
    "\n",
    "### Encoder vs Decoder\n",
    "\n",
    "#### Encoder Stack\n",
    "- **Self-attention**: Bidirectional (can see all positions)\n",
    "- **Use case**: BERT, classification tasks\n",
    "- **Parallel processing**: All positions processed simultaneously\n",
    "\n",
    "#### Decoder Stack\n",
    "- **Masked self-attention**: Causal (only past positions)\n",
    "- **Cross-attention**: Attends to encoder output\n",
    "- **Use case**: GPT, generation tasks\n",
    "- **Autoregressive**: Generates one token at a time\n",
    "\n",
    "### Classification with Transformers\n",
    "\n",
    "For classification tasks:\n",
    "1. **Add [CLS] token** at the beginning\n",
    "2. **Pass through transformer layers**\n",
    "3. **Use [CLS] representation** for classification\n",
    "4. **Add classification head** (linear layer)\n",
    "\n",
    "$$\\text{prediction} = \\text{softmax}(W \\cdot \\text{[CLS]} + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer implementation\n",
    "print(\"=== COMPLETE TRANSFORMER IMPLEMENTATION ===\")\n",
    "print()\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single Transformer encoder block\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, attn_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Stack of Transformer encoder blocks\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attention_weights = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, mask)\n",
    "            attention_weights.append(attn)\n",
    "            \n",
    "        return x, attention_weights\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"Transformer for sequence classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, \n",
    "                 d_ff, max_length, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Special tokens\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through encoder\n",
    "        x, attention_weights = self.encoder(x, mask)\n",
    "        \n",
    "        # Use CLS token for classification\n",
    "        cls_output = x[:, 0]  # First token is CLS\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# Test the complete transformer\n",
    "print(\"Testing Complete Transformer Architecture:\")\n",
    "print()\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 1000\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "max_length = 50\n",
    "num_classes = 2\n",
    "batch_size = 4\n",
    "seq_len = 20\n",
    "\n",
    "# Create model\n",
    "transformer = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_length=max_length,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Sample input (token indices)\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "logits, attention_weights = transformer(sample_input)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  Model dimension: {d_model}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Number of layers: {num_layers}\")\n",
    "print(f\"  Feed-forward dimension: {d_ff}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print()\n",
    "\n",
    "print(f\"Input/Output Shapes:\")\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output logits shape: {logits.shape}\")\n",
    "print(f\"  Number of attention layers: {len(attention_weights)}\")\n",
    "print(f\"  Attention weights shape per layer: {attention_weights[0].shape}\")\n",
    "print()\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size (MB): {total_params * 4 / (1024**2):.2f}\")\n",
    "print()\n",
    "\n",
    "# Test prediction\n",
    "with torch.no_grad():\n",
    "    predictions = F.softmax(logits, dim=-1)\n",
    "    predicted_classes = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "print(f\"Sample Predictions:\")\n",
    "for i in range(min(batch_size, 3)):\n",
    "    pred_class = predicted_classes[i].item()\n",
    "    confidence = predictions[i, pred_class].item()\n",
    "    print(f\"  Sample {i}: Class {pred_class} (confidence: {confidence:.3f})\")\n",
    "\n",
    "print(\"\\nâœ… Complete Transformer architecture implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tabular-transformers",
   "metadata": {},
   "source": [
    "## 4. Transformers for Tabular Data\n",
    "\n",
    "### Challenges with Tabular Data\n",
    "\n",
    "Traditional transformers were designed for sequences (text, speech), but tabular data has different characteristics:\n",
    "\n",
    "#### Key Differences\n",
    "- **No natural order**: Features don't have sequential relationships\n",
    "- **Mixed data types**: Numerical and categorical features\n",
    "- **Different scales**: Features may have vastly different ranges\n",
    "- **Feature interactions**: Complex non-linear relationships\n",
    "\n",
    "### TabTransformer Approach\n",
    "\n",
    "The TabTransformer (Huang et al., 2020) adapts transformers for tabular data:\n",
    "\n",
    "#### Architecture Components\n",
    "\n",
    "1. **Categorical Embeddings**: Convert categorical features to dense vectors\n",
    "2. **Column Embeddings**: Add column-specific embeddings (like positional encoding)\n",
    "3. **Transformer Layers**: Apply self-attention across features\n",
    "4. **Feature Fusion**: Combine transformed categorical with numerical features\n",
    "5. **Classification Head**: Final layers for prediction\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For categorical features:\n",
    "$$e_i = \\text{Embedding}(x_i) + \\text{ColEmbedding}(i)$$\n",
    "\n",
    "For numerical features:\n",
    "$$n_j = \\text{LayerNorm}(x_j)$$\n",
    "\n",
    "Final prediction:\n",
    "$$\\hat{y} = \\text{MLP}([\\text{Transformer}(E); N])$$\n",
    "\n",
    "Where:\n",
    "- $E$: Categorical embeddings matrix\n",
    "- $N$: Numerical features vector\n",
    "- $[;]$: Concatenation operation\n",
    "\n",
    "### Advantages of TabTransformer\n",
    "\n",
    "âœ… **Feature Interactions**: Captures complex relationships between features\n",
    "âœ… **Interpretability**: Attention weights show feature importance\n",
    "âœ… **Robustness**: Less prone to overfitting than deep MLPs\n",
    "âœ… **Scalability**: Handles high-cardinality categorical features well\n",
    "âœ… **Transfer Learning**: Can leverage pre-trained representations\n",
    "\n",
    "### When to Use TabTransformer\n",
    "\n",
    "- **Many categorical features** with high cardinality\n",
    "- **Complex feature interactions** expected\n",
    "- **Medium to large datasets** (>10k samples)\n",
    "- **When interpretability** matters\n",
    "- **Mixed data types** (categorical + numerical)\n",
    "\n",
    "### Comparison with Traditional Methods\n",
    "\n",
    "| Method | Categorical Handling | Feature Interactions | Interpretability | Training Speed |\n",
    "|--------|---------------------|---------------------|------------------|----------------|\n",
    "| **XGBoost** | Manual encoding | Limited | Medium | Fast |\n",
    "| **Neural Networks** | Manual encoding | Good | Low | Medium |\n",
    "| **TabTransformer** | Automatic embeddings | Excellent | High | Slow |\n",
    "| **Random Forest** | Manual encoding | Limited | Medium | Fast |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tabular-transformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement TabTransformer for tabular data\n",
    "print(\"=== TABTRANSFORMER IMPLEMENTATION ===\")\n",
    "print()\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    \"\"\"Transformer architecture adapted for tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_dims, numerical_features, \n",
    "                 d_model=64, num_heads=8, num_layers=3, \n",
    "                 d_ff=256, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.numerical_features = numerical_features\n",
    "        self.num_categorical = len(categorical_dims)\n",
    "        \n",
    "        # Categorical embeddings\n",
    "        self.categorical_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, d_model) for dim in categorical_dims\n",
    "        ])\n",
    "        \n",
    "        # Column embeddings (like positional encoding for features)\n",
    "        self.column_embeddings = nn.Embedding(self.num_categorical, d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        \n",
    "        # Numerical feature processing\n",
    "        self.numerical_layer_norm = nn.LayerNorm(numerical_features) if numerical_features > 0 else None\n",
    "        \n",
    "        # Classification head\n",
    "        classifier_input_dim = d_model * self.num_categorical + numerical_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_ff // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, categorical_features, numerical_features=None):\n",
    "        batch_size = categorical_features.size(0)\n",
    "        \n",
    "        # Process categorical features\n",
    "        categorical_embeddings = []\n",
    "        for i, embedding_layer in enumerate(self.categorical_embeddings):\n",
    "            # Get embedding for each categorical feature\n",
    "            cat_emb = embedding_layer(categorical_features[:, i])\n",
    "            \n",
    "            # Add column embedding\n",
    "            col_emb = self.column_embeddings(torch.tensor(i, device=cat_emb.device))\n",
    "            cat_emb = cat_emb + col_emb\n",
    "            \n",
    "            categorical_embeddings.append(cat_emb)\n",
    "        \n",
    "        # Stack categorical embeddings: [batch_size, num_categorical, d_model]\n",
    "        categorical_embeddings = torch.stack(categorical_embeddings, dim=1)\n",
    "        \n",
    "        # Apply transformer to categorical features\n",
    "        transformed_categorical, attention_weights = self.transformer(categorical_embeddings)\n",
    "        \n",
    "        # Flatten transformed categorical features\n",
    "        transformed_categorical = transformed_categorical.view(batch_size, -1)\n",
    "        \n",
    "        # Combine with numerical features if available\n",
    "        if numerical_features is not None and self.numerical_features > 0:\n",
    "            # Normalize numerical features\n",
    "            normalized_numerical = self.numerical_layer_norm(numerical_features)\n",
    "            # Concatenate categorical and numerical features\n",
    "            combined_features = torch.cat([transformed_categorical, normalized_numerical], dim=1)\n",
    "        else:\n",
    "            combined_features = transformed_categorical\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(combined_features)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# Custom dataset class for tabular data\n",
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_data, numerical_data, labels):\n",
    "        self.categorical_data = torch.tensor(categorical_data, dtype=torch.long)\n",
    "        self.numerical_data = torch.tensor(numerical_data, dtype=torch.float32) if numerical_data is not None else None\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'categorical': self.categorical_data[idx],\n",
    "            'numerical': self.numerical_data[idx] if self.numerical_data is not None else None,\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Load and preprocess Titanic data for TabTransformer\n",
    "print(\"Preparing Titanic dataset for TabTransformer...\")\n",
    "print()\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Training samples: {len(train_df)}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "print(f\"  Feature names: {feature_names}\")\n",
    "print()\n",
    "\n",
    "# For demonstration, let's create some categorical features\n",
    "# We'll bin some numerical features to create categories\n",
    "def create_categorical_features(df):\n",
    "    df_cat = df.copy()\n",
    "    \n",
    "    # Create age groups (assuming 'Age' exists or we use first feature)\n",
    "    age_col = feature_names[0]  # Use first feature as age proxy\n",
    "    df_cat['AgeGroup'] = pd.cut(df_cat[age_col], bins=5, labels=False).fillna(0).astype(int)\n",
    "    \n",
    "    # Create fare groups (using second feature)\n",
    "    if len(feature_names) > 1:\n",
    "        fare_col = feature_names[1]\n",
    "        df_cat['FareGroup'] = pd.cut(df_cat[fare_col], bins=4, labels=False).fillna(0).astype(int)\n",
    "    else:\n",
    "        df_cat['FareGroup'] = 0\n",
    "    \n",
    "    # Create a simple categorical based on feature combinations\n",
    "    df_cat['Category'] = ((df_cat[feature_names[0]] > df_cat[feature_names[0]].mean()).astype(int) + \n",
    "                         (df_cat[feature_names[-1]] > df_cat[feature_names[-1]].mean()).astype(int))\n",
    "    \n",
    "    return df_cat\n",
    "\n",
    "train_with_cat = create_categorical_features(train_df)\n",
    "test_with_cat = create_categorical_features(test_df)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['AgeGroup', 'FareGroup', 'Category']\n",
    "numerical_features = [f for f in feature_names if f not in categorical_features]\n",
    "\n",
    "# Get categorical dimensions (number of unique values for each categorical feature)\n",
    "categorical_dims = []\n",
    "for cat_feat in categorical_features:\n",
    "    unique_vals = max(train_with_cat[cat_feat].max(), test_with_cat[cat_feat].max()) + 1\n",
    "    categorical_dims.append(unique_vals)\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Categorical dimensions: {categorical_dims}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print()\n",
    "\n",
    "# Prepare data for TabTransformer\n",
    "X_train_cat = train_with_cat[categorical_features].values\n",
    "X_test_cat = test_with_cat[categorical_features].values\n",
    "X_train_num = train_with_cat[numerical_features].values if numerical_features else None\n",
    "X_test_num = test_with_cat[numerical_features].values if numerical_features else None\n",
    "\n",
    "# Standardize numerical features\n",
    "if X_train_num is not None:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "# Create TabTransformer model\n",
    "tab_transformer = TabTransformer(\n",
    "    categorical_dims=categorical_dims,\n",
    "    numerical_features=len(numerical_features) if numerical_features else 0,\n",
    "    d_model=32,  # Smaller for this demo\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_ff=128,\n",
    "    num_classes=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"TabTransformer Model:\")\n",
    "print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "print(f\"  Numerical features: {len(numerical_features)}\")\n",
    "print(f\"  Model dimension: 32\")\n",
    "print(f\"  Attention heads: 4\")\n",
    "print(f\"  Transformer layers: 2\")\n",
    "print()\n",
    "\n",
    "# Test forward pass\n",
    "sample_cat = torch.tensor(X_train_cat[:4], dtype=torch.long)\n",
    "sample_num = torch.tensor(X_train_num[:4], dtype=torch.float32) if X_train_num is not None else None\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, attention = tab_transformer(sample_cat, sample_num)\n",
    "    predictions = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(f\"Forward Pass Test:\")\n",
    "print(f\"  Input categorical shape: {sample_cat.shape}\")\n",
    "if sample_num is not None:\n",
    "    print(f\"  Input numerical shape: {sample_num.shape}\")\n",
    "print(f\"  Output logits shape: {logits.shape}\")\n",
    "print(f\"  Attention layers: {len(attention)}\")\n",
    "print(f\"  Sample predictions: {predictions[0].numpy()}\")\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = sum(p.numel() for p in tab_transformer.parameters())\n",
    "print(f\"\\nTabTransformer Parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\nâœ… TabTransformer implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-tabtransformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TabTransformer on Titanic dataset\n",
    "print(\"=== TRAINING TABTRANSFORMER ===\")\n",
    "print()\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TabularDataset(X_train_cat, X_train_num, y_train)\n",
    "test_dataset = TabularDataset(X_test_cat, X_test_num, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training Setup:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print()\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tab_transformer.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(tab_transformer.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        categorical = batch['categorical'].to(device)\n",
    "        numerical = batch['numerical'].to(device) if batch['numerical'] is not None else None\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _ = model(categorical, numerical)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            categorical = batch['categorical'].to(device)\n",
    "            numerical = batch['numerical'].to(device) if batch['numerical'] is not None else None\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits, _ = model(categorical, numerical)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return (total_loss / len(loader), correct / total, \n",
    "            np.array(all_predictions), np.array(all_probabilities), np.array(all_labels))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_val_acc = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Training TabTransformer...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(tab_transformer, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_preds, val_probs, val_labels = evaluate_model(\n",
    "        tab_transformer, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        best_predictions = val_preds\n",
    "        best_probabilities = val_probs\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} (epoch {best_epoch+1})\")\n",
    "\n",
    "# Calculate final metrics\n",
    "auc_score = roc_auc_score(val_labels, best_probabilities[:, 1])\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Test Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"  Test AUC: {auc_score:.4f}\")\n",
    "print(f\"  Best epoch: {best_epoch + 1}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, best_predictions, target_names=['Died', 'Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-tabtransformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TabTransformer results and attention patterns\n",
    "print(\"=== TABTRANSFORMER ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "axes[0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(x=best_epoch+1, color='green', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-entropy Loss')\n",
    "axes[0].set_title('TabTransformer Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(epochs, train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(epochs, val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axvline(x=best_epoch+1, color='green', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('TabTransformer Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze attention patterns\n",
    "print(\"\\nAnalyzing Attention Patterns:\")\n",
    "print()\n",
    "\n",
    "# Get attention weights for a sample\n",
    "tab_transformer.eval()\n",
    "with torch.no_grad():\n",
    "    sample_cat = torch.tensor(X_test_cat[:1], dtype=torch.long).to(device)\n",
    "    sample_num = torch.tensor(X_test_num[:1], dtype=torch.float32).to(device) if X_test_num is not None else None\n",
    "    \n",
    "    _, attention_weights = tab_transformer(sample_cat, sample_num)\n",
    "\n",
    "# Average attention across heads and layers\n",
    "avg_attention = torch.stack(attention_weights).mean(dim=0).mean(dim=1)  # Average across layers and heads\n",
    "avg_attention = avg_attention[0].cpu().numpy()  # Take first sample\n",
    "\n",
    "# Create attention heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(avg_attention, \n",
    "            annot=True, \n",
    "            fmt='.3f', \n",
    "            cmap='Blues',\n",
    "            xticklabels=categorical_features,\n",
    "            yticklabels=categorical_features)\n",
    "plt.title('Feature-to-Feature Attention Weights\\n(Averaged across layers and heads)')\n",
    "plt.xlabel('Attended Features')\n",
    "plt.ylabel('Attending Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "# Calculate how much each feature is attended to (column sums)\n",
    "feature_importance = avg_attention.sum(axis=0)\n",
    "feature_importance_normalized = feature_importance / feature_importance.sum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categorical_features, feature_importance_normalized, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Categorical Features')\n",
    "plt.ylabel('Normalized Attention Score')\n",
    "plt.title('Feature Importance from Attention Weights')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, importance in zip(bars, feature_importance_normalized):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{importance:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Feature Importance (from attention):\")\n",
    "for feat, imp in zip(categorical_features, feature_importance_normalized):\n",
    "    print(f\"  {feat}: {imp:.3f}\")\n",
    "print()\n",
    "\n",
    "# Compare with traditional model (for reference)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare data for traditional models (combine categorical and numerical)\n",
    "X_train_combined = np.concatenate([X_train_cat, X_train_num], axis=1) if X_train_num is not None else X_train_cat\n",
    "X_test_combined = np.concatenate([X_test_cat, X_test_num], axis=1) if X_test_num is not None else X_test_cat\n",
    "feature_names_combined = categorical_features + (numerical_features if numerical_features else [])\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_combined, y_train)\n",
    "rf_pred = rf_model.predict(X_test_combined)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "rf_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test_combined)[:, 1])\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_combined, y_train)\n",
    "lr_pred = lr_model.predict(X_test_combined)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_model.predict_proba(X_test_combined)[:, 1])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'AUC':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'TabTransformer':<20} {best_val_acc:<10.4f} {auc_score:<10.4f} {total_params:<12,}\")\n",
    "print(f\"{'Random Forest':<20} {rf_acc:<10.4f} {rf_auc:<10.4f} {'~100K':<12}\")\n",
    "print(f\"{'Logistic Regression':<20} {lr_acc:<10.4f} {lr_auc:<10.4f} {len(feature_names_combined)+1:<12}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Insights:\")\n",
    "if best_val_acc > max(rf_acc, lr_acc):\n",
    "    print(\"   âœ… TabTransformer achieved the best accuracy\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Traditional models performed better (likely due to small dataset size)\")\n",
    "\n",
    "print(f\"   ðŸ” Most important feature: {categorical_features[np.argmax(feature_importance_normalized)]}\")\n",
    "print(f\"   ðŸ“ˆ TabTransformer provides interpretable attention weights\")\n",
    "print(f\"   âš¡ Traditional models train faster but TabTransformer scales better\")\n",
    "print(f\"   ðŸŽ¯ TabTransformer excels with larger datasets and more complex interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer-applications",
   "metadata": {},
   "source": [
    "## 5. Modern Transformer Applications\n",
    "\n",
    "### Text Classification with Pre-trained Models\n",
    "\n",
    "The transformer revolution in NLP brought us powerful pre-trained models:\n",
    "\n",
    "#### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **Architecture**: Encoder-only transformer\n",
    "- **Training**: Masked Language Modeling + Next Sentence Prediction\n",
    "- **Strength**: Understanding context from both directions\n",
    "- **Use cases**: Classification, question answering, sentiment analysis\n",
    "\n",
    "#### RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
    "- **Improvements**: Better training procedure, more data\n",
    "- **Performance**: Often outperforms BERT\n",
    "- **Training**: Only Masked Language Modeling (no NSP)\n",
    "\n",
    "#### DistilBERT\n",
    "- **Size**: 66% smaller than BERT\n",
    "- **Speed**: 60% faster inference\n",
    "- **Performance**: Retains 97% of BERT's performance\n",
    "- **Use case**: Production deployments with speed constraints\n",
    "\n",
    "### Vision Transformers (ViT)\n",
    "\n",
    "Transformers adapted for computer vision:\n",
    "\n",
    "#### Key Innovations\n",
    "1. **Image Patches**: Treat image patches as \"tokens\"\n",
    "2. **Linear Projection**: Convert patches to embeddings\n",
    "3. **Position Embeddings**: 2D positional encoding\n",
    "4. **Classification Token**: Special [CLS] token for image classification\n",
    "\n",
    "#### Mathematical Formulation\n",
    "$$\\text{Image} \\rightarrow \\text{Patches} \\rightarrow \\text{Linear Projection} \\rightarrow \\text{Transformer} \\rightarrow \\text{Classification}$$\n",
    "\n",
    "### Multimodal Transformers\n",
    "\n",
    "Combining different data types:\n",
    "\n",
    "#### CLIP (Contrastive Language-Image Pre-training)\n",
    "- **Joint Training**: Text and image encoders\n",
    "- **Zero-shot**: Classification without task-specific training\n",
    "- **Applications**: Image search, visual question answering\n",
    "\n",
    "#### DALL-E / Stable Diffusion\n",
    "- **Text-to-Image**: Generate images from text descriptions\n",
    "- **Architecture**: Transformer decoders with attention mechanisms\n",
    "- **Applications**: Creative AI, content generation\n",
    "\n",
    "### Transformer Variants\n",
    "\n",
    "#### Efficient Transformers\n",
    "- **Linformer**: Linear attention complexity\n",
    "- **Performer**: Fast attention via random features\n",
    "- **Reformer**: Reversible layers, locality-sensitive hashing\n",
    "\n",
    "#### Specialized Architectures\n",
    "- **Switch Transformer**: Sparse expert models\n",
    "- **Swin Transformer**: Hierarchical vision transformer\n",
    "- **DeBERTa**: Disentangled attention mechanisms\n",
    "\n",
    "### Why Transformers Dominate\n",
    "\n",
    "#### Technical Advantages\n",
    "1. **Parallelization**: Efficient training on modern hardware\n",
    "2. **Long-range Dependencies**: Capture relationships across long sequences\n",
    "3. **Transfer Learning**: Pre-trained models work across tasks\n",
    "4. **Interpretability**: Attention weights provide insights\n",
    "5. **Scalability**: Performance improves with model size\n",
    "\n",
    "#### Ecosystem Benefits\n",
    "1. **Hugging Face**: Easy access to pre-trained models\n",
    "2. **Hardware Support**: Optimized for GPUs and TPUs\n",
    "3. **Research Community**: Rapid innovation and improvements\n",
    "4. **Industry Adoption**: Used by major tech companies\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "#### Emerging Trends\n",
    "- **Multimodal Models**: Combining text, images, audio, video\n",
    "- **Efficient Architectures**: Reducing computational requirements\n",
    "- **Foundation Models**: Large, general-purpose models\n",
    "- **Specialized Applications**: Domain-specific transformers\n",
    "\n",
    "#### Challenges\n",
    "- **Computational Cost**: Large models require significant resources\n",
    "- **Data Requirements**: Need large datasets for training\n",
    "- **Interpretability**: Understanding what models learn\n",
    "- **Bias and Fairness**: Addressing model biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-analysis",
   "metadata": {},
   "source": [
    "## 6. When to Use Transformers: A Comprehensive Guide\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Scenario | Best Choice | Why | Alternatives |\n",
    "|----------|-------------|-----|-------------|\n",
    "| **Text Classification** | Pre-trained BERT/RoBERTa | Superior language understanding | CNN, LSTM, Traditional ML |\n",
    "| **Image Classification** | Vision Transformer (ViT) | State-of-the-art on large datasets | CNN (ResNet, EfficientNet) |\n",
    "| **Small Tabular Data (<10k)** | XGBoost, Random Forest | Faster, less prone to overfitting | TabTransformer |\n",
    "| **Large Tabular Data (>100k)** | TabTransformer | Captures complex interactions | Neural Networks, XGBoost |\n",
    "| **Multimodal Tasks** | CLIP, DALL-E variants | Designed for multiple modalities | Separate models + fusion |\n",
    "| **Real-time Applications** | DistilBERT, Efficient models | Speed optimization | Traditional ML, smaller models |\n",
    "| **Limited Computational Resources** | Traditional ML, small CNNs | Resource efficiency | Cloud-based transformer APIs |\n",
    "| **High Interpretability Needed** | TabTransformer (attention weights) | Attention provides insights | Linear models, decision trees |\n",
    "\n",
    "### Transformer Advantages\n",
    "\n",
    "#### âœ… Use Transformers When:\n",
    "\n",
    "1. **Large Datasets Available**\n",
    "   - >10k samples for tabular data\n",
    "   - >100k samples for vision tasks\n",
    "   - Transfer learning possible with smaller datasets\n",
    "\n",
    "2. **Complex Pattern Recognition Needed**\n",
    "   - Long-range dependencies in sequences\n",
    "   - Complex feature interactions in tabular data\n",
    "   - Multimodal understanding required\n",
    "\n",
    "3. **State-of-the-art Performance Required**\n",
    "   - Research or competition settings\n",
    "   - Performance is more important than speed\n",
    "   - Resources available for training/inference\n",
    "\n",
    "4. **Transfer Learning Beneficial**\n",
    "   - Pre-trained models available for your domain\n",
    "   - Fine-tuning can achieve good results with limited data\n",
    "   - Domain adaptation needed\n",
    "\n",
    "5. **Interpretability Through Attention**\n",
    "   - Understanding model decisions important\n",
    "   - Attention weights provide meaningful insights\n",
    "   - Feature importance analysis needed\n",
    "\n",
    "#### âŒ Avoid Transformers When:\n",
    "\n",
    "1. **Small Datasets**\n",
    "   - <1k samples without pre-training\n",
    "   - Simple patterns that don't require complex models\n",
    "   - Risk of overfitting outweighs benefits\n",
    "\n",
    "2. **Real-time Constraints**\n",
    "   - Millisecond response time requirements\n",
    "   - Mobile or edge deployment\n",
    "   - Limited computational resources\n",
    "\n",
    "3. **Simple Problems**\n",
    "   - Linear relationships dominate\n",
    "   - Traditional methods achieve good performance\n",
    "   - Complexity doesn't justify the overhead\n",
    "\n",
    "4. **Strict Interpretability Requirements**\n",
    "   - Regulatory compliance needs full explainability\n",
    "   - Medical or legal applications\n",
    "   - Rule-based decisions required\n",
    "\n",
    "5. **Resource Constraints**\n",
    "   - Limited GPU/TPU access\n",
    "   - Cost-sensitive applications\n",
    "   - Simple deployment requirements\n",
    "\n",
    "### Best Practices for Transformer Usage\n",
    "\n",
    "#### 1. Start with Pre-trained Models\n",
    "- Use Hugging Face Model Hub\n",
    "- Fine-tune rather than training from scratch\n",
    "- Consider domain-specific pre-trained models\n",
    "\n",
    "#### 2. Optimize for Your Use Case\n",
    "- DistilBERT for speed-critical applications\n",
    "- RoBERTa for maximum accuracy\n",
    "- Mobile-optimized models for edge deployment\n",
    "\n",
    "#### 3. Proper Data Preprocessing\n",
    "- Text: Appropriate tokenization, handling special characters\n",
    "- Tabular: Categorical embeddings, numerical normalization\n",
    "- Images: Proper resizing, patch extraction\n",
    "\n",
    "#### 4. Training Strategies\n",
    "- Use learning rate scheduling\n",
    "- Implement early stopping\n",
    "- Monitor validation metrics carefully\n",
    "- Use appropriate regularization (dropout, weight decay)\n",
    "\n",
    "#### 5. Production Considerations\n",
    "- Model quantization for smaller size\n",
    "- ONNX conversion for faster inference\n",
    "- Batch processing for efficiency\n",
    "- Caching for repeated inputs\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "#### Development Costs\n",
    "- **High**: Initial setup, training infrastructure\n",
    "- **Medium**: Fine-tuning pre-trained models\n",
    "- **Low**: Using transformer APIs (GPT-3, BERT APIs)\n",
    "\n",
    "#### Computational Costs\n",
    "- **Training**: High GPU/TPU requirements\n",
    "- **Inference**: Moderate to high, depends on model size\n",
    "- **Storage**: Large model files (100MB to several GB)\n",
    "\n",
    "#### Performance Benefits\n",
    "- **Accuracy**: Often state-of-the-art results\n",
    "- **Generalization**: Good transfer learning capabilities\n",
    "- **Flexibility**: Same architecture for different tasks\n",
    "\n",
    "### Hybrid Approaches\n",
    "\n",
    "#### Ensemble Methods\n",
    "- Combine transformers with traditional models\n",
    "- Use transformers for feature extraction, traditional ML for final prediction\n",
    "- Weighted voting between different model types\n",
    "\n",
    "#### Two-Stage Approaches\n",
    "- Fast filtering with traditional models\n",
    "- Detailed analysis with transformers\n",
    "- Reduces computational cost while maintaining accuracy\n",
    "\n",
    "#### Progressive Enhancement\n",
    "- Start with simple models\n",
    "- Add transformer components gradually\n",
    "- Measure ROI at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison of different approaches\n",
    "print(\"=== COMPREHENSIVE METHOD COMPARISON ===\")\n",
    "print()\n",
    "\n",
    "import time\n",
    "\n",
    "# We'll compare our TabTransformer with other methods on the same dataset\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Prepare comparison data\n",
    "methods = {}\n",
    "\n",
    "# Traditional ML models\n",
    "models_traditional = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"Testing traditional machine learning methods...\")\n",
    "print()\n",
    "\n",
    "for name, model in models_traditional.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use combined features for traditional models\n",
    "    model.fit(X_train_combined, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predictions\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(X_test_combined)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    probabilities = model.predict_proba(X_test_combined)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    \n",
    "    # Estimate model size (very rough)\n",
    "    if hasattr(model, 'coef_'):\n",
    "        model_size = model.coef_.size * 8  # bytes\n",
    "    elif hasattr(model, 'n_features_in_'):\n",
    "        model_size = model.n_features_in_ * 1000  # rough estimate\n",
    "    else:\n",
    "        model_size = 100000  # default estimate\n",
    "    \n",
    "    methods[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'training_time': training_time,\n",
    "        'prediction_time': prediction_time * 1000,  # ms\n",
    "        'model_size_mb': model_size / (1024 * 1024),\n",
    "        'interpretability': 'High' if name == 'Logistic Regression' else 'Medium' if 'Forest' in name else 'Low',\n",
    "        'complexity': 'Low' if name in ['Logistic Regression'] else 'Medium' if name in ['Random Forest', 'SVM'] else 'High'\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Training: {training_time:.2f}s\")\n",
    "\n",
    "# Add TabTransformer results\n",
    "tab_transformer_training_time = num_epochs * len(train_loader) * 0.01  # Rough estimate\n",
    "tab_transformer_pred_time = len(y_test) * 0.001  # Rough estimate per sample in ms\n",
    "\n",
    "methods['TabTransformer'] = {\n",
    "    'accuracy': best_val_acc,\n",
    "    'auc': auc_score,\n",
    "    'training_time': tab_transformer_training_time,\n",
    "    'prediction_time': tab_transformer_pred_time,\n",
    "    'model_size_mb': total_params * 4 / (1024 * 1024),\n",
    "    'interpretability': 'Medium',  # Attention weights provide some interpretability\n",
    "    'complexity': 'High'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPREHENSIVE METHOD COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(methods).T\n",
    "comparison_df = comparison_df.sort_values('auc', ascending=False)\n",
    "\n",
    "print(f\"{'Method':<20} {'Accuracy':<9} {'AUC':<7} {'Train(s)':<8} {'Pred(ms)':<8} {'Size(MB)':<9} {'Interpret.':<11} {'Complex.':<9}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for name, row in comparison_df.iterrows():\n",
    "    train_time_str = f\"{row['training_time']:.1f}\" if row['training_time'] < 100 else f\"{row['training_time']:.0f}\"\n",
    "    pred_time_str = f\"{row['prediction_time']:.1f}\" if row['prediction_time'] < 100 else f\"{row['prediction_time']:.0f}\"\n",
    "    size_str = f\"{row['model_size_mb']:.1f}\" if row['model_size_mb'] < 10 else f\"{row['model_size_mb']:.0f}\"\n",
    "    \n",
    "    print(f\"{name:<20} {row['accuracy']:<9.4f} {row['auc']:<7.4f} {train_time_str:<8} {pred_time_str:<8} {size_str:<9} {row['interpretability']:<11} {row['complexity']:<9}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Analysis and recommendations\n",
    "best_accuracy = comparison_df['accuracy'].idxmax()\n",
    "best_auc = comparison_df['auc'].idxmax()\n",
    "fastest_training = comparison_df['training_time'].idxmin()\n",
    "fastest_prediction = comparison_df['prediction_time'].idxmin()\n",
    "smallest_model = comparison_df['model_size_mb'].idxmin()\n",
    "\n",
    "print(\"ðŸ† PERFORMANCE LEADERS:\")\n",
    "print(f\"   Best Accuracy: {best_accuracy} ({comparison_df.loc[best_accuracy, 'accuracy']:.4f})\")\n",
    "print(f\"   Best AUC: {best_auc} ({comparison_df.loc[best_auc, 'auc']:.4f})\")\n",
    "print(f\"   Fastest Training: {fastest_training} ({comparison_df.loc[fastest_training, 'training_time']:.1f}s)\")\n",
    "print(f\"   Fastest Prediction: {fastest_prediction} ({comparison_df.loc[fastest_prediction, 'prediction_time']:.1f}ms)\")\n",
    "print(f\"   Smallest Model: {smallest_model} ({comparison_df.loc[smallest_model, 'model_size_mb']:.1f}MB)\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“Š METHOD ANALYSIS:\")\n",
    "print()\n",
    "\n",
    "# Traditional ML analysis\n",
    "traditional_methods = [name for name in comparison_df.index if name != 'TabTransformer']\n",
    "best_traditional = comparison_df.loc[traditional_methods].sort_values('auc', ascending=False).index[0]\n",
    "\n",
    "print(f\"Traditional ML:\")\n",
    "print(f\"   â€¢ Best performer: {best_traditional}\")\n",
    "print(f\"   â€¢ Advantages: Fast training, small models, good interpretability\")\n",
    "print(f\"   â€¢ Best for: Small datasets, production constraints, baseline models\")\n",
    "print()\n",
    "\n",
    "print(f\"TabTransformer:\")\n",
    "tab_rank = list(comparison_df.index).index('TabTransformer') + 1\n",
    "print(f\"   â€¢ Performance rank: #{tab_rank} out of {len(comparison_df)}\")\n",
    "print(f\"   â€¢ Advantages: Feature interactions, attention interpretability, scalability\")\n",
    "print(f\"   â€¢ Best for: Large datasets, complex patterns, when interpretability matters\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸŽ¯ RECOMMENDATIONS:\")\n",
    "print()\n",
    "\n",
    "dataset_size = len(y_train)\n",
    "if dataset_size < 1000:\n",
    "    rec = \"Use traditional ML (Random Forest or Gradient Boosting)\"\n",
    "    reason = \"Small dataset size favors traditional methods\"\n",
    "elif dataset_size < 10000:\n",
    "    rec = \"Start with traditional ML, consider TabTransformer if complex patterns expected\"\n",
    "    reason = \"Medium dataset - test both approaches\"\n",
    "else:\n",
    "    rec = \"TabTransformer likely to perform better with proper tuning\"\n",
    "    reason = \"Large dataset can leverage transformer advantages\"\n",
    "\n",
    "print(f\"For this dataset ({dataset_size} samples):\")\n",
    "print(f\"   â€¢ Recommendation: {rec}\")\n",
    "print(f\"   â€¢ Reason: {reason}\")\n",
    "print()\n",
    "\n",
    "print(\"General Guidelines:\")\n",
    "print(\"   â€¢ Speed critical: Use Logistic Regression or Random Forest\")\n",
    "print(\"   â€¢ Maximum accuracy: Use ensemble of top performers\")\n",
    "print(\"   â€¢ Interpretability: Use Logistic Regression or TabTransformer (attention)\")\n",
    "print(\"   â€¢ Production deployment: Consider model size and prediction speed\")\n",
    "print(\"   â€¢ Research/competition: Use TabTransformer or ensemble methods\")\n",
    "\n",
    "print(\"\\nâœ… Comprehensive comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### ðŸŽ¯ What You've Learned\n",
    "\n",
    "1. **Transformer Architecture**: Attention mechanisms, self-attention, and multi-head attention\n",
    "2. **Mathematical Foundations**: Scaled dot-product attention, positional encoding\n",
    "3. **Complete Implementation**: From basic attention to full transformer models\n",
    "4. **TabTransformer**: Adapting transformers for tabular data classification\n",
    "5. **Practical Applications**: Text, vision, and multimodal transformers\n",
    "6. **Performance Analysis**: When to use transformers vs traditional methods\n",
    "7. **Real-world Considerations**: Training, deployment, and optimization strategies\n",
    "\n",
    "### ðŸš€ Transformer Revolution Impact\n",
    "\n",
    "#### Why Transformers Changed Everything\n",
    "\n",
    "âœ… **Parallelization**: Unlike RNNs, can process all positions simultaneously\n",
    "âœ… **Long-range Dependencies**: Direct connections between distant elements\n",
    "âœ… **Transfer Learning**: Pre-trained models work across domains and tasks\n",
    "âœ… **Scalability**: Performance improves with model size and data\n",
    "âœ… **Universality**: Same architecture works for text, images, tabular data\n",
    "âœ… **Interpretability**: Attention weights provide insight into model decisions\n",
    "\n",
    "#### Key Technical Innovations\n",
    "\n",
    "1. **Self-Attention**: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "2. **Multi-Head Attention**: Multiple attention mechanisms in parallel\n",
    "3. **Positional Encoding**: Inject sequence order information\n",
    "4. **Layer Normalization**: Stabilize training in deep networks\n",
    "5. **Residual Connections**: Enable training of very deep models\n",
    "\n",
    "### ðŸ› ï¸ Practical Implementation Guide\n",
    "\n",
    "#### For Text Classification\n",
    "```python\n",
    "# Use pre-trained models from Hugging Face\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "```\n",
    "\n",
    "#### For Tabular Data\n",
    "```python\n",
    "# Implement TabTransformer architecture\n",
    "model = TabTransformer(\n",
    "    categorical_dims=categorical_dims,\n",
    "    numerical_features=num_features,\n",
    "    d_model=64, num_heads=8\n",
    ")\n",
    "```\n",
    "\n",
    "#### Training Best Practices\n",
    "- Start with pre-trained models when available\n",
    "- Use learning rate scheduling (warmup + decay)\n",
    "- Apply dropout and weight decay for regularization\n",
    "- Monitor validation metrics with early stopping\n",
    "- Use gradient clipping for stability\n",
    "\n",
    "### ðŸ“Š Decision Framework: When to Use Transformers\n",
    "\n",
    "#### âœ… Perfect for Transformers\n",
    "- **Large datasets** (>10k samples)\n",
    "- **Complex patterns** and feature interactions\n",
    "- **Sequential or structured data** (text, images as patches)\n",
    "- **Transfer learning opportunities** (pre-trained models available)\n",
    "- **State-of-the-art performance** required\n",
    "- **Interpretability** through attention weights needed\n",
    "\n",
    "#### âŒ Consider Alternatives When\n",
    "- **Small datasets** (<1k samples without pre-training)\n",
    "- **Real-time requirements** (millisecond response times)\n",
    "- **Limited computational resources** (mobile, edge devices)\n",
    "- **Simple patterns** adequately handled by traditional ML\n",
    "- **Strict interpretability** requirements (regulatory compliance)\n",
    "\n",
    "#### ðŸ”„ Hybrid Approaches\n",
    "- Use transformers for **feature extraction**, traditional ML for **final prediction**\n",
    "- **Ensemble methods** combining transformers with other models\n",
    "- **Two-stage systems**: Fast filtering + detailed transformer analysis\n",
    "\n",
    "### ðŸŽ¯ Performance Insights from Our Analysis\n",
    "\n",
    "#### Tabular Data Results\n",
    "- **Traditional ML** often wins on small datasets (<10k samples)\n",
    "- **TabTransformer** provides interpretable attention weights\n",
    "- **Gradient Boosting** methods (XGBoost, LightGBM) remain strong baselines\n",
    "- **Ensemble approaches** typically achieve best performance\n",
    "\n",
    "#### Key Metrics Comparison\n",
    "| Metric | Traditional ML | TabTransformer | Winner |\n",
    "|--------|----------------|----------------|--------|\n",
    "| **Training Speed** | âš¡ Fast (seconds) | ðŸŒ Slow (minutes) | Traditional |\n",
    "| **Prediction Speed** | âš¡ Very Fast (<1ms) | ðŸ• Moderate (10ms) | Traditional |\n",
    "| **Model Size** | ðŸ’¾ Small (KB-MB) | ðŸ“¦ Large (10-100MB) | Traditional |\n",
    "| **Feature Interactions** | ðŸ“ˆ Limited | ðŸŽ¯ Excellent | Transformers |\n",
    "| **Interpretability** | ðŸ” Variable | ðŸ‘ï¸ Attention weights | Transformers |\n",
    "| **Scalability** | ðŸ“Š Good | ðŸš€ Excellent | Transformers |\n",
    "\n",
    "### ðŸ’¡ Advanced Tips and Tricks\n",
    "\n",
    "#### Optimization Strategies\n",
    "1. **Model Compression**: Distillation, pruning, quantization\n",
    "2. **Efficient Architectures**: Linformer, Performer, Reformer\n",
    "3. **Mixed Precision Training**: Use FP16 to reduce memory usage\n",
    "4. **Gradient Accumulation**: Simulate larger batch sizes\n",
    "5. **Model Parallelism**: Split large models across devices\n",
    "\n",
    "#### Production Deployment\n",
    "1. **ONNX Conversion**: Optimize inference speed\n",
    "2. **TensorRT/TorchScript**: Further optimization for NVIDIA GPUs\n",
    "3. **Batch Processing**: Group predictions for efficiency\n",
    "4. **Caching**: Store results for repeated inputs\n",
    "5. **Load Balancing**: Distribute inference across multiple instances\n",
    "\n",
    "### ðŸŒŸ Future Directions\n",
    "\n",
    "#### Emerging Trends\n",
    "- **Foundation Models**: Large, general-purpose transformers (GPT, PaLM)\n",
    "- **Multimodal Transformers**: Combining text, image, audio, video\n",
    "- **Efficient Architectures**: Reducing computational requirements\n",
    "- **Specialized Applications**: Domain-specific transformer variants\n",
    "- **Automated Architecture Search**: Finding optimal transformer designs\n",
    "\n",
    "#### Research Frontiers\n",
    "- **In-context Learning**: Learning without parameter updates\n",
    "- **Chain-of-Thought**: Reasoning capabilities in transformers\n",
    "- **Retrieval-Augmented Generation**: Combining transformers with knowledge bases\n",
    "- **Constitutional AI**: Aligning transformer behavior with human values\n",
    "\n",
    "### ðŸŽ“ Learning Path Forward\n",
    "\n",
    "#### Next Steps\n",
    "1. **Experiment with Hugging Face**: Try pre-trained models on your data\n",
    "2. **Implement Vision Transformers**: Apply to image classification tasks\n",
    "3. **Explore Multimodal Models**: CLIP, DALL-E, and similar architectures\n",
    "4. **Study Efficient Variants**: Linformer, Performer, Switch Transformer\n",
    "5. **Build Production Systems**: Deploy transformers at scale\n",
    "\n",
    "#### Resources for Continued Learning\n",
    "- **Papers**: \"Attention Is All You Need\", \"BERT\", \"Vision Transformer\"\n",
    "- **Implementations**: Hugging Face Transformers library\n",
    "- **Courses**: CS224N (Stanford NLP), Fast.ai Deep Learning\n",
    "- **Communities**: Papers With Code, AI/ML Twitter, Reddit r/MachineLearning\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand the transformer architecture that revolutionized machine learning. From the mathematical foundations of attention mechanisms to practical implementations for tabular data, you've gained comprehensive knowledge of when and how to use these powerful models.\n",
    "\n",
    "**Remember**: Transformers are incredibly powerful but not always the best choice. Start with simpler methods, understand your data and constraints, then apply transformers where they provide clear advantages. The key to success is matching the right tool to the right problem.\n",
    "\n",
    "Keep exploring, keep building, and most importantly - have fun pushing the boundaries of what's possible with transformers! ðŸš€ðŸ¤–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}