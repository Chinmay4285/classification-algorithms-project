{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classification: A Complete Guide\n",
    "\n",
    "Welcome to your comprehensive guide to **Support Vector Machine (SVM) classification**! This notebook will teach you about one of the most powerful and theoretically grounded machine learning algorithms.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Geometric Intuition**: How SVM finds the best decision boundary\n",
    "2. **Mathematical Foundation**: Maximum margin principle and support vectors\n",
    "3. **The Kernel Trick**: Handling non-linear relationships\n",
    "4. **Different Kernels**: Linear, polynomial, RBF, and custom kernels\n",
    "5. **Hyperparameter Tuning**: C parameter and kernel parameters\n",
    "6. **Advantages & Limitations**: When to use SVM\n",
    "7. **Practical Implementation**: Real-world applications and optimization\n",
    "8. **Multi-class Classification**: How SVM handles multiple classes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geometric-intuition",
   "metadata": {},
   "source": [
    "## 1. The Geometric Intuition: Finding the Best Boundary\n",
    "\n",
    "### Imagine This Scenario\n",
    "\n",
    "You're a referee in a soccer match, and you need to draw a line to separate two teams on the field. Where would you draw it?\n",
    "\n",
    "**Intuitive Answer**: You'd draw the line as far as possible from both teams, giving maximum \"breathing room\" to both sides.\n",
    "\n",
    "This is exactly what **Support Vector Machine** does!\n",
    "\n",
    "### The SVM Approach\n",
    "\n",
    "Unlike other algorithms that just find \"any\" line that separates classes:\n",
    "- **Logistic Regression**: Finds a line based on probability\n",
    "- **Decision Tree**: Creates rectangular boundaries\n",
    "- **SVM**: Finds the line with **maximum margin** from both classes\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Decision Boundary**: The line (or hyperplane) that separates classes\n",
    "- **Margin**: The distance between the decision boundary and the nearest points\n",
    "- **Support Vectors**: The data points closest to the decision boundary\n",
    "- **Maximum Margin**: SVM finds the boundary that maximizes this distance\n",
    "\n",
    "### Why Maximum Margin?\n",
    "\n",
    "1. **Better Generalization**: Larger margin = more confident predictions\n",
    "2. **Robust to Noise**: Small changes in data won't affect the boundary\n",
    "3. **Unique Solution**: There's only one maximum margin solution\n",
    "4. **Theoretical Guarantees**: Strong mathematical foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.data_utils import load_titanic_data\n",
    "from utils.evaluation import ModelEvaluator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[START] Support Vector Machine Classification Tutorial\")\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-example",
   "metadata": {},
   "source": [
    "## 2. Simple Example: Visualizing the Margin\n",
    "\n",
    "Let's start with a simple 2D example to visualize how SVM works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-simple-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D dataset for visualization\n",
    "print(\"=== SIMPLE SVM VISUALIZATION ===\")\n",
    "print(\"Creating a 2D dataset to visualize SVM concepts\")\n",
    "print()\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_simple, y_simple = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {X_simple.shape}\")\n",
    "print(f\"Classes: {np.unique(y_simple)}\")\n",
    "print(f\"Feature 1 range: [{X_simple[:, 0].min():.2f}, {X_simple[:, 0].max():.2f}]\")\n",
    "print(f\"Feature 2 range: [{X_simple[:, 1].min():.2f}, {X_simple[:, 1].max():.2f}]\")\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple, \n",
    "                     cmap='viridis', s=100, alpha=0.8, edgecolors='black')\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Simple 2D Classification Dataset')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"This is our playground for understanding SVM!\")\n",
    "print(\"We can see two classes that are linearly separable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-boundaries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different decision boundaries\n",
    "print(\"=== COMPARING DECISION BOUNDARIES ===\")\n",
    "print()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create different models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42)\n",
    "}\n",
    "\n",
    "# Train models and visualize boundaries\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    # Create a mesh to plot the decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                        s=100, alpha=0.8, edgecolors='black')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return scatter\n",
    "\n",
    "# Plot each model's decision boundary\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_simple, y_simple)\n",
    "    accuracy = accuracy_score(y_simple, model.predict(X_simple))\n",
    "    \n",
    "    scatter = plot_decision_boundary(model, X_simple, y_simple, axes[i], \n",
    "                                   f'{name}\\nAccuracy: {accuracy:.3f}')\n",
    "    \n",
    "    print(f\"{name}: {accuracy:.3f} accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"Observation: All models achieve perfect accuracy on this simple dataset,\")\n",
    "print(\"but SVM finds the boundary with maximum margin from both classes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-svm-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVM components: margin, support vectors, decision boundary\n",
    "print(\"=== SVM COMPONENTS VISUALIZATION ===\")\n",
    "print()\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear.fit(X_simple, y_simple)\n",
    "\n",
    "# Get support vectors\n",
    "support_vectors = svm_linear.support_vectors_\n",
    "support_vector_indices = svm_linear.support_\n",
    "n_support = svm_linear.n_support_\n",
    "\n",
    "print(f\"Support Vector Analysis:\")\n",
    "print(f\"  Total support vectors: {len(support_vectors)}\")\n",
    "print(f\"  Class 0 support vectors: {n_support[0]}\")\n",
    "print(f\"  Class 1 support vectors: {n_support[1]}\")\n",
    "print(f\"  Support vector indices: {support_vector_indices}\")\n",
    "print()\n",
    "\n",
    "# Create detailed visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot decision boundary\n",
    "h = 0.1\n",
    "x_min, x_max = X_simple[:, 0].min() - 1, X_simple[:, 0].max() + 1\n",
    "y_min, y_max = X_simple[:, 1].min() - 1, X_simple[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = svm_linear.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "plt.contour(xx, yy, Z, colors='black', levels=[-1, 0, 1], alpha=0.5, \n",
    "           linestyles=['--', '-', '--'], linewidths=[2, 3, 2])\n",
    "plt.contourf(xx, yy, Z, alpha=0.2, cmap='RdYlBu')\n",
    "\n",
    "# Plot all data points\n",
    "colors = ['red', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y_simple == i\n",
    "    plt.scatter(X_simple[idx, 0], X_simple[idx, 1], \n",
    "               c=color, s=100, alpha=0.8, \n",
    "               label=f'Class {i}', edgecolors='black')\n",
    "\n",
    "# Highlight support vectors\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
    "           s=300, facecolors='none', edgecolors='yellow', \n",
    "           linewidths=3, label='Support Vectors')\n",
    "\n",
    "# Add annotations\n",
    "plt.text(0.02, 0.98, f'Support Vectors: {len(support_vectors)}', \n",
    "         transform=plt.gca().transAxes, fontsize=12, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "         verticalalignment='top')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM Components: Decision Boundary, Margins, and Support Vectors\\n' +\n",
    "         'Solid line = Decision boundary, Dashed lines = Margin boundaries')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Components:\")\n",
    "print(\"  Black solid line: Decision boundary (where decision_function = 0)\")\n",
    "print(\"  Black dashed lines: Margin boundaries (where decision_function = ±1)\")\n",
    "print(\"  Yellow circles: Support vectors (points that define the margin)\")\n",
    "print(\"  Colored regions: Confidence regions (darker = more confident)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-foundation",
   "metadata": {},
   "source": [
    "## 3. Mathematical Foundation: The Optimization Problem\n",
    "\n",
    "### The SVM Optimization Problem\n",
    "\n",
    "SVM solves a **quadratic optimization problem**:\n",
    "\n",
    "**Objective**: Maximize the margin = Minimize $\\frac{1}{2}||w||^2$\n",
    "\n",
    "**Subject to**: $y_i(w \\cdot x_i + b) \\geq 1$ for all training points\n",
    "\n",
    "Where:\n",
    "- $w$: weight vector (defines the orientation of the boundary)\n",
    "- $b$: bias term (defines the position of the boundary)\n",
    "- $x_i$: training samples\n",
    "- $y_i$: class labels (-1 or +1)\n",
    "\n",
    "### Hard vs Soft Margin\n",
    "\n",
    "**Hard Margin SVM**: \n",
    "- Assumes data is perfectly separable\n",
    "- No misclassifications allowed\n",
    "- Can fail if data has noise or overlap\n",
    "\n",
    "**Soft Margin SVM**:\n",
    "- Allows some misclassifications\n",
    "- Introduces slack variables $\\xi_i$\n",
    "- Controlled by parameter $C$\n",
    "\n",
    "**Soft Margin Formulation**:\n",
    "\n",
    "**Minimize**: $\\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n}\\xi_i$\n",
    "\n",
    "**Subject to**: \n",
    "- $y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i$\n",
    "- $\\xi_i \\geq 0$\n",
    "\n",
    "### The C Parameter\n",
    "\n",
    "- **Large C**: Prioritizes correct classification (small margin, less regularization)\n",
    "- **Small C**: Prioritizes large margin (more regularization, allows misclassifications)\n",
    "\n",
    "This is the **bias-variance tradeoff**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-parameter-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of C parameter\n",
    "print(\"=== C PARAMETER DEMONSTRATION ===\")\n",
    "print()\n",
    "\n",
    "# Create noisy data with some overlap\n",
    "np.random.seed(123)\n",
    "X_noisy, y_noisy = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,  # Smaller separation = more overlap\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Add some noise\n",
    "X_noisy += np.random.normal(0, 0.1, X_noisy.shape)\n",
    "\n",
    "# Test different C values\n",
    "C_values = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "print(\"Effect of C parameter on SVM:\")\n",
    "print(\"C Value | Support Vectors | Training Accuracy\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    # Train SVM with different C values\n",
    "    svm = SVC(kernel='linear', C=C, random_state=42)\n",
    "    svm.fit(X_noisy, y_noisy)\n",
    "    \n",
    "    # Get metrics\n",
    "    n_sv = len(svm.support_vectors_)\n",
    "    train_acc = accuracy_score(y_noisy, svm.predict(X_noisy))\n",
    "    \n",
    "    print(f\"{C:6.1f}   | {n_sv:13d}   | {train_acc:13.3f}\")\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = X_noisy[:, 0].min() - 1, X_noisy[:, 0].max() + 1\n",
    "    y_min, y_max = X_noisy[:, 1].min() - 1, X_noisy[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, colors='black', levels=[-1, 0, 1], alpha=0.5,\n",
    "              linestyles=['--', '-', '--'], linewidths=[1, 2, 1])\n",
    "    ax.contourf(xx, yy, Z, alpha=0.2, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'blue']\n",
    "    for j, color in enumerate(colors):\n",
    "        idx = y_noisy == j\n",
    "        ax.scatter(X_noisy[idx, 0], X_noisy[idx, 1], \n",
    "                  c=color, s=50, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "              s=150, facecolors='none', edgecolors='yellow', linewidths=2)\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(f'C = {C}\\nSVs: {n_sv}, Acc: {train_acc:.3f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"Key Insights:\")\n",
    "print(\"  Small C: More support vectors, wider margin, lower training accuracy\")\n",
    "print(\"  Large C: Fewer support vectors, narrower margin, higher training accuracy\")\n",
    "print(\"  Trade-off: Margin width vs classification accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kernel-trick",
   "metadata": {},
   "source": [
    "## 4. The Kernel Trick: Handling Non-Linear Data\n",
    "\n",
    "### The Problem with Linear Boundaries\n",
    "\n",
    "Real-world data is often **not linearly separable**:\n",
    "- XOR problem\n",
    "- Circular patterns\n",
    "- Complex decision boundaries\n",
    "\n",
    "### The Kernel Trick Solution\n",
    "\n",
    "**Key Idea**: Map data to a higher-dimensional space where it becomes linearly separable!\n",
    "\n",
    "**Original Space**: $\\mathbb{R}^2$ (2D)\n",
    "**Feature Space**: $\\mathbb{R}^{\\infty}$ (infinite dimensional)\n",
    "\n",
    "**Example**: \n",
    "- Original: $(x_1, x_2)$\n",
    "- Mapped: $(x_1, x_2, x_1^2, x_2^2, x_1x_2, \\sqrt{2}x_1x_2, ...)$\n",
    "\n",
    "### Popular Kernels\n",
    "\n",
    "1. **Linear**: $K(x_i, x_j) = x_i \\cdot x_j$\n",
    "2. **Polynomial**: $K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d$\n",
    "3. **RBF (Gaussian)**: $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$\n",
    "4. **Sigmoid**: $K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + r)$\n",
    "\n",
    "### The Magic\n",
    "\n",
    "We **never explicitly compute** the high-dimensional mapping! The kernel function computes the inner product in the transformed space directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kernel-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear dataset to demonstrate kernel trick\n",
    "print(\"=== KERNEL TRICK DEMONSTRATION ===\")\n",
    "print()\n",
    "\n",
    "# Create XOR-like dataset (not linearly separable)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Create circular pattern\n",
    "angles = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "radius_inner = np.random.uniform(0.5, 1.5, n_samples//2)\n",
    "radius_outer = np.random.uniform(2.5, 3.5, n_samples//2)\n",
    "\n",
    "# Inner circle (class 0)\n",
    "X_inner = np.column_stack([\n",
    "    radius_inner * np.cos(angles[:n_samples//2]),\n",
    "    radius_inner * np.sin(angles[:n_samples//2])\n",
    "])\n",
    "\n",
    "# Outer circle (class 1)\n",
    "X_outer = np.column_stack([\n",
    "    radius_outer * np.cos(angles[n_samples//2:]),\n",
    "    radius_outer * np.sin(angles[n_samples//2:])\n",
    "])\n",
    "\n",
    "# Combine data\n",
    "X_circles = np.vstack([X_inner, X_outer])\n",
    "y_circles = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])\n",
    "\n",
    "# Add some noise\n",
    "X_circles += np.random.normal(0, 0.2, X_circles.shape)\n",
    "\n",
    "print(f\"Non-linear dataset created: {X_circles.shape}\")\n",
    "print(f\"Inner circle samples: {np.sum(y_circles == 0)}\")\n",
    "print(f\"Outer circle samples: {np.sum(y_circles == 1)}\")\n",
    "print()\n",
    "\n",
    "# Plot the non-linear data\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y_circles == i\n",
    "    plt.scatter(X_circles[idx, 0], X_circles[idx, 1], \n",
    "               c=color, s=60, alpha=0.7, \n",
    "               label=f'Class {i}', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Non-Linearly Separable Data (Concentric Circles)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"This data cannot be separated by a straight line!\")\n",
    "print(\"We need the kernel trick to handle this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-kernels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different kernels on non-linear data\n",
    "print(\"=== COMPARING DIFFERENT KERNELS ===\")\n",
    "print()\n",
    "\n",
    "# Define different kernels to test\n",
    "kernels = {\n",
    "    'Linear': SVC(kernel='linear', C=1.0, random_state=42),\n",
    "    'Polynomial (degree=2)': SVC(kernel='poly', degree=2, C=1.0, random_state=42),\n",
    "    'Polynomial (degree=3)': SVC(kernel='poly', degree=3, C=1.0, random_state=42),\n",
    "    'RBF (gamma=1)': SVC(kernel='rbf', gamma=1.0, C=1.0, random_state=42),\n",
    "    'RBF (gamma=0.1)': SVC(kernel='rbf', gamma=0.1, C=1.0, random_state=42),\n",
    "    'RBF (gamma=10)': SVC(kernel='rbf', gamma=10.0, C=1.0, random_state=42)\n",
    "}\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "print(\"Kernel Performance Comparison:\")\n",
    "print(\"Kernel                | Training Accuracy | Support Vectors\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Function to plot decision boundary\n",
    "def plot_svm_boundary(svm, X, y, ax, title):\n",
    "    # Create mesh\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Get decision function values\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        idx = y == i\n",
    "        ax.scatter(X[idx, 0], X[idx, 1], c=color, s=50, alpha=0.7,\n",
    "                  edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "              s=100, facecolors='none', edgecolors='yellow', linewidths=2)\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Train and visualize each kernel\n",
    "for i, (name, svm) in enumerate(kernels.items()):\n",
    "    # Train model\n",
    "    svm.fit(X_circles, y_circles)\n",
    "    \n",
    "    # Get performance metrics\n",
    "    train_acc = accuracy_score(y_circles, svm.predict(X_circles))\n",
    "    n_sv = len(svm.support_vectors_)\n",
    "    \n",
    "    print(f\"{name:<20} | {train_acc:13.3f}     | {n_sv:12d}\")\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plot_svm_boundary(svm, X_circles, y_circles, axes[i], \n",
    "                     f'{name}\\nAcc: {train_acc:.3f}, SVs: {n_sv}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"Key Observations:\")\n",
    "print(\"  Linear kernel: Fails on non-linear data (draws straight line)\")\n",
    "print(\"  Polynomial kernels: Can capture some non-linearity\")\n",
    "print(\"  RBF kernel: Excellent for this circular pattern\")\n",
    "print(\"  RBF gamma parameter: Controls flexibility (high gamma = more flexible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rbf-kernel-deep-dive",
   "metadata": {},
   "source": [
    "## 5. Deep Dive: RBF Kernel Parameters\n",
    "\n",
    "The **RBF (Radial Basis Function)** kernel is the most popular kernel for SVM:\n",
    "\n",
    "### RBF Kernel Formula\n",
    "\n",
    "$$K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$$\n",
    "\n",
    "### Gamma Parameter ($\\gamma$)\n",
    "\n",
    "Controls the **influence** of a single training example:\n",
    "\n",
    "- **High $\\gamma$**: \n",
    "  - Close points have high influence\n",
    "  - Far points have very low influence  \n",
    "  - Creates complex, wiggly boundaries\n",
    "  - Risk of overfitting\n",
    "\n",
    "- **Low $\\gamma$**:\n",
    "  - Points have influence over larger distances\n",
    "  - Creates smoother boundaries\n",
    "  - Risk of underfitting\n",
    "\n",
    "### Hyperparameter Interaction\n",
    "\n",
    "**C and $\\gamma$ work together**:\n",
    "- Both control model complexity\n",
    "- Need to tune both simultaneously\n",
    "- Grid search is commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rbf-parameter-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze RBF kernel parameters systematically\n",
    "print(\"=== RBF KERNEL PARAMETER ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# Load real dataset for analysis\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "# Scale features for SVM (important!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset: Titanic survival prediction\")\n",
    "print(f\"Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_scaled.shape[0]}\")\n",
    "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# Test different combinations of C and gamma\n",
    "C_range = [0.1, 1, 10, 100]\n",
    "gamma_range = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "print(\"Grid Search Results (Training Accuracy):\")\n",
    "print(\"C\\\\gamma\", end=\"\")\n",
    "for gamma in gamma_range:\n",
    "    print(f\"{gamma:>8.3f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "results_grid = []\n",
    "for C in C_range:\n",
    "    print(f\"{C:>6.1f}\", end=\"\")\n",
    "    row_results = []\n",
    "    for gamma in gamma_range:\n",
    "        # Train SVM with these parameters\n",
    "        svm = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "        svm.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get training and test accuracy\n",
    "        train_acc = accuracy_score(y_train, svm.predict(X_train_scaled))\n",
    "        test_acc = accuracy_score(y_test, svm.predict(X_test_scaled))\n",
    "        \n",
    "        row_results.append({\n",
    "            'C': C,\n",
    "            'gamma': gamma,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'n_support': len(svm.support_)\n",
    "        })\n",
    "        \n",
    "        print(f\"{train_acc:>8.3f}\", end=\"\")\n",
    "    \n",
    "    results_grid.extend(row_results)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"Grid Search Results (Test Accuracy):\")\n",
    "print(\"C\\\\gamma\", end=\"\")\n",
    "for gamma in gamma_range:\n",
    "    print(f\"{gamma:>8.3f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "idx = 0\n",
    "for C in C_range:\n",
    "    print(f\"{C:>6.1f}\", end=\"\")\n",
    "    for gamma in gamma_range:\n",
    "        test_acc = results_grid[idx]['test_acc']\n",
    "        print(f\"{test_acc:>8.3f}\", end=\"\")\n",
    "        idx += 1\n",
    "    print()\n",
    "\n",
    "# Find best parameters\n",
    "best_result = max(results_grid, key=lambda x: x['test_acc'])\n",
    "print(f\"\\nBest parameters:\")\n",
    "print(f\"  C = {best_result['C']}\")\n",
    "print(f\"  gamma = {best_result['gamma']}\")\n",
    "print(f\"  Test accuracy = {best_result['test_acc']:.3f}\")\n",
    "print(f\"  Support vectors = {best_result['n_support']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-parameter-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter effects\n",
    "print(\"=== VISUALIZING PARAMETER EFFECTS ===\")\n",
    "print()\n",
    "\n",
    "# Create DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results_grid)\n",
    "\n",
    "# Create pivot tables for heatmaps\n",
    "train_acc_pivot = results_df.pivot(index='C', columns='gamma', values='train_acc')\n",
    "test_acc_pivot = results_df.pivot(index='C', columns='gamma', values='test_acc')\n",
    "overfitting_pivot = train_acc_pivot - test_acc_pivot\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Training Accuracy Heatmap\n",
    "sns.heatmap(train_acc_pivot, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "           ax=axes[0,0], cbar_kws={'label': 'Training Accuracy'})\n",
    "axes[0,0].set_title('Training Accuracy vs C and Gamma')\n",
    "axes[0,0].set_xlabel('Gamma')\n",
    "axes[0,0].set_ylabel('C')\n",
    "\n",
    "# Plot 2: Test Accuracy Heatmap  \n",
    "sns.heatmap(test_acc_pivot, annot=True, fmt='.3f', cmap='YlGnBu',\n",
    "           ax=axes[0,1], cbar_kws={'label': 'Test Accuracy'})\n",
    "axes[0,1].set_title('Test Accuracy vs C and Gamma')\n",
    "axes[0,1].set_xlabel('Gamma')\n",
    "axes[0,1].set_ylabel('C')\n",
    "\n",
    "# Plot 3: Overfitting (Train - Test Accuracy)\n",
    "sns.heatmap(overfitting_pivot, annot=True, fmt='.3f', cmap='Reds',\n",
    "           ax=axes[1,0], cbar_kws={'label': 'Overfitting Gap'})\n",
    "axes[1,0].set_title('Overfitting Gap (Train - Test Accuracy)')\n",
    "axes[1,0].set_xlabel('Gamma')\n",
    "axes[1,0].set_ylabel('C')\n",
    "\n",
    "# Plot 4: Number of Support Vectors\n",
    "n_support_pivot = results_df.pivot(index='C', columns='gamma', values='n_support')\n",
    "sns.heatmap(n_support_pivot, annot=True, fmt='d', cmap='viridis_r',\n",
    "           ax=axes[1,1], cbar_kws={'label': 'Number of Support Vectors'})\n",
    "axes[1,1].set_title('Number of Support Vectors vs C and Gamma')\n",
    "axes[1,1].set_xlabel('Gamma')\n",
    "axes[1,1].set_ylabel('C')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights from Parameter Analysis:\")\n",
    "print(\"1. High C + High Gamma: High training accuracy, risk of overfitting\")\n",
    "print(\"2. Low C + Low Gamma: Lower accuracy, but better generalization\")\n",
    "print(\"3. Support vectors decrease with higher C (stricter margin)\")\n",
    "print(\"4. Sweet spot: Balance between accuracy and generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiclass",
   "metadata": {},
   "source": [
    "## 6. Multi-class Classification\n",
    "\n",
    "SVM is naturally a **binary classifier**, but real-world problems often have multiple classes.\n",
    "\n",
    "### Two Main Approaches:\n",
    "\n",
    "#### 1. One-vs-One (OvO)\n",
    "- Train $\\frac{k(k-1)}{2}$ binary classifiers\n",
    "- Each classifier separates two classes\n",
    "- Final prediction: majority vote\n",
    "- **Pros**: Fewer samples per classifier, often more accurate\n",
    "- **Cons**: More models to train and store\n",
    "\n",
    "#### 2. One-vs-Rest (OvR)\n",
    "- Train $k$ binary classifiers\n",
    "- Each classifier separates one class from all others\n",
    "- Final prediction: highest confidence score\n",
    "- **Pros**: Fewer models, faster training\n",
    "- **Cons**: Imbalanced datasets for each classifier\n",
    "\n",
    "### Scikit-learn Default\n",
    "- Uses **One-vs-One** for SVM\n",
    "- Automatically handles multi-class problems\n",
    "- User doesn't need to worry about implementation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-class SVM\n",
    "print(\"=== MULTI-CLASS SVM DEMONSTRATION ===\")\n",
    "print()\n",
    "\n",
    "# Load Iris dataset (3 classes)\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Use only 2 features for visualization\n",
    "X_iris_2d = X_iris[:, [0, 2]]  # Sepal length and Petal length\n",
    "feature_names_2d = [iris.feature_names[0], iris.feature_names[2]]\n",
    "\n",
    "print(f\"Iris Dataset:\")\n",
    "print(f\"  Samples: {X_iris_2d.shape[0]}\")\n",
    "print(f\"  Features: {feature_names_2d}\")\n",
    "print(f\"  Classes: {iris.target_names}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_iris)}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris_2d, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train multi-class SVM\n",
    "svm_multiclass = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_multiclass.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_multiclass.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Multi-class SVM Results:\")\n",
    "print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"  Number of Support Vectors: {len(svm_multiclass.support_)}\")\n",
    "print(f\"  Support Vectors per class: {svm_multiclass.n_support_}\")\n",
    "print()\n",
    "\n",
    "# Show classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-multiclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-class decision boundaries\n",
    "print(\"=== MULTI-CLASS DECISION BOUNDARIES ===\")\n",
    "print()\n",
    "\n",
    "# Create a detailed plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Get predictions for the mesh\n",
    "Z = svm_multiclass.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision regions\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "\n",
    "# Plot training data\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y_train == i\n",
    "    plt.scatter(X_train_scaled[idx, 0], X_train_scaled[idx, 1], \n",
    "               c=color, s=100, alpha=0.8, \n",
    "               label=f'{iris.target_names[i]} (train)', \n",
    "               edgecolors='black', linewidth=0.5, marker='o')\n",
    "\n",
    "# Plot test data with different markers\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y_test == i\n",
    "    plt.scatter(X_test_scaled[idx, 0], X_test_scaled[idx, 1], \n",
    "               c=color, s=100, alpha=0.8, \n",
    "               label=f'{iris.target_names[i]} (test)', \n",
    "               edgecolors='white', linewidth=2, marker='s')\n",
    "\n",
    "# Highlight support vectors\n",
    "support_vectors_scaled = scaler.transform(svm_multiclass.support_vectors_)\n",
    "plt.scatter(support_vectors_scaled[:, 0], support_vectors_scaled[:, 1],\n",
    "           s=200, facecolors='none', edgecolors='yellow', \n",
    "           linewidths=3, label='Support Vectors')\n",
    "\n",
    "plt.xlabel(f'{feature_names_2d[0]} (scaled)')\n",
    "plt.ylabel(f'{feature_names_2d[1]} (scaled)')\n",
    "plt.title(f'Multi-class SVM Decision Boundaries\\nAccuracy: {accuracy:.3f}')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Multi-class Visualization Explained:\")\n",
    "print(\"  Colored regions: Decision regions for each class\")\n",
    "print(\"  Circles: Training data points\")\n",
    "print(\"  Squares: Test data points\")\n",
    "print(\"  Yellow circles: Support vectors\")\n",
    "print(\"  Boundaries: Where SVM is uncertain between classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-considerations",
   "metadata": {},
   "source": [
    "## 7. Practical Considerations\n",
    "\n",
    "### When to Use SVM\n",
    "\n",
    "✅ **Good for**:\n",
    "- **High-dimensional data**: Text classification, genomics\n",
    "- **Small to medium datasets**: SVM scales well\n",
    "- **Non-linear relationships**: RBF kernel handles complex patterns\n",
    "- **Robust classification**: Good generalization with proper tuning\n",
    "- **Binary classification**: Natural fit for SVM\n",
    "\n",
    "❌ **Avoid when**:\n",
    "- **Very large datasets**: Quadratic complexity in training samples\n",
    "- **Many features >> samples**: Risk of overfitting\n",
    "- **Noisy data with many outliers**: Sensitive to outliers\n",
    "- **Probability estimates needed**: Not SVM's natural output\n",
    "- **Interpretability required**: Black box with kernels\n",
    "\n",
    "### Preprocessing Requirements\n",
    "\n",
    "1. **Feature Scaling**: **Mandatory** for SVM!\n",
    "   - StandardScaler or MinMaxScaler\n",
    "   - SVM is sensitive to feature scales\n",
    "\n",
    "2. **Handling Missing Values**: SVM cannot handle NaN\n",
    "   - Impute or remove missing values\n",
    "\n",
    "3. **Outlier Treatment**: Consider outlier removal\n",
    "   - SVM is sensitive to outliers\n",
    "\n",
    "### Hyperparameter Tuning Strategy\n",
    "\n",
    "1. **Start with defaults**: `C=1.0`, `gamma='scale'`\n",
    "2. **Grid search**: Tune C and gamma together\n",
    "3. **Cross-validation**: Use stratified CV\n",
    "4. **Logarithmic search**: C and gamma in log space\n",
    "5. **Validation curve**: Plot performance vs parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive SVM evaluation with best practices\n",
    "print(\"=== COMPREHENSIVE SVM EVALUATION ===\")\n",
    "print()\n",
    "\n",
    "# Load and prepare Titanic dataset\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "print(f\"Dataset: Titanic Survival Prediction\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_train)}\")\n",
    "print()\n",
    "\n",
    "# Feature scaling (crucial for SVM!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling applied (StandardScaler)\")\n",
    "print(f\"  Before scaling - Feature 1: [{X_train.iloc[:, 0].min():.2f}, {X_train.iloc[:, 0].max():.2f}]\")\n",
    "print(f\"  After scaling  - Feature 1: [{X_train_scaled[:, 0].min():.2f}, {X_train_scaled[:, 0].max():.2f}]\")\n",
    "print()\n",
    "\n",
    "# Hyperparameter tuning with GridSearch\n",
    "print(\"Hyperparameter tuning with GridSearchCV...\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_grid = SVC(kernel='rbf', random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    svm_grid, param_grid, \n",
    "    cv=5, scoring='accuracy', \n",
    "    n_jobs=-1, verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "print()\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "y_proba = best_svm.predict_proba(X_test_scaled) if hasattr(best_svm, 'predict_proba') else None\n",
    "\n",
    "# Comprehensive evaluation\n",
    "evaluator = ModelEvaluator(\"Optimized SVM\")\n",
    "metrics = evaluator.evaluate_classification(\n",
    "    y_test, y_pred, y_proba, \n",
    "    class_names=['Died', 'Survived']\n",
    ")\n",
    "\n",
    "evaluator.print_detailed_report()\n",
    "\n",
    "print(f\"\\nSVM-Specific Metrics:\")\n",
    "print(f\"  Support Vectors: {len(best_svm.support_)}\")\n",
    "print(f\"  Support Vectors per class: {best_svm.n_support_}\")\n",
    "print(f\"  Kernel: {best_svm.kernel}\")\n",
    "print(f\"  C parameter: {best_svm.C}\")\n",
    "print(f\"  Gamma parameter: {best_svm.gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-scaling-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the importance of feature scaling\n",
    "print(\"=== IMPORTANCE OF FEATURE SCALING ===\")\n",
    "print()\n",
    "\n",
    "# Compare SVM performance with and without scaling\n",
    "print(\"Comparing SVM performance with and without feature scaling:\")\n",
    "print()\n",
    "\n",
    "# Without scaling\n",
    "svm_unscaled = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_unscaled.fit(X_train, y_train)\n",
    "pred_unscaled = svm_unscaled.predict(X_test)\n",
    "acc_unscaled = accuracy_score(y_test, pred_unscaled)\n",
    "\n",
    "# With scaling\n",
    "svm_scaled = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "pred_scaled = svm_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
    "\n",
    "print(f\"Without scaling:\")\n",
    "print(f\"  Accuracy: {acc_unscaled:.3f}\")\n",
    "print(f\"  Support Vectors: {len(svm_unscaled.support_)}\")\n",
    "print()\n",
    "print(f\"With scaling:\")\n",
    "print(f\"  Accuracy: {acc_scaled:.3f}\")\n",
    "print(f\"  Support Vectors: {len(svm_scaled.support_)}\")\n",
    "print()\n",
    "print(f\"Improvement from scaling: {acc_scaled - acc_unscaled:+.3f}\")\n",
    "\n",
    "# Show feature scales\n",
    "print(f\"\\nFeature scale ranges (before scaling):\")\n",
    "for i, feature in enumerate(feature_names):\n",
    "    min_val = X_train.iloc[:, i].min()\n",
    "    max_val = X_train.iloc[:, i].max()\n",
    "    print(f\"  {feature}: [{min_val:.2f}, {max_val:.2f}] (range: {max_val-min_val:.2f})\")\n",
    "\n",
    "print(\"\\nThis demonstrates why feature scaling is crucial for SVM!\")\n",
    "print(\"Features with larger scales can dominate the distance calculations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### 🎯 What You've Learned\n",
    "\n",
    "1. **Geometric Intuition**: SVM finds the maximum margin decision boundary\n",
    "2. **Mathematical Foundation**: Quadratic optimization with support vectors\n",
    "3. **Kernel Trick**: Mapping data to higher dimensions without explicit computation\n",
    "4. **Hyperparameters**: C controls margin-accuracy tradeoff, gamma controls kernel flexibility\n",
    "5. **Multi-class Handling**: One-vs-One and One-vs-Rest strategies\n",
    "6. **Practical Considerations**: Feature scaling is mandatory, good for high-dimensional data\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Advanced Kernels**: Explore custom kernels for domain-specific problems\n",
    "2. **SVM Variants**: Learn about SVR (regression), One-Class SVM (anomaly detection)\n",
    "3. **Large-scale SVM**: Study SGD-based solvers for big data\n",
    "4. **Feature Engineering**: Create better features for SVM\n",
    "5. **Ensemble Methods**: Combine SVM with other algorithms\n",
    "\n",
    "### 💡 Key Insights\n",
    "\n",
    "- **Maximum Margin Principle**: Provides better generalization than arbitrary boundaries\n",
    "- **Support Vectors**: Only a subset of training data determines the decision boundary\n",
    "- **Kernel Magic**: Can handle complex non-linear relationships efficiently\n",
    "- **Parameter Sensitivity**: C and gamma must be tuned carefully together\n",
    "- **Scaling Critical**: Feature scaling is not optional - it's essential\n",
    "- **High-Dimensional Strength**: Excels when features >> samples\n",
    "\n",
    "### 🛠️ Best Practices\n",
    "\n",
    "1. **Always scale features** using StandardScaler or MinMaxScaler\n",
    "2. **Start with RBF kernel** and default parameters\n",
    "3. **Use GridSearchCV** to tune C and gamma simultaneously\n",
    "4. **Cross-validate** to get robust performance estimates\n",
    "5. **Check for outliers** - they can heavily influence SVM\n",
    "6. **Consider linear SVM** for very high-dimensional data\n",
    "\n",
    "### ⚡ When to Choose SVM\n",
    "\n",
    "**Perfect for:**\n",
    "- Text classification (high dimensions)\n",
    "- Image recognition with proper features\n",
    "- Bioinformatics (gene expression, protein classification)\n",
    "- Small to medium datasets with complex patterns\n",
    "\n",
    "**Consider alternatives for:**\n",
    "- Very large datasets (>100k samples)\n",
    "- Simple linear relationships\n",
    "- When interpretability is crucial\n",
    "- Real-time applications requiring fast prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand Support Vector Machines, one of the most mathematically elegant and powerful machine learning algorithms. SVM combines solid theoretical foundations with practical effectiveness, making it a valuable tool in your machine learning toolkit.\n",
    "\n",
    "Remember: The key to SVM success is in the details - proper scaling, careful hyperparameter tuning, and choosing the right kernel for your data! 🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}