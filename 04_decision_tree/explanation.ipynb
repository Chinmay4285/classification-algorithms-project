{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Decision Tree Classification: A Complete Guide\n",
    "\n",
    "Welcome to your comprehensive guide to **Decision Tree classification**! This notebook will teach you everything about one of the most intuitive and interpretable machine learning algorithms.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Intuitive Understanding**: How decision trees mimic human decision-making\n",
    "2. **Tree Construction**: How algorithms build trees step by step\n",
    "3. **Splitting Criteria**: Gini impurity, entropy, and information gain\n",
    "4. **Overfitting & Pruning**: Controlling tree complexity\n",
    "5. **Practical Implementation**: Real-world examples with visualization\n",
    "6. **Advantages & Limitations**: When to use decision trees\n",
    "7. **Feature Importance**: Understanding what the model learned\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intuition",
   "metadata": {},
   "source": [
    "## 1. Intuition: How Do We Make Decisions?\n",
    "\n",
    "### Human Decision Making\n",
    "\n",
    "**Example**: Should you go outside today?\n",
    "\n",
    "```\n",
    "Is it raining?\n",
    "├── Yes → Stay inside\n",
    "└── No → Is it sunny?\n",
    "    ├── Yes → Go outside!\n",
    "    └── No → Is it cold?\n",
    "        ├── Yes → Stay inside\n",
    "        └── No → Go outside!\n",
    "```\n",
    "\n",
    "This is exactly how a **Decision Tree** works! It asks a series of yes/no questions to reach a decision.\n",
    "\n",
    "### Machine Learning Decision Making\n",
    "\n",
    "**Example**: Predicting if a customer will buy a product\n",
    "\n",
    "```\n",
    "Is income > $50,000?\n",
    "├── Yes → Is age < 30?\n",
    "│   ├── Yes → Will BUY (90% confidence)\n",
    "│   └── No → Will NOT buy (75% confidence)\n",
    "└── No → Is price < $100?\n",
    "    ├── Yes → Will BUY (60% confidence)\n",
    "    └── No → Will NOT buy (95% confidence)\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Root Node**: The top question (most important feature)\n",
    "- **Internal Nodes**: Decision points (questions)\n",
    "- **Leaf Nodes**: Final predictions\n",
    "- **Branches**: Possible answers to questions\n",
    "- **Depth**: How many questions deep the tree goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from utils.data_utils import load_titanic_data\n",
    "from utils.evaluation import ModelEvaluator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[START] Decision Tree Classification Tutorial\")\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-example",
   "metadata": {},
   "source": [
    "## 2. Simple Example: Building Your First Tree\n",
    "\n",
    "Let's start with a simple dataset and build a decision tree step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-simple-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset for understanding\n",
    "print(\"=== SIMPLE DECISION TREE EXAMPLE ===\")\n",
    "print(\"Dataset: Predicting if a person will play tennis\")\n",
    "print()\n",
    "\n",
    "# Create synthetic tennis dataset\n",
    "data = {\n",
    "    'Weather': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast',\n",
    "                'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool',\n",
    "                   'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal',\n",
    "                'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong',\n",
    "            'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'Play_Tennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes',\n",
    "                   'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "df_tennis = pd.DataFrame(data)\n",
    "print(\"Tennis Dataset:\")\n",
    "print(df_tennis)\n",
    "print()\n",
    "\n",
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create encoders for each categorical column\n",
    "encoders = {}\n",
    "df_encoded = df_tennis.copy()\n",
    "\n",
    "for column in df_tennis.columns:\n",
    "    if df_tennis[column].dtype == 'object':\n",
    "        encoders[column] = LabelEncoder()\n",
    "        df_encoded[column] = encoders[column].fit_transform(df_tennis[column])\n",
    "\n",
    "print(\"Encoded Dataset (for machine learning):\")\n",
    "print(df_encoded)\n",
    "print()\n",
    "\n",
    "# Show encoding mappings\n",
    "print(\"Encoding Mappings:\")\n",
    "for column, encoder in encoders.items():\n",
    "    mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    print(f\"{column}: {mapping}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-simple-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decision tree on tennis dataset\n",
    "X_tennis = df_encoded.drop('Play_Tennis', axis=1)\n",
    "y_tennis = df_encoded['Play_Tennis']\n",
    "\n",
    "print(\"=== BUILDING DECISION TREE ===\")\n",
    "print(f\"Features: {list(X_tennis.columns)}\")\n",
    "print(f\"Target: Play_Tennis (0=No, 1=Yes)\")\n",
    "print()\n",
    "\n",
    "# Create a simple decision tree\n",
    "tree_simple = DecisionTreeClassifier(\n",
    "    max_depth=3,  # Limit depth for interpretability\n",
    "    random_state=42,\n",
    "    min_samples_split=2\n",
    ")\n",
    "\n",
    "tree_simple.fit(X_tennis, y_tennis)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tennis = tree_simple.predict(X_tennis)\n",
    "accuracy_tennis = accuracy_score(y_tennis, y_pred_tennis)\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy_tennis:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance = tree_simple.feature_importances_\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in zip(X_tennis.columns, feature_importance):\n",
    "    print(f\"  {feature}: {importance:.3f}\")\n",
    "print()\n",
    "\n",
    "# Print the tree in text format\n",
    "print(\"Decision Tree Rules:\")\n",
    "tree_rules = export_text(tree_simple, feature_names=list(X_tennis.columns))\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-simple-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the simple decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(tree_simple, \n",
    "          feature_names=X_tennis.columns,\n",
    "          class_names=['No', 'Yes'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree: Tennis Playing Decision')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to read this tree:\")\n",
    "print(\"1. Start at the top (root node)\")\n",
    "print(\"2. Follow the path based on your data\")\n",
    "print(\"3. Each node shows:\")\n",
    "print(\"   - The splitting condition (e.g., Weather <= 1.5)\")\n",
    "print(\"   - Gini impurity (measure of 'messiness')\")\n",
    "print(\"   - Number of samples\")\n",
    "print(\"   - Class distribution [No, Yes]\")\n",
    "print(\"   - Predicted class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "splitting-criteria",
   "metadata": {},
   "source": [
    "## 3. How Trees Choose Splits: The Mathematics\n",
    "\n",
    "Decision trees need to choose the **best question** to ask at each node. But how do they decide what \"best\" means?\n",
    "\n",
    "### 3.1 Gini Impurity\n",
    "\n",
    "**Gini Impurity** measures how \"mixed up\" the classes are in a node.\n",
    "\n",
    "**Formula**: $Gini = 1 - \\sum_{i=1}^{c} p_i^2$\n",
    "\n",
    "Where $p_i$ is the probability of class $i$\n",
    "\n",
    "**Examples**:\n",
    "- Pure node (all same class): Gini = 0\n",
    "- 50/50 split: Gini = 0.5 (maximum impurity)\n",
    "- 90/10 split: Gini = 0.18\n",
    "\n",
    "### 3.2 Entropy and Information Gain\n",
    "\n",
    "**Entropy** is another measure of impurity based on information theory.\n",
    "\n",
    "**Formula**: $Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
    "\n",
    "**Information Gain**: $IG = Entropy_{parent} - \\sum \\frac{N_{child}}{N_{parent}} \\times Entropy_{child}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "splitting-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate splitting criteria calculations\n",
    "print(\"=== UNDERSTANDING SPLITTING CRITERIA ===\")\n",
    "print()\n",
    "\n",
    "def calculate_gini(y):\n",
    "    \"\"\"Calculate Gini impurity\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    gini = 1 - np.sum(probabilities ** 2)\n",
    "    return gini\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"Calculate entropy\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Add small value to avoid log(0)\n",
    "    return entropy\n",
    "\n",
    "# Example calculations with tennis dataset\n",
    "print(\"Example: Root node impurity calculation\")\n",
    "print(f\"Total samples: {len(y_tennis)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_tennis)}\")\n",
    "print(f\"Class probabilities: {np.bincount(y_tennis) / len(y_tennis)}\")\n",
    "print()\n",
    "\n",
    "root_gini = calculate_gini(y_tennis)\n",
    "root_entropy = calculate_entropy(y_tennis)\n",
    "\n",
    "print(f\"Root node Gini impurity: {root_gini:.3f}\")\n",
    "print(f\"Root node Entropy: {root_entropy:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show examples of pure and impure nodes\n",
    "print(\"Impurity Examples:\")\n",
    "examples = [\n",
    "    ([0, 0, 0, 0], \"Pure node (all class 0)\"),\n",
    "    ([1, 1, 1, 1], \"Pure node (all class 1)\"),\n",
    "    ([0, 0, 1, 1], \"50/50 split (maximum impurity)\"),\n",
    "    ([0, 0, 0, 1], \"75/25 split\"),\n",
    "    ([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], \"90/10 split\")\n",
    "]\n",
    "\n",
    "for example_y, description in examples:\n",
    "    gini = calculate_gini(example_y)\n",
    "    entropy = calculate_entropy(example_y)\n",
    "    print(f\"{description:30} - Gini: {gini:.3f}, Entropy: {entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-criteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different splitting criteria on the same data\n",
    "print(\"=== COMPARING SPLITTING CRITERIA ===\")\n",
    "print(\"Training trees with different criteria on Titanic dataset\")\n",
    "print()\n",
    "\n",
    "# Load Titanic data\n",
    "X_train, X_test, y_train, y_test, feature_names = load_titanic_data()\n",
    "\n",
    "criteria = ['gini', 'entropy']\n",
    "results = []\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "for i, criterion in enumerate(criteria):\n",
    "    # Train tree with specific criterion\n",
    "    tree = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=4,  # Limit depth for visualization\n",
    "        random_state=42,\n",
    "        min_samples_split=10\n",
    "    )\n",
    "    \n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Criterion': criterion.title(),\n",
    "        'Accuracy': accuracy,\n",
    "        'Tree_Depth': tree.tree_.max_depth,\n",
    "        'Num_Leaves': tree.tree_.n_leaves\n",
    "    })\n",
    "    \n",
    "    print(f\"{criterion.title()} Criterion:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Tree depth: {tree.tree_.max_depth}\")\n",
    "    print(f\"  Number of leaves: {tree.tree_.n_leaves}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize tree (first few levels only)\n",
    "    ax = axes[i]\n",
    "    plot_tree(tree, \n",
    "              max_depth=2,  # Show only first 2 levels\n",
    "              feature_names=feature_names,\n",
    "              class_names=['Died', 'Survived'],\n",
    "              filled=True,\n",
    "              rounded=True,\n",
    "              fontsize=8,\n",
    "              ax=ax)\n",
    "    ax.set_title(f'Decision Tree with {criterion.title()} Criterion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Comparison Results:\")\n",
    "print(results_df)\n",
    "print()\n",
    "print(\"Note: Both criteria often produce similar results, but may create\")\n",
    "print(\"slightly different tree structures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting",
   "metadata": {},
   "source": [
    "## 4. The Overfitting Problem\n",
    "\n",
    "Decision trees have a **major weakness**: they can easily **overfit** to training data.\n",
    "\n",
    "### What is Overfitting?\n",
    "\n",
    "**Overfitting** occurs when a model learns the training data too well, including noise and outliers, making it perform poorly on new data.\n",
    "\n",
    "### Why Do Decision Trees Overfit?\n",
    "\n",
    "1. **Perfect Memory**: Trees can create a unique path for every training sample\n",
    "2. **No Generalization**: They memorize rather than learn patterns\n",
    "3. **Complex Boundaries**: Can create overly complex decision boundaries\n",
    "\n",
    "### Signs of Overfitting\n",
    "\n",
    "- High training accuracy (often 100%)\n",
    "- Much lower test accuracy\n",
    "- Very deep trees with many branches\n",
    "- Complex rules that seem specific to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting\n",
    "print(\"=== DEMONSTRATING OVERFITTING ===\")\n",
    "print()\n",
    "\n",
    "# Create trees with different maximum depths\n",
    "depths = [1, 2, 3, 5, 10, 20, None]  # None means no limit\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "tree_sizes = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(\n",
    "        max_depth=depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred = tree.predict(X_train)\n",
    "    test_pred = tree.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    tree_sizes.append(tree.tree_.n_leaves)\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else 'Unlimited'\n",
    "    print(f\"Max Depth {depth_str:>9}: Train={train_acc:.3f}, Test={test_acc:.3f}, Leaves={tree.tree_.n_leaves}\")\n",
    "\n",
    "print()\n",
    "print(\"Observation: As depth increases, training accuracy increases but\")\n",
    "print(\"test accuracy may decrease - this is overfitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-overfitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Accuracy vs Tree Depth\n",
    "ax1 = axes[0]\n",
    "depth_labels = [str(d) if d is not None else 'Unlimited' for d in depths]\n",
    "x_pos = range(len(depths))\n",
    "\n",
    "ax1.plot(x_pos, train_accuracies, 'o-', label='Training Accuracy', linewidth=2, markersize=8)\n",
    "ax1.plot(x_pos, test_accuracies, 's-', label='Test Accuracy', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Maximum Tree Depth')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Overfitting in Decision Trees')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(depth_labels, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Tree Size vs Depth\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x_pos, tree_sizes, alpha=0.7, color='orange')\n",
    "ax2.set_xlabel('Maximum Tree Depth')\n",
    "ax2.set_ylabel('Number of Leaf Nodes')\n",
    "ax2.set_title('Tree Complexity vs Depth')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(depth_labels, rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add values on top of bars\n",
    "for i, v in enumerate(tree_sizes):\n",
    "    ax2.text(i, v + max(tree_sizes) * 0.01, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"1. Training accuracy always improves with more complexity\")\n",
    "print(\"2. Test accuracy peaks at moderate complexity, then decreases\")\n",
    "print(\"3. Tree size grows exponentially with unlimited depth\")\n",
    "print(\"4. The gap between train/test accuracy indicates overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Takeaways\n",
    "\n",
    "### 🎯 What You've Learned\n",
    "\n",
    "1. **Intuitive Understanding**: Decision trees mimic human decision-making\n",
    "2. **Tree Construction**: How algorithms choose splits using impurity measures\n",
    "3. **Overfitting Problem**: Trees easily memorize training data\n",
    "4. **Splitting Criteria**: Gini impurity vs entropy for node purity\n",
    "5. **Interpretability**: Reading and understanding tree decisions\n",
    "6. **Practical Implementation**: Real-world application and evaluation\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Ensemble Methods**: Learn Random Forest and Gradient Boosting\n",
    "2. **Advanced Pruning**: Explore post-pruning techniques\n",
    "3. **Hyperparameter Tuning**: Optimize tree parameters\n",
    "4. **Regression Trees**: Extend to continuous target variables\n",
    "5. **Feature Engineering**: Create better features for tree-based models\n",
    "\n",
    "### 💡 Key Insights\n",
    "\n",
    "- **Interpretability vs Accuracy**: Decision trees trade some accuracy for interpretability\n",
    "- **Overfitting is Real**: Always validate on separate data\n",
    "- **Feature Engineering Matters**: Good features lead to better trees\n",
    "- **Ensemble Methods**: Single trees are unstable; ensembles are better\n",
    "- **Start Simple**: Begin with shallow trees and increase complexity carefully\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand decision trees, one of the most intuitive machine learning algorithms. While they have limitations, decision trees form the foundation for powerful ensemble methods like Random Forest and Gradient Boosting.\n",
    "\n",
    "Remember: The best model is often the one you can understand and explain! 🌳"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}